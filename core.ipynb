{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3025d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e622e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:23:50_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bc7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import json\n",
    "from pathlib import Path as Data_Path\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d21f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128; Torch-cuda version: 12.8; Torch Geometric version: 2.7.0.\n"
     ]
    }
   ],
   "source": [
    "# Import relevant ML libraries\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Embedding, ModuleList, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv, GATConv, SAGEConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}; Torch-cuda version: {torch.version.cuda}; Torch Geometric version: {torch_geometric.__version__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2c58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "seed = 224\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795f1e6",
   "metadata": {},
   "source": [
    "# `Loading the Data from JSON Files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3f05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"/home/DSE411/Documents/Tanishq/mlg-music-recommendation\"\n",
    "DATA_DIR = Data_Path('spotify_million_playlist_dataset/data')\n",
    "os.chdir(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00733f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'OVO', 'collaborative': 'false', 'pid': 241000, 'modified_at': 1493596800, 'num_tracks': 45, 'num_albums': 10, 'num_followers': 1, 'tracks': [{'pos': 0, 'artist_name': 'Drake', 'track_uri': 'spotify:track:7jslhIiELQkgW9IHeYNOWE', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Big Rings', 'album_uri': 'spotify:album:1ozpmkWcCHwsQ4QTnxOOdT', 'duration_ms': 217706, 'album_name': 'What A Time To Be Alive'}, {'pos': 1, 'artist_name': 'Drake', 'track_uri': 'spotify:track:2AGottAzfC8bHzF7kEJ3Wa', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Diamonds Dancing', 'album_uri': 'spotify:album:1ozpmkWcCHwsQ4QTnxOOdT', 'duration_ms': 314631, 'album_name': 'What A Time To Be Alive'}, {'pos': 2, 'artist_name': 'Drake', 'track_uri': 'spotify:track:27GmP9AWRs744SzKcpJsTZ', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Jumpman', 'album_uri': 'spotify:album:1ozpmkWcCHwsQ4QTnxOOdT', 'duration_ms': 205879, 'album_name': 'What A Time To Be Alive'}, {'pos': 3, 'artist_name': 'Future', 'track_uri': 'spotify:track:2ACys0pX0SjmpQmQWzp7wt', 'artist_uri': 'spotify:artist:1RyvyyTE3xzB2ZywiAwp0i', 'track_name': 'Jersey', 'album_uri': 'spotify:album:1ozpmkWcCHwsQ4QTnxOOdT', 'duration_ms': 188702, 'album_name': 'What A Time To Be Alive'}, {'pos': 4, 'artist_name': 'Drake', 'track_uri': 'spotify:track:1DmnEYXa4WfbdhAPwNzgD8', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': '30 for 30 Freestyle', 'album_uri': 'spotify:album:1ozpmkWcCHwsQ4QTnxOOdT', 'duration_ms': 253934, 'album_name': 'What A Time To Be Alive'}, {'pos': 5, 'artist_name': 'Drake', 'track_uri': 'spotify:track:1ID1QFSNNxi0hiZCNcwjUC', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Legend', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 241853, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 6, 'artist_name': 'Drake', 'track_uri': 'spotify:track:79XrkTOfV1AqySNjVlygpW', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Energy', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 181933, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 7, 'artist_name': 'Drake', 'track_uri': 'spotify:track:12d5QFwzh60IIHlsSnAvps', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': '10 Bands', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 177733, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 8, 'artist_name': 'Drake', 'track_uri': 'spotify:track:5InOp6q2vvx0fShv3bzFLZ', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Know Yourself', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 275840, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 9, 'artist_name': 'Drake', 'track_uri': 'spotify:track:3a8tAZFJxlmBwOtrf5L1oC', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': '6 God', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 180666, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 10, 'artist_name': 'Drake', 'track_uri': 'spotify:track:4kdfjhj9xNkYU0R8xlDy8k', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': '6 Man', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 167653, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 11, 'artist_name': 'Drake', 'track_uri': 'spotify:track:5mZJwWdxAOR4xUvSGZvvMU', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': '6PM In New York', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 283306, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 12, 'artist_name': 'Drake', 'track_uri': 'spotify:track:7JXZq0JgG2zTrSOAgY8VMC', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Jungle', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 320400, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 13, 'artist_name': 'Drake', 'track_uri': 'spotify:track:4zLOwx1yRJXWkHKt1XzF1p', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Now & Forever', 'album_uri': 'spotify:album:0ptlfJfwGTy0Yvrk14JK1I', 'duration_ms': 281320, 'album_name': \"If You're Reading This It's Too Late\"}, {'pos': 14, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6cT5orvyKqwghJp6KB9vG0', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Furthest Thing', 'album_uri': 'spotify:album:2gXTTQ713nCELgPOS0qWyt', 'duration_ms': 267373, 'album_name': 'Nothing Was The Same'}, {'pos': 15, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6V2D8Lls36APk0THDjBDfE', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Started From the Bottom', 'album_uri': 'spotify:album:2gXTTQ713nCELgPOS0qWyt', 'duration_ms': 174133, 'album_name': 'Nothing Was The Same'}, {'pos': 16, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6oF3Es1YzzmLKjGBfThUvD', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Worst Behavior', 'album_uri': 'spotify:album:2gXTTQ713nCELgPOS0qWyt', 'duration_ms': 270186, 'album_name': 'Nothing Was The Same'}, {'pos': 17, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6KziiQUOoCmC7Kc7Rv4jar', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Wu-Tang Forever', 'album_uri': 'spotify:album:2gXTTQ713nCELgPOS0qWyt', 'duration_ms': 217680, 'album_name': 'Nothing Was The Same'}, {'pos': 18, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6jdOi5U5LBzQrc4c1VT983', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': \"Hold On, We're Going Home\", 'album_uri': 'spotify:album:2ZUFSbIkmFkGag000RWOpA', 'duration_ms': 227880, 'album_name': 'Nothing Was The Same'}, {'pos': 19, 'artist_name': 'Drake', 'track_uri': 'spotify:track:4kNvYhyl8R6m1vykVkcuBu', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'All Me', 'album_uri': 'spotify:album:2gXTTQ713nCELgPOS0qWyt', 'duration_ms': 270866, 'album_name': 'Nothing Was The Same'}, {'pos': 20, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6LxSe8YmdPxy095Ux6znaQ', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Headlines', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 235986, 'album_name': 'Take Care'}, {'pos': 21, 'artist_name': 'Drake', 'track_uri': 'spotify:track:0V4l4GQhgnWQGtCWpvA7va', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Crew Love', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 208813, 'album_name': 'Take Care'}, {'pos': 22, 'artist_name': 'Drake', 'track_uri': 'spotify:track:124NFj84ppZ5pAxTuVQYCQ', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Take Care', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 277386, 'album_name': 'Take Care'}, {'pos': 23, 'artist_name': 'Drake', 'track_uri': 'spotify:track:1D9XLqQp2YYiOxrr5KLb8K', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Under Ground Kings', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 212613, 'album_name': 'Take Care'}, {'pos': 24, 'artist_name': 'Drake', 'track_uri': 'spotify:track:7udsBKuqnJ5csWTAkR0vEI', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': \"We'll Be Fine\", 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 247946, 'album_name': 'Take Care'}, {'pos': 25, 'artist_name': 'Drake', 'track_uri': 'spotify:track:7yfg0Eer6UZZt5tZ1XdsWz', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Make Me Proud', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 219893, 'album_name': 'Take Care'}, {'pos': 26, 'artist_name': 'Drake', 'track_uri': 'spotify:track:1QBwk6GTCxVdC2hoSw9tlM', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Lord Knows', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 307640, 'album_name': 'Take Care'}, {'pos': 27, 'artist_name': 'Drake', 'track_uri': 'spotify:track:0m1KYWlT6LhFRBDVq9UNx4', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'HYFR (Hell Ya Fucking Right)', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 206626, 'album_name': 'Take Care'}, {'pos': 28, 'artist_name': 'Drake', 'track_uri': 'spotify:track:0jF2AdhsalO1L7KkhK4LE5', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Practice', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 237733, 'album_name': 'Take Care'}, {'pos': 29, 'artist_name': 'Drake', 'track_uri': 'spotify:track:74atKkOasLOVzvqB6mYgga', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'The Real Her', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 321080, 'album_name': 'Take Care'}, {'pos': 30, 'artist_name': 'Drake', 'track_uri': 'spotify:track:73tgFzBug5Ifk1Retdtwk7', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Fireworks', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 313280, 'album_name': 'Thank Me Later'}, {'pos': 31, 'artist_name': 'Drake', 'track_uri': 'spotify:track:6BdgtqiV3oXNqBikezwdvC', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Over', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 233560, 'album_name': 'Thank Me Later'}, {'pos': 32, 'artist_name': 'Drake', 'track_uri': 'spotify:track:0llA0pYA6GpGk7fTjew0wO', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'The Resistance', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 225360, 'album_name': 'Thank Me Later'}, {'pos': 33, 'artist_name': 'Drake', 'track_uri': 'spotify:track:7v0hKO3RYhEXt2EPXf4AOS', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Show Me A Good Time', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 210360, 'album_name': 'Thank Me Later'}, {'pos': 34, 'artist_name': 'Drake', 'track_uri': 'spotify:track:75L0qdzRnhwV62UXoNq3pE', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Up All Night', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 234333, 'album_name': 'Thank Me Later'}, {'pos': 35, 'artist_name': 'Drake', 'track_uri': 'spotify:track:0PWQqF5PvqRq0OPLSH0FKI', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Light Up', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 274146, 'album_name': 'Thank Me Later'}, {'pos': 36, 'artist_name': 'Drake', 'track_uri': 'spotify:track:1U4mweNwisxNj23ffuC9gO', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Miss Me', 'album_uri': 'spotify:album:6jlrjFR9mJV3jd1IPSplXU', 'duration_ms': 305520, 'album_name': 'Thank Me Later'}, {'pos': 37, 'artist_name': 'Drake', 'track_uri': 'spotify:track:3fyMH1t6UPeR5croea9PrR', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Best I Ever Had', 'album_uri': 'spotify:album:61NNWRxokNUQx0aYysBL76', 'duration_ms': 257760, 'album_name': 'So Far Gone'}, {'pos': 38, 'artist_name': 'Drake', 'track_uri': 'spotify:track:0faYOYKrE2vel4dQQqgXkf', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Houstatlantavegas', 'album_uri': 'spotify:album:61NNWRxokNUQx0aYysBL76', 'duration_ms': 290426, 'album_name': 'So Far Gone'}, {'pos': 39, 'artist_name': 'Drake', 'track_uri': 'spotify:track:2RwFo57JjznjveEqvBuRJX', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Successful', 'album_uri': 'spotify:album:61NNWRxokNUQx0aYysBL76', 'duration_ms': 351720, 'album_name': 'So Far Gone'}, {'pos': 40, 'artist_name': 'Drake', 'track_uri': 'spotify:track:2ZRJRe82aZaVhOKKlbJr4v', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Summer Sixteen', 'album_uri': 'spotify:album:0UWfsn9kLlH565DoEuvdtA', 'duration_ms': 202450, 'album_name': 'Summer Sixteen'}, {'pos': 41, 'artist_name': 'Drake', 'track_uri': 'spotify:track:4jtyUzZm9WLc2AdaJ1dso7', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': '0 To 100 / The Catch Up', 'album_uri': 'spotify:album:5OkxLN5XaE9rLbgK2FuKBE', 'duration_ms': 275226, 'album_name': '0 To 100 / The Catch Up'}, {'pos': 42, 'artist_name': 'Drake', 'track_uri': 'spotify:track:4Kz4RdRCceaA9VgTqBhBfa', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'The Motto', 'album_uri': 'spotify:album:6X1x82kppWZmDzlXXK3y3q', 'duration_ms': 181573, 'album_name': 'Take Care'}, {'pos': 43, 'artist_name': 'PARTYNEXTDOOR', 'track_uri': 'spotify:track:1wZqJM5FGDEl3FjHDxDyQd', 'artist_uri': 'spotify:artist:2HPaUgqeutzr3jx5a9WyDV', 'track_name': 'Come and See Me (feat. Drake)', 'album_uri': 'spotify:album:2FXGUAESmG5l9YPrzWPvHI', 'duration_ms': 235477, 'album_name': 'PARTYNEXTDOOR 3 (P3)'}, {'pos': 44, 'artist_name': 'Drake', 'track_uri': 'spotify:track:2cx10hB95ygrUp2RsZW7Oh', 'artist_uri': 'spotify:artist:3TVXtAsR1Inumwj472S9r4', 'track_name': 'Connect', 'album_uri': 'spotify:album:2gXTTQ713nCELgPOS0qWyt', 'duration_ms': 296413, 'album_name': 'Nothing Was The Same'}], 'num_edits': 3, 'duration_ms': 11082994, 'num_artists': 3}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{DATA_DIR}/{os.listdir(DATA_DIR)[0]}\") as jf:\n",
    "  example_file = json.load(jf)\n",
    "\n",
    "print(example_file['playlists'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cec57e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define classes for the data that we are going to load. The data is stored in JSON files, each\n",
    "which contain playlists, which themselves contain tracks. Thus, we define three classes:\n",
    "  Track       --> contains information for a specific track (its id, name, etc.)\n",
    "  Playlist    --> contains information for a specific playlist (its id, name, etc. as well as a list of Tracks)\n",
    "  JSONFile    --> contains the loaded json file and stores a dictionary of all of the Playlists\n",
    "\n",
    "Note: if we were to use the artist information, we could make an Artist class\n",
    "\"\"\"\n",
    "\n",
    "class Track:\n",
    "  \"\"\"\n",
    "  Simple class for a track, containing its attributes:\n",
    "    1. URI (a unique id)\n",
    "    2. Name\n",
    "    3. Artist info (URI and name)\n",
    "    4. Parent playlist\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, track_dict, playlist):\n",
    "    self.uri = track_dict[\"track_uri\"]\n",
    "    self.name = track_dict[\"track_name\"]\n",
    "    self.artist_uri = track_dict[\"artist_uri\"]\n",
    "    self.artist_name = track_dict[\"artist_name\"]\n",
    "    self.album_uri = track_dict[\"album_uri\"]\n",
    "    self.album_name = track_dict[\"album_name\"]\n",
    "    self.playlist = playlist\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Track {self.uri} called {self.name} by {self.artist_uri} ({self.artist_name}) and in {self.album_uri} | {self.album_name} in playlist {self.playlist}.\"\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Track {self.uri}\"\n",
    "\n",
    "class Playlist:\n",
    "  \"\"\"\n",
    "  Simple class for a playlist, containing its attributes:\n",
    "    1. Name (playlist and its associated index)\n",
    "    2. Title (playlist title in the Spotify dataset)\n",
    "    3. Loaded dictionary from the raw json for the playlist\n",
    "    4. Dictionary of tracks (track_uri : Track), populated by .load_tracks()\n",
    "    5. List of artists uris\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, json_data, index):\n",
    "\n",
    "    self.name = f\"playlist_{index}\"\n",
    "    self.title = json_data[\"name\"]\n",
    "    self.data = json_data\n",
    "\n",
    "    self.tracks = {}\n",
    "\n",
    "  def load_tracks(self):\n",
    "    \"\"\" Call this function to load all of the tracks in the json data for the playlist.\"\"\"\n",
    "\n",
    "    tracks_list = self.data[\"tracks\"]\n",
    "    self.tracks = {x[\"track_uri\"] : Track(x, self.name) for x in tracks_list}\n",
    "\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Playlist {self.name} with {len(self.tracks)} tracks loaded.\"\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Playlist {self.name}\"\n",
    "\n",
    "class JSONFile:\n",
    "  \"\"\"\n",
    "  Simple class for a JSON file, containing its attributes:\n",
    "    1. File Name\n",
    "    2. Index to begin numbering playlists at\n",
    "    3. Loaded dictionary from the raw json for the full file\n",
    "    4. Dictionary of playlists (name : Playlist), populated by .process_file()\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, data_path, file_name, start_index):\n",
    "\n",
    "    self.file_name = file_name\n",
    "    self.start_index = start_index\n",
    "\n",
    "    with open(join(data_path, file_name)) as json_file:\n",
    "      json_data = json.load(json_file)\n",
    "    self.data = json_data\n",
    "\n",
    "    self.playlists = {}\n",
    "\n",
    "  def process_file(self):\n",
    "    \"\"\" Call this function to load all of the playlists in the json data.\"\"\"\n",
    "\n",
    "    for i, playlist_json in enumerate(self.data[\"playlists\"]):\n",
    "      playlist = Playlist(playlist_json, self.start_index + i)\n",
    "      playlist.load_tracks()\n",
    "      self.playlists[playlist.name] = playlist\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"JSON {self.file_name} has {len(self.playlists)} playlists loaded.\"\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90a70f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6cb4a431fd465b89086883cd080a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files processed:   0%|          | 0/50 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = Data_Path('spotify_million_playlist_dataset/data')\n",
    "N_FILES_TO_USE = 50\n",
    "\n",
    "file_names = sorted(os.listdir(DATA_PATH))\n",
    "file_names_to_use = file_names[:N_FILES_TO_USE]\n",
    "\n",
    "n_playlists = 0\n",
    "\n",
    "# load each json file, and store it in a list of files\n",
    "JSONs = []\n",
    "for file_name in tqdm(file_names_to_use, desc='Files processed: ', unit='files', total=len(file_names_to_use)):\n",
    "  json_file = JSONFile(DATA_PATH, file_name, n_playlists)\n",
    "  json_file.process_file()\n",
    "  n_playlists += len(json_file.playlists)\n",
    "  JSONs.append(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83098628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc0210d4b3349fd8d3ac236c44f7195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "playlist_data = {}\n",
    "track_data = []\n",
    "playlists = []\n",
    "tracks = []\n",
    "\n",
    "# build list of all unique playlists, tracks\n",
    "for json_file in tqdm(JSONs):\n",
    "  playlists += [p.name for p in json_file.playlists.values()]\n",
    "  tracks += [track.uri for playlist in json_file.playlists.values() for track in list(playlist.tracks.values())]\n",
    "  playlist_data = playlist_data | json_file.playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9182d09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 457016, 3303932)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(playlists), len(set(tracks)), len(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c47d2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tracks = len(set(tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d556ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mlg/lib/python3.10/site-packages/networkx/classes/graph.py:629\u001b[0m, in \u001b[0;36mGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     newnode \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m\n\u001b[1;32m    630\u001b[0m     newdict \u001b[38;5;241m=\u001b[39m attr\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[1;32m      5\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from([\n\u001b[1;32m      6\u001b[0m     (p, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_type\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaylist\u001b[39m\u001b[38;5;124m\"\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m playlists\n\u001b[1;32m      7\u001b[0m ])\n\u001b[0;32m----> 8\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtracks\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# add node types of track to album and artist\u001b[39;00m\n\u001b[1;32m     12\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from([\n\u001b[1;32m     13\u001b[0m     (track\u001b[38;5;241m.\u001b[39malbum_uri, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:track\u001b[38;5;241m.\u001b[39malbum_uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_type\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbum\u001b[39m\u001b[38;5;124m\"\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m p_name, playlist \u001b[38;5;129;01min\u001b[39;00m playlist_data\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m playlist\u001b[38;5;241m.\u001b[39mtracks\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     14\u001b[0m ])\n",
      "File \u001b[0;32m~/miniconda3/envs/mlg/lib/python3.10/site-packages/networkx/classes/graph.py:629\u001b[0m, in \u001b[0;36mGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes_for_adding:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m         newnode \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m\n\u001b[1;32m    630\u001b[0m         newdict \u001b[38;5;241m=\u001b[39m attr\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## create graph from these lists\n",
    "\n",
    "# adding nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from([\n",
    "    (p, {'name':p, \"node_type\" : \"playlist\"}) for p in playlists\n",
    "])\n",
    "G.add_nodes_from([\n",
    "    (t, {'name':t, \"node_type\" : \"track\"}) for t in tracks\n",
    "])\n",
    "# add node types of track to album and artist\n",
    "G.add_nodes_from([\n",
    "    (track.album_uri, {'name':track.album_uri, \"node_type\" : \"album\"}) for p_name, playlist in playlist_data.items() for track in playlist.tracks.values()\n",
    "])\n",
    "G.add_nodes_from([\n",
    "    (track.artist_uri, {'name':track.artist_uri, \"node_type\" : \"artist\"}) for p_name, playlist in playlist_data.items() for track in playlist.tracks.values()   \n",
    "])\n",
    "\n",
    "# adding edges\n",
    "track_edge_list = []\n",
    "album_edge_list = []\n",
    "artist_edge_list = []\n",
    "for p_name, playlist in playlist_data.items():\n",
    "  for track in playlist.tracks.values():\n",
    "    track_edge_list.append((p_name, track.uri))\n",
    "    album_edge_list.append((track.uri, track.album_uri))\n",
    "    artist_edge_list.append((track.uri, track.artist_uri))\n",
    "\n",
    "G.add_edges_from(track_edge_list, edge_types=\"track_in_playlist\")\n",
    "G.add_edges_from(album_edge_list, edge_types=\"track_in_album\")\n",
    "G.add_edges_from(artist_edge_list, edge_types=\"track_by_artist\")\n",
    "\n",
    "print('Num nodes:', G.number_of_nodes(), '. Num edges:', G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'track': 457016, 'album': 192812, 'artist': 79189, 'playlist': 50000})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter([d[\"node_type\"] for (_, d) in G.nodes(data=True)])\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_theme = \"Set1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141817f",
   "metadata": {},
   "source": [
    "# `Graph for Visualization (N=20)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d9afa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9eb2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 2721\n",
      "Edges: 3746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- CONFIG --------\n",
    "NUM_PLAYLISTS_TO_SAMPLE = 20\n",
    "\n",
    "# -------- STEP 1: sample playlists --------\n",
    "playlist_nodes = [\n",
    "    n for n, d in G.nodes(data=True) if d[\"node_type\"] == \"playlist\"\n",
    "]\n",
    "\n",
    "sampled_playlists = random.sample(playlist_nodes, NUM_PLAYLISTS_TO_SAMPLE)\n",
    "\n",
    "sub_nodes = set(sampled_playlists)\n",
    "\n",
    "# -------- STEP 2: playlist → track --------\n",
    "for p in sampled_playlists:\n",
    "    for neigh in G.neighbors(p):\n",
    "        if G.nodes[neigh][\"node_type\"] == \"track\":\n",
    "            sub_nodes.add(neigh)\n",
    "\n",
    "# -------- STEP 3: track → album & artist ONLY --------\n",
    "for t in list(sub_nodes):\n",
    "    if G.nodes[t][\"node_type\"] == \"track\":\n",
    "        for neigh in G.neighbors(t):\n",
    "            if G.nodes[neigh][\"node_type\"] in [\"album\", \"artist\"]:\n",
    "                sub_nodes.add(neigh)\n",
    "\n",
    "# -------- STEP 4: build subgraph --------\n",
    "sub_G_vis = G.subgraph(sub_nodes).copy()\n",
    "\n",
    "print(\"Nodes:\", sub_G_vis.number_of_nodes())\n",
    "print(\"Edges:\", sub_G_vis.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ad396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "100_subgraph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"100_subgraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x740442462b00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(height=\"800px\", width=\"100%\", notebook=True)\n",
    "net.force_atlas_2based()   # fast, stable layout\n",
    "\n",
    "color_map = {\n",
    "    \"playlist\": \"#ff3333\",\n",
    "    \"track\": \"#ff9933\",\n",
    "    \"album\": \"#9933ff\",\n",
    "    \"artist\": \"#999999\"\n",
    "}\n",
    "\n",
    "for node, data in sub_G_vis.nodes(data=True):\n",
    "    net.add_node(\n",
    "        node,\n",
    "        label=data[\"node_type\"],        # node type displayed\n",
    "        title=node,                     # hover shows node ID\n",
    "        color=color_map[data[\"node_type\"]],\n",
    "        size=8\n",
    "    )\n",
    "\n",
    "for u, v in sub_G_vis.edges():\n",
    "    net.add_edge(u, v, color=\"#cccccc\")\n",
    "\n",
    "net.show(\"20_subgraph.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce538d8b",
   "metadata": {},
   "source": [
    "# `Actual Graph to use for training N=10k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8621df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m                 sub_nodes\u001b[38;5;241m.\u001b[39madd(neigh)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# -------- STEP 4: build subgraph --------\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m sub_G \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNodes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sub_G\u001b[38;5;241m.\u001b[39mnumber_of_nodes())\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdges:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sub_G\u001b[38;5;241m.\u001b[39mnumber_of_edges())\n",
      "File \u001b[0;32m~/miniconda3/envs/mlg/lib/python3.10/site-packages/networkx/classes/graph.py:1648\u001b[0m, in \u001b[0;36mGraph.copy\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1646\u001b[0m G\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[1;32m   1647\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from((n, d\u001b[38;5;241m.\u001b[39mcopy()) \u001b[38;5;28;01mfor\u001b[39;00m n, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m-> 1648\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatadict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatadict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m~/miniconda3/envs/mlg/lib/python3.10/site-packages/networkx/classes/graph.py:1024\u001b[0m, in \u001b[0;36mGraph.add_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add all the edges in ebunch_to_add.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m>>> G.add_edges_from(list((5, n) for n in G.nodes))\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ebunch_to_add:\n\u001b[0;32m-> 1024\u001b[0m     ne \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ne \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m   1026\u001b[0m         u, v, dd \u001b[38;5;241m=\u001b[39m e\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "# -------- CONFIG --------\n",
    "NUM_PLAYLISTS_TO_SAMPLE = 10000\n",
    "\n",
    "# -------- STEP 1: sample playlists --------\n",
    "playlist_nodes = [\n",
    "    n for n, d in G.nodes(data=True) if d[\"node_type\"] == \"playlist\"\n",
    "]\n",
    "\n",
    "sampled_playlists = random.sample(playlist_nodes, NUM_PLAYLISTS_TO_SAMPLE)\n",
    "\n",
    "sub_nodes = set(sampled_playlists)\n",
    "\n",
    "# -------- STEP 2: playlist → track --------\n",
    "for p in sampled_playlists:\n",
    "    for neigh in G.neighbors(p):\n",
    "        if G.nodes[neigh][\"node_type\"] == \"track\":\n",
    "            sub_nodes.add(neigh)\n",
    "\n",
    "# -------- STEP 3: track → album & artist ONLY --------\n",
    "for t in list(sub_nodes):\n",
    "    if G.nodes[t][\"node_type\"] == \"track\":\n",
    "        for neigh in G.neighbors(t):\n",
    "            if G.nodes[neigh][\"node_type\"] in [\"album\", \"artist\"]:\n",
    "                sub_nodes.add(neigh)\n",
    "\n",
    "# -------- STEP 4: build subgraph --------\n",
    "sub_G = G.subgraph(sub_nodes).copy()\n",
    "\n",
    "print(\"Nodes:\", sub_G.number_of_nodes())\n",
    "print(\"Edges:\", sub_G.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef71b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'track': 171855, 'album': 81720, 'artist': 35797, 'playlist': 10000})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter([d[\"node_type\"] for (_, d) in sub_G.nodes(data=True)])\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa56a34",
   "metadata": {},
   "source": [
    "# `Save Graph for regular use`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "201ba9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"10K_playlist_graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sub_G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587859d",
   "metadata": {},
   "source": [
    "# `Load Graph `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e3cdeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nodes: 299372 . Num edges: 1001704\n"
     ]
    }
   ],
   "source": [
    "# Note if you've already generated the graph above, you can skip those steps, and simply run set reload to True!\n",
    "reload = True\n",
    "if reload:\n",
    "  sub_G = pickle.load(open(\"10K_playlist_graph.pkl\", \"rb\"))\n",
    "print('Num nodes:', sub_G.number_of_nodes(), '. Num edges:', sub_G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b44c3f",
   "metadata": {},
   "source": [
    "# `Convert to Heterogenous Data Loader in PyG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7af3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def nx_to_heterodata(G):\n",
    "    data = HeteroData()\n",
    "\n",
    "    # ---- 1. Map nodes per type ----\n",
    "    node_maps = {}\n",
    "    for node, attr in G.nodes(data=True):\n",
    "        ntype = attr[\"node_type\"]\n",
    "        if ntype not in node_maps:\n",
    "            node_maps[ntype] = {}\n",
    "        node_maps[ntype][node] = len(node_maps[ntype])\n",
    "\n",
    "    # ---- 2. Set num_nodes per node type ----\n",
    "    for ntype, idmap in node_maps.items():\n",
    "        data[ntype].num_nodes = len(idmap)\n",
    "\n",
    "    # ---- 3. Collect edges grouped by (src_type, rel, dst_type) ----\n",
    "    edge_groups = {}\n",
    "\n",
    "    for u, v, attr in G.edges(data=True):\n",
    "        rel = attr[\"edge_types\"]               # your key\n",
    "        src_t = G.nodes[u][\"node_type\"]\n",
    "        dst_t = G.nodes[v][\"node_type\"]\n",
    "\n",
    "        src_id = node_maps[src_t][u]\n",
    "        dst_id = node_maps[dst_t][v]\n",
    "\n",
    "        edge_type = (src_t, rel, dst_t)\n",
    "\n",
    "        if edge_type not in edge_groups:\n",
    "            edge_groups[edge_type] = []\n",
    "\n",
    "        edge_groups[edge_type].append([src_id, dst_id])\n",
    "\n",
    "    # ---- 4. Write to PyG ----\n",
    "    for etype, edges in edge_groups.items():\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        data[etype].edge_index = edge_index\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "439d8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  track={ num_nodes=171855 },\n",
      "  album={ num_nodes=81720 },\n",
      "  playlist={ num_nodes=10000 },\n",
      "  artist={ num_nodes=35797 },\n",
      "  (track, track_in_playlist, playlist)={ edge_index=[2, 329839] },\n",
      "  (track, track_in_album, album)={ edge_index=[2, 85611] },\n",
      "  (track, track_by_artist, artist)={ edge_index=[2, 87106] },\n",
      "  (album, track_in_album, track)={ edge_index=[2, 86244] },\n",
      "  (playlist, track_in_playlist, track)={ edge_index=[2, 328155] },\n",
      "  (artist, track_by_artist, track)={ edge_index=[2, 84749] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hetero_data = nx_to_heterodata(sub_G)\n",
    "print(hetero_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea606704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('track', 'track_in_playlist', 'playlist'),\n",
       " ('track', 'track_in_album', 'album'),\n",
       " ('track', 'track_by_artist', 'artist'),\n",
       " ('album', 'track_in_album', 'track'),\n",
       " ('playlist', 'track_in_playlist', 'track'),\n",
       " ('artist', 'track_by_artist', 'track')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_types = hetero_data.edge_types\n",
    "edge_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1046308",
   "metadata": {},
   "source": [
    "# `Train/Val/Test Split via RandomLinkSplit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e1b4f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('track', 'track_in_playlist', 'playlist'),\n",
       " ('track', 'track_in_album', 'album'),\n",
       " ('track', 'track_by_artist', 'artist'),\n",
       " ('album', 'track_in_album', 'track'),\n",
       " ('playlist', 'track_in_playlist', 'track'),\n",
       " ('artist', 'track_by_artist', 'track')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0486775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### This is wrong -- need to specify which edge types to split on\n",
    "# convert to train/val/test splits\n",
    "# transform = RandomLinkSplit(\n",
    "#     is_undirected=True,\n",
    "#     add_negative_train_samples=False,\n",
    "#     neg_sampling_ratio=0,\n",
    "#     num_val=0.15, num_test=0.15,\n",
    "#     edge_types=edge_types\n",
    "# )\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    disjoint_train_ratio=0.0,\n",
    "    split_labels=True,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=[('playlist', 'track_in_playlist', 'track')],\n",
    "    rev_edge_types=[('track', 'track_in_playlist', 'playlist')],\n",
    ")\n",
    "\n",
    "\n",
    "train_split, val_split, test_split = transform(hetero_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99717022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  track={ num_nodes=171855 },\n",
       "  album={ num_nodes=81720 },\n",
       "  playlist={ num_nodes=10000 },\n",
       "  artist={ num_nodes=35797 },\n",
       "  (track, track_in_playlist, playlist)={ edge_index=[2, 262525] },\n",
       "  (track, track_in_album, album)={ edge_index=[2, 85611] },\n",
       "  (track, track_by_artist, artist)={ edge_index=[2, 87106] },\n",
       "  (album, track_in_album, track)={ edge_index=[2, 86244] },\n",
       "  (playlist, track_in_playlist, track)={\n",
       "    edge_index=[2, 262525],\n",
       "    pos_edge_label=[262525],\n",
       "    pos_edge_label_index=[2, 262525],\n",
       "  },\n",
       "  (artist, track_by_artist, track)={ edge_index=[2, 84749] }\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5146aa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ..., 171333, 171468, 171623],\n",
       "        [  4555,   1326,   6228,  ...,   9984,   9993,   9994]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_type = hetero_data.edge_types[0]\n",
    "hetero_data[edge_type].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d4b8c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('track', 'track_in_playlist', 'playlist'),\n",
       " [('track', 'track_in_playlist', 'playlist'),\n",
       "  ('track', 'track_in_album', 'album'),\n",
       "  ('track', 'track_by_artist', 'artist'),\n",
       "  ('album', 'track_in_album', 'track'),\n",
       "  ('playlist', 'track_in_playlist', 'track'),\n",
       "  ('artist', 'track_by_artist', 'track')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_type, hetero_data.edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53faf71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward edge: ('playlist', 'track_in_playlist', 'track')\n",
      "Reverse edge: ('track', 'track_in_playlist', 'playlist')\n",
      "Train supervision edges: 262525\n",
      "Val supervision edges:   32815\n",
      "Test supervision edges:  32815\n",
      "Train MP edges (fwd):    262525\n",
      "Train MP edges (rev):    262525\n",
      "Val MP edges (fwd):      262525\n",
      "Val MP edges (rev):      262525\n",
      "Test MP edges (fwd):     295340\n",
      "Test MP edges (rev):     295340\n"
     ]
    }
   ],
   "source": [
    "# Edge types\n",
    "fwd_edge = ('playlist', 'track_in_playlist', 'track')\n",
    "rev_edge = ('track', 'track_in_playlist', 'playlist')\n",
    "\n",
    "def convert_edge_types(split, edge_type):\n",
    "    # Convert message-passing edges\n",
    "    if 'edge_index' in split[edge_type]:\n",
    "        split[edge_type].edge_index = split[edge_type].edge_index.long()\n",
    "\n",
    "    # Convert label edges (used for link prediction)\n",
    "    if \"edge_label_index\" in split[edge_type]:\n",
    "        split[edge_type].edge_label_index = split[edge_type].edge_label_index.long()\n",
    "\n",
    "# Convert FORWARD + REVERSE edges\n",
    "for split in [train_split, val_split, test_split]:\n",
    "    convert_edge_types(split, fwd_edge)\n",
    "    convert_edge_types(split, rev_edge)\n",
    "\n",
    "# Print stats\n",
    "print(\"Forward edge:\", fwd_edge)\n",
    "print(\"Reverse edge:\", rev_edge)\n",
    "\n",
    "print(f\"Train supervision edges: {train_split[fwd_edge].pos_edge_label_index.shape[1]}\")\n",
    "print(f\"Val supervision edges:   {val_split[fwd_edge].pos_edge_label_index.shape[1]}\")\n",
    "print(f\"Test supervision edges:  {test_split[fwd_edge].pos_edge_label_index.shape[1]}\")\n",
    "\n",
    "print(f\"Train MP edges (fwd):    {train_split[fwd_edge].edge_index.shape[1]}\")\n",
    "print(f\"Train MP edges (rev):    {train_split[rev_edge].edge_index.shape[1]}\")\n",
    "\n",
    "print(f\"Val MP edges (fwd):      {val_split[fwd_edge].edge_index.shape[1]}\")\n",
    "print(f\"Val MP edges (rev):      {val_split[rev_edge].edge_index.shape[1]}\")\n",
    "\n",
    "print(f\"Test MP edges (fwd):     {test_split[fwd_edge].edge_index.shape[1]}\")\n",
    "print(f\"Test MP edges (rev):     {test_split[rev_edge].edge_index.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdd9d5",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334aaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCN(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#       Here we adapt the LightGCN model from Torch Geometric for our purposes. We allow\n",
    "#       for customizable convolutional layers, custom embeddings. In addition, we deifne some\n",
    "#       additional custom functions.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_nodes: int,\n",
    "#         embedding_dim: int,\n",
    "#         num_layers: int,\n",
    "#         alpha: Optional[Union[float, Tensor]] = None,\n",
    "#         alpha_learnable = False,\n",
    "#         conv_layer = \"LGC\",\n",
    "#         name = None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         alpha_string = \"alpha\" if alpha_learnable else \"\"\n",
    "#         self.name = f\"LGCN_{conv_layer}_{num_layers}_e{embedding_dim}_nodes{num_nodes}_{alpha_string}\"\n",
    "#         self.num_nodes = num_nodes\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         if alpha_learnable == True:\n",
    "#           alpha_vals = torch.rand(num_layers+1)\n",
    "#           alpha = nn.Parameter(alpha_vals/torch.sum(alpha_vals))\n",
    "#           print(f\"Alpha learnable, initialized to: {alpha.softmax(dim=-1)}\")\n",
    "#         else:\n",
    "#           if alpha is None:\n",
    "#               alpha = 1. / (num_layers + 1)\n",
    "\n",
    "#           if isinstance(alpha, Tensor):\n",
    "#               assert alpha.size(0) == num_layers + 1\n",
    "#           else:\n",
    "#               alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "\n",
    "#         self.register_buffer('alpha', alpha)\n",
    "\n",
    "#         self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "\n",
    "#         # initialize convolutional layers\n",
    "#         self.conv_layer = conv_layer\n",
    "#         if conv_layer == \"LGC\":\n",
    "#           self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
    "#         elif conv_layer == \"GAT\":\n",
    "#           # initialize Graph Attention layer with multiple heads\n",
    "#           # initialize linear layers to aggregate heads\n",
    "#           n_heads = 5\n",
    "#           self.convs = ModuleList(\n",
    "#               [GATConv(in_channels = embedding_dim, out_channels = embedding_dim, heads = n_heads, dropout = 0.5, **kwargs) for _ in range(num_layers)]\n",
    "#           )\n",
    "#           self.linears = ModuleList([Linear(n_heads * embedding_dim, embedding_dim) for _ in range(num_layers)])\n",
    "\n",
    "#         elif conv_layer == \"SAGE\":\n",
    "#           #  initialize GraphSAGE conv\n",
    "#           self.convs = ModuleList(\n",
    "#               [SAGEConv(in_channels = embedding_dim, out_channels = embedding_dim, **kwargs) for _ in range(num_layers)]\n",
    "#           )\n",
    "\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "#         for conv in self.convs:\n",
    "#             conv.reset_parameters()\n",
    "\n",
    "#     def get_embedding(self, edge_index: Adj) -> Tensor:\n",
    "#         x = self.embedding.weight\n",
    "\n",
    "#         weights = self.alpha.softmax(dim=-1)\n",
    "#         out = x * weights[0]\n",
    "\n",
    "#         for i in range(self.num_layers):\n",
    "#             x = self.convs[i](x, edge_index)\n",
    "#             if self.conv_layer == \"GAT\":\n",
    "#               x = self.linears[i](x)\n",
    "#             out = out + x * weights[i + 1]\n",
    "\n",
    "#         return out\n",
    "\n",
    "#     def initialize_embeddings(self, data):\n",
    "#       # initialize with the data node features\n",
    "#         self.embedding.weight.data.copy_(data.node_feature)\n",
    "\n",
    "\n",
    "#     def forward(self, edge_index: Adj,\n",
    "#                 edge_label_index: OptTensor = None) -> Tensor:\n",
    "#         if edge_label_index is None:\n",
    "#             if isinstance(edge_index, SparseTensor):\n",
    "#                 edge_label_index = torch.stack(edge_index.coo()[:2], dim=0)\n",
    "#             else:\n",
    "#                 edge_label_index = edge_index\n",
    "\n",
    "#         out = self.get_embedding(edge_index)\n",
    "\n",
    "#         return self.predict_link_embedding(out, edge_label_index)\n",
    "\n",
    "#     def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
    "#                      prob: bool = False) -> Tensor:\n",
    "\n",
    "#         pred = self(edge_index, edge_label_index).sigmoid()\n",
    "#         return pred if prob else pred.round()\n",
    "\n",
    "#     def predict_link_embedding(self, embed: Adj, edge_label_index: Adj) -> Tensor:\n",
    "\n",
    "#         embed_src = embed[edge_label_index[0]]\n",
    "#         embed_dst = embed[edge_label_index[1]]\n",
    "#         return (embed_src * embed_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "#     def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
    "#                   dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
    "#         out_src = out_dst = self.get_embedding(edge_index)\n",
    "\n",
    "#         if src_index is not None:\n",
    "#             out_src = out_src[src_index]\n",
    "\n",
    "#         if dst_index is not None:\n",
    "#             out_dst = out_dst[dst_index]\n",
    "\n",
    "#         pred = out_src @ out_dst.t()\n",
    "#         top_index = pred.topk(k, dim=-1).indices\n",
    "\n",
    "#         if dst_index is not None:  # Map local top-indices to original indices.\n",
    "#             top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
    "\n",
    "#         return top_index\n",
    "\n",
    "\n",
    "#     def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
    "#                        **kwargs) -> Tensor:\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "#         return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "#     def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
    "#                             lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
    "#         r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "#         Personalized Ranking (BPR) loss.\"\"\"\n",
    "#         loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
    "#         return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)\n",
    "\n",
    "#     def bpr_loss(self, pos_scores, neg_scores):\n",
    "#       return - torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
    "#                 f'{self.embedding_dim}, num_layers={self.num_layers})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c383c8f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 394\u001b[0m\n\u001b[1;32m    391\u001b[0m pos_pred \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Sample negatives\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m neg_edge_index, neg_edge_label \u001b[38;5;241m=\u001b[39m \u001b[43msample_negatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m neg_pred \u001b[38;5;241m=\u001b[39m model(batch, neg_edge_index)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 32\u001b[0m, in \u001b[0;36msample_negatives\u001b[0;34m(batch, neg_ratio)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(neg_edges) \u001b[38;5;241m<\u001b[39m num_neg:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Random playlist and track from batch\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     p_idx \u001b[38;5;241m=\u001b[39m batch_playlists[torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(batch_playlists), (\u001b[38;5;241m1\u001b[39m,))]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 32\u001b[0m     t_idx \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_tracks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_tracks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Keep if not a positive edge\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (p_idx, t_idx) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pos_set:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList, Linear\n",
    "from torch_geometric.nn import RGCNConv, SAGEConv, HGTConv, to_hetero\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from typing import Optional, Union, Dict, List\n",
    "\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Base heterogeneous GNN model for playlist-track recommendation.\n",
    "    Supports RGCN, GraphSAGE-H, and HGT architectures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata: tuple,  # (node_types, edge_types)\n",
    "        num_nodes_dict: Dict[str, int],  # {'playlist': 10000, 'track': 171855, ...}\n",
    "        embedding_dim: int,\n",
    "        num_layers: int,\n",
    "        model_type: str = \"RGCN\",  # \"RGCN\", \"SAGE\", \"HGT\"\n",
    "        alpha: Optional[Union[float, Tensor]] = None,\n",
    "        heads: int = 4,  # For HGT\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.metadata = metadata\n",
    "        self.node_types = metadata[0]\n",
    "        self.edge_types = metadata[1]\n",
    "        self.num_nodes_dict = num_nodes_dict\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        # Alpha weighting for layer combinations\n",
    "        if alpha is None:\n",
    "            alpha = 1. / (num_layers + 1)\n",
    "        if isinstance(alpha, Tensor):\n",
    "            assert alpha.size(0) == num_layers + 1\n",
    "        else:\n",
    "            alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        \n",
    "        # Create embeddings for each node type\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            node_type: Embedding(num_nodes, embedding_dim)\n",
    "            for node_type, num_nodes in num_nodes_dict.items()\n",
    "        })\n",
    "        \n",
    "        # Create convolutional layers based on model type\n",
    "        self.convs = ModuleList()\n",
    "        \n",
    "        if model_type == \"RGCN\":\n",
    "            # Relational GCN - handles different edge types with relation-specific weights\n",
    "            num_relations = len(self.edge_types)\n",
    "            for _ in range(num_layers):\n",
    "                self.convs.append(\n",
    "                    RGCNConv(\n",
    "                        embedding_dim, \n",
    "                        embedding_dim, \n",
    "                        num_relations=num_relations,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        elif model_type == \"SAGE\":\n",
    "            # GraphSAGE for heterogeneous graphs (will be wrapped with to_hetero)\n",
    "            for _ in range(num_layers):\n",
    "                self.convs.append(\n",
    "                    SAGEConv(embedding_dim, embedding_dim, **kwargs)\n",
    "                )\n",
    "        \n",
    "        elif model_type == \"HGT\":\n",
    "            # Heterogeneous Graph Transformer\n",
    "            for _ in range(num_layers):\n",
    "                self.convs.append(\n",
    "                    HGTConv(\n",
    "                        embedding_dim,\n",
    "                        embedding_dim,\n",
    "                        metadata,\n",
    "                        heads=heads,\n",
    "                        **kwargs\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for emb in self.embeddings.values():\n",
    "            torch.nn.init.xavier_uniform_(emb.weight)\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def get_embedding(self, batch) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Get node embeddings for heterogeneous batch.\n",
    "        \n",
    "        Args:\n",
    "            batch: HeteroData batch\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of embeddings for each node type\n",
    "        \"\"\"\n",
    "        # Initialize with learned embeddings using node_id (global IDs in batch)\n",
    "        # This indexes the FULL embedding table with batch's global node IDs\n",
    "        x_dict = {\n",
    "            node_type: self.embeddings[node_type](batch[node_type].n_id)\n",
    "            for node_type in self.node_types\n",
    "        }\n",
    "        \n",
    "        weights = self.alpha.softmax(dim=-1)\n",
    "        \n",
    "        # Initialize output with weighted initial embeddings\n",
    "        out_dict = {\n",
    "            node_type: x * weights[0]\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "        \n",
    "        # Message passing\n",
    "        if self.model_type == \"RGCN\":\n",
    "            # RGCN uses homogeneous edge_index with edge_type\n",
    "            edge_index, edge_type = self._to_homogeneous(batch)\n",
    "            \n",
    "            # Convert node features to homogeneous\n",
    "            x = torch.cat([x_dict[nt] for nt in self.node_types], dim=0)\n",
    "            \n",
    "            for i, conv in enumerate(self.convs):\n",
    "                x = conv(x, edge_index, edge_type)\n",
    "                x = x.relu()\n",
    "                \n",
    "                # Add to output with weight\n",
    "                start_idx = 0\n",
    "                for node_type in self.node_types:\n",
    "                    num_nodes = x_dict[node_type].size(0)\n",
    "                    out_dict[node_type] = out_dict[node_type] + x[start_idx:start_idx+num_nodes] * weights[i + 1]\n",
    "                    start_idx += num_nodes\n",
    "        \n",
    "        elif self.model_type == \"SAGE\":\n",
    "            # GraphSAGE with heterogeneous message passing\n",
    "            edge_index_dict = batch.edge_index_dict\n",
    "            \n",
    "            for i, conv in enumerate(self.convs):\n",
    "                # Apply convolution for each edge type\n",
    "                x_dict_new = {}\n",
    "                for node_type in self.node_types:\n",
    "                    # Aggregate messages from all edge types involving this node\n",
    "                    msgs = []\n",
    "                    for edge_type in self.edge_types:\n",
    "                        src, rel, dst = edge_type\n",
    "                        if dst == node_type and edge_type in edge_index_dict:\n",
    "                            edge_index = edge_index_dict[edge_type]\n",
    "                            msg = conv((x_dict[src], x_dict[dst]), edge_index)\n",
    "                            msgs.append(msg)\n",
    "                    \n",
    "                    if msgs:\n",
    "                        x_dict_new[node_type] = torch.stack(msgs).mean(dim=0).relu()\n",
    "                    else:\n",
    "                        x_dict_new[node_type] = x_dict[node_type]\n",
    "                    \n",
    "                    # Add to output with weight\n",
    "                    out_dict[node_type] = out_dict[node_type] + x_dict_new[node_type] * weights[i + 1]\n",
    "                \n",
    "                x_dict = x_dict_new\n",
    "        \n",
    "        elif self.model_type == \"HGT\":\n",
    "            # Heterogeneous Graph Transformer\n",
    "            edge_index_dict = batch.edge_index_dict\n",
    "            \n",
    "            for i, conv in enumerate(self.convs):\n",
    "                x_dict = conv(x_dict, edge_index_dict)\n",
    "                x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "                \n",
    "                # Add to output with weight\n",
    "                for node_type in self.node_types:\n",
    "                    out_dict[node_type] = out_dict[node_type] + x_dict[node_type] * weights[i + 1]\n",
    "        \n",
    "        return out_dict\n",
    "\n",
    "    def _to_homogeneous(self, batch):\n",
    "        \"\"\"Convert heterogeneous batch to homogeneous for RGCN.\"\"\"\n",
    "        edge_indices = []\n",
    "        edge_types = []\n",
    "        \n",
    "        # Create node offset mapping\n",
    "        node_offset = {}\n",
    "        offset = 0\n",
    "        for node_type in self.node_types:\n",
    "            node_offset[node_type] = offset\n",
    "            offset += batch[node_type].num_nodes\n",
    "        \n",
    "        # Collect all edges with their types\n",
    "        for edge_type_idx, edge_type in enumerate(self.edge_types):\n",
    "            src_type, _, dst_type = edge_type\n",
    "            if edge_type in batch.edge_index_dict:\n",
    "                edge_index = batch.edge_index_dict[edge_type]\n",
    "                # Offset node indices\n",
    "                edge_index_offset = edge_index.clone()\n",
    "                edge_index_offset[0] += node_offset[src_type]\n",
    "                edge_index_offset[1] += node_offset[dst_type]\n",
    "                \n",
    "                edge_indices.append(edge_index_offset)\n",
    "                edge_types.append(torch.full((edge_index.size(1),), edge_type_idx, dtype=torch.long))\n",
    "        \n",
    "        edge_index = torch.cat(edge_indices, dim=1)\n",
    "        edge_type = torch.cat(edge_types, dim=0)\n",
    "        \n",
    "        return edge_index.to(batch['playlist'].n_id.device), edge_type.to(batch['playlist'].n_id.device)\n",
    "\n",
    "    def forward(self, batch, edge_label_index: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for link prediction.\n",
    "        \n",
    "        Args:\n",
    "            batch: HeteroData batch\n",
    "            edge_label_index: [2, num_edges] edges to predict (playlist, track)\n",
    "            \n",
    "        Returns:\n",
    "            Prediction scores for edges\n",
    "        \"\"\"\n",
    "        if edge_label_index is None:\n",
    "            edge_label_index = batch['playlist', 'track_in_playlist', 'track'].pos_edge_label_index\n",
    "        \n",
    "        out_dict = self.get_embedding(batch)\n",
    "        \n",
    "        return self.predict_link_embedding(out_dict, edge_label_index, batch)\n",
    "\n",
    "    def predict_link_embedding(self, embed_dict: Dict[str, Tensor], edge_label_index: Tensor, \n",
    "                              batch=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict link scores using dot product.\n",
    "        \n",
    "        Args:\n",
    "            embed_dict: Dictionary of embeddings per node type (LOCAL batch embeddings)\n",
    "            edge_label_index: [2, num_edges] (playlist_global_idx, track_global_idx)\n",
    "            batch: HeteroData batch for global-to-local mapping\n",
    "            \n",
    "        Returns:\n",
    "            scores: [num_edges] prediction scores\n",
    "        \"\"\"\n",
    "        # embed_dict contains embeddings indexed by LOCAL batch indices\n",
    "        # edge_label_index contains GLOBAL node IDs\n",
    "        # We need to map global IDs to local batch indices\n",
    "        \n",
    "        if batch is None:\n",
    "            # No batch provided, assume edge_label_index has local indices\n",
    "            playlist_emb = embed_dict['playlist']\n",
    "            track_emb = embed_dict['track']\n",
    "            embed_src = playlist_emb[edge_label_index[0]]\n",
    "            embed_dst = track_emb[edge_label_index[1]]\n",
    "        else:\n",
    "            # Get full embedding tables and index directly by global ID\n",
    "            playlist_emb_full = self.embeddings['playlist'].weight\n",
    "            track_emb_full = self.embeddings['track'].weight\n",
    "            \n",
    "            # But we need the message-passed embeddings from embed_dict\n",
    "            # Solution: index the full embeddings, then apply the final layer output\n",
    "            # Actually, let's just use batch n_id to map properly\n",
    "            \n",
    "            playlist_n_id = batch['playlist'].n_id\n",
    "            track_n_id = batch['track'].n_id\n",
    "            \n",
    "            # Create mapping\n",
    "            # Find local index for each global ID in edge_label_index\n",
    "            playlist_global_ids = edge_label_index[0]\n",
    "            track_global_ids = edge_label_index[1]\n",
    "            \n",
    "            # Use searchsorted for efficient mapping (requires sorted n_id)\n",
    "            playlist_sorted, playlist_sort_idx = playlist_n_id.sort()\n",
    "            track_sorted, track_sort_idx = track_n_id.sort()\n",
    "            \n",
    "            playlist_pos = torch.searchsorted(playlist_sorted, playlist_global_ids)\n",
    "            track_pos = torch.searchsorted(track_sorted, track_global_ids)\n",
    "            \n",
    "            # Get actual local indices\n",
    "            playlist_local = playlist_sort_idx[playlist_pos]\n",
    "            track_local = track_sort_idx[track_pos]\n",
    "            \n",
    "            # Index embeddings\n",
    "            embed_src = embed_dict['playlist'][playlist_local]\n",
    "            embed_dst = embed_dict['track'][track_local]\n",
    "        \n",
    "        return (embed_src * embed_dst).sum(dim=-1)\n",
    "\n",
    "    def predict_link(self, batch, edge_label_index: OptTensor = None, prob: bool = False) -> Tensor:\n",
    "        \"\"\"Predict links with optional probability output.\"\"\"\n",
    "        pred = self(batch, edge_label_index).sigmoid()\n",
    "        return pred if prob else pred.round()\n",
    "\n",
    "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"Binary cross-entropy loss for link prediction.\"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "    def bpr_loss(self, pos_scores: Tensor, neg_scores: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Bayesian Personalized Ranking loss.\n",
    "        Handles multiple negatives per positive.\n",
    "        \"\"\"\n",
    "        num_pos = pos_scores.size(0)\n",
    "        num_neg = neg_scores.size(0)\n",
    "        \n",
    "        if num_pos == num_neg:\n",
    "            # Equal number of positives and negatives\n",
    "            return -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "        else:\n",
    "            # Multiple negatives per positive - expand positives\n",
    "            neg_ratio = num_neg // num_pos\n",
    "            pos_scores_expanded = pos_scores.repeat_interleave(neg_ratio)\n",
    "            return -torch.log(torch.sigmoid(pos_scores_expanded - neg_scores)).mean()\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
    "                           lambda_reg: float = 1e-4) -> Tensor:\n",
    "        \"\"\"BPR loss with L2 regularization.\"\"\"\n",
    "        bpr = self.bpr_loss(pos_edge_rank, neg_edge_rank)\n",
    "        \n",
    "        # L2 regularization on embeddings\n",
    "        reg_loss = 0\n",
    "        for emb in self.embeddings.values():\n",
    "            reg_loss += emb.weight.norm(2).pow(2)\n",
    "        \n",
    "        return bpr + lambda_reg * reg_loss\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        total_nodes = sum(self.num_nodes_dict.values())\n",
    "        return (f'{self.__class__.__name__}({self.model_type}, '\n",
    "                f'nodes={total_nodes}, '\n",
    "                f'emb_dim={self.embedding_dim}, '\n",
    "                f'layers={self.num_layers})')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Usage Examples\n",
    "# ============================================================================\n",
    "\n",
    "def create_model_rgcn(metadata, num_nodes_dict, embedding_dim=128, num_layers=3):\n",
    "    \"\"\"Create RGCN model.\"\"\"\n",
    "    return HeteroGNN(\n",
    "        metadata=metadata,\n",
    "        num_nodes_dict=num_nodes_dict,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        model_type=\"RGCN\"\n",
    "    )\n",
    "\n",
    "def create_model_sage(metadata, num_nodes_dict, embedding_dim=128, num_layers=3):\n",
    "    \"\"\"Create GraphSAGE-H model.\"\"\"\n",
    "    return HeteroGNN(\n",
    "        metadata=metadata,\n",
    "        num_nodes_dict=num_nodes_dict,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        model_type=\"SAGE\"\n",
    "    )\n",
    "\n",
    "def create_model_hgt(metadata, num_nodes_dict, embedding_dim=128, num_layers=3, heads=4):\n",
    "    \"\"\"Create HGT model.\"\"\"\n",
    "    return HeteroGNN(\n",
    "        metadata=metadata,\n",
    "        num_nodes_dict=num_nodes_dict,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        model_type=\"HGT\",\n",
    "        heads=heads\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "metadata = train_split.metadata()\n",
    "num_nodes_dict = {\n",
    "    'playlist': 10000,\n",
    "    'track': 171855,\n",
    "    'album': 81720,\n",
    "    'artist': 35797\n",
    "}\n",
    "\n",
    "# Create model\n",
    "device = \"cpu\"\n",
    "model = create_model_rgcn(metadata, num_nodes_dict, embedding_dim=128, num_layers=3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    pos_pred = model(batch)\n",
    "    \n",
    "    # Sample negatives\n",
    "    neg_edge_index, neg_edge_label = sample_negatives(batch)\n",
    "    neg_pred = model(batch, neg_edge_index)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = model.bpr_loss(pos_pred, neg_pred)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73ddaf",
   "metadata": {},
   "source": [
    "# `v19 works`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4176a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6932\n",
      "Pos pred range: [0.0002, 0.0044]\n",
      "Neg pred range: [0.0002, 0.0042]\n",
      "Pos pred mean: 0.0012, Neg pred mean: 0.0013\n",
      "Total gradient norm: 0.0024\n"
     ]
    }
   ],
   "source": [
    "metadata = train_split.metadata()\n",
    "num_nodes_dict = {\n",
    "    'playlist': 10000,\n",
    "    'track': 171855,\n",
    "    'album': 81720,\n",
    "    'artist': 35797\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = create_model_rgcn(metadata, num_nodes_dict, embedding_dim=128, num_layers=3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop sanity check\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    optimizer.zero_grad()  # ADD THIS! You forgot to zero gradients\n",
    "    \n",
    "    # Forward pass\n",
    "    pos_pred = model(batch)\n",
    "    \n",
    "    # Sample negatives\n",
    "    neg_edge_index, neg_edge_label = sample_negatives(batch)\n",
    "    neg_pred = model(batch, neg_edge_index)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = model.bpr_loss(pos_pred, neg_pred)\n",
    "    \n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(f\"Pos pred range: [{pos_pred.min():.4f}, {pos_pred.max():.4f}]\")\n",
    "    print(f\"Neg pred range: [{neg_pred.min():.4f}, {neg_pred.max():.4f}]\")\n",
    "    print(f\"Pos pred mean: {pos_pred.mean():.4f}, Neg pred mean: {neg_pred.mean():.4f}\")\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradients are flowing\n",
    "    total_grad_norm = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            total_grad_norm += param.grad.norm().item()\n",
    "    print(f\"Total gradient norm: {total_grad_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7dd7fa",
   "metadata": {},
   "source": [
    "# `Bayesian Personalized Ranking Loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3cc45e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(_Loss):\n",
    "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
    "    observed entry to be higher than its unobserved counterparts\n",
    "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
    "\n",
    "    .. math::\n",
    "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
    "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
    "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
    "\n",
    "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
    "    We compute the mean BPR loss for simplicity.\n",
    "\n",
    "    Args:\n",
    "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
    "            (default: 0).\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`torch.nn.modules.loss._Loss` class.\n",
    "    \"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
    "            in the :obj:`negatives` entry should correspond to the same\n",
    "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).sum()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2289bac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5923,   8685,    641,  ...,    776,   7743,   5947],\n",
       "        [122716, 166856, 140277,  ...,  54836, 161325, 105642]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split[('playlist', 'track_in_playlist', 'track')].pos_edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f90eece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  track={ num_nodes=171855 },\n",
       "  album={ num_nodes=81720 },\n",
       "  playlist={ num_nodes=10000 },\n",
       "  artist={ num_nodes=35797 },\n",
       "  (track, track_in_playlist, playlist)={ edge_index=[2, 262525] },\n",
       "  (track, track_in_album, album)={ edge_index=[2, 85611] },\n",
       "  (track, track_by_artist, artist)={ edge_index=[2, 87106] },\n",
       "  (album, track_in_album, track)={ edge_index=[2, 86244] },\n",
       "  (playlist, track_in_playlist, track)={\n",
       "    edge_index=[2, 262525],\n",
       "    pos_edge_label=[262525],\n",
       "    pos_edge_label_index=[2, 262525],\n",
       "  },\n",
       "  (artist, track_by_artist, track)={ edge_index=[2, 84749] },\n",
       "  (track, rev_track_in_playlist, playlist)={}\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f2a1282",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NodeStorage' object has no attribute 'node_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     neg_edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((playlists\u001b[38;5;241m.\u001b[39mrepeat_interleave(num_neg),\n\u001b[1;32m     11\u001b[0m                                   neg_tracks\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m neg_edge_index\n\u001b[0;32m---> 14\u001b[0m neg_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43msample_negatives_hetero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m neg_edge_index\n",
      "Cell \u001b[0;32mIn[62], line 5\u001b[0m, in \u001b[0;36msample_negatives_hetero\u001b[0;34m(batch, num_neg)\u001b[0m\n\u001b[1;32m      2\u001b[0m playlists \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaylist\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_in_playlist\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpos_edge_label_index[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m pos_tracks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaylist\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_in_playlist\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpos_edge_label_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m all_tracks \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_id\u001b[49m  \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# e.g., [103, 33, 5002, 22331, 655] local track indices inside sampled subgraph\u001b[39;00m\n\u001b[1;32m      8\u001b[0m neg_tracks \u001b[38;5;241m=\u001b[39m all_tracks[torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_tracks), (\u001b[38;5;28mlen\u001b[39m(playlists), num_neg))]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlg/lib/python3.10/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NodeStorage' object has no attribute 'node_id'"
     ]
    }
   ],
   "source": [
    "def sample_negatives_hetero(batch, num_neg=5):\n",
    "    playlists = batch['playlist','track_in_playlist','track'].pos_edge_label_index[0]\n",
    "    pos_tracks = batch['playlist','track_in_playlist','track'].pos_edge_label_index[1]\n",
    "\n",
    "    all_tracks = batch['track'].node_id  \n",
    "    # e.g., [103, 33, 5002, 22331, 655] local track indices inside sampled subgraph\n",
    "\n",
    "    neg_tracks = all_tracks[torch.randint(0, len(all_tracks), (len(playlists), num_neg))]\n",
    "\n",
    "    neg_edge_index = torch.stack((playlists.repeat_interleave(num_neg),\n",
    "                                  neg_tracks.reshape(-1)), dim=0)\n",
    "\n",
    "    return neg_edge_index\n",
    "neg_edge_index = sample_negatives_hetero(train_split, num_neg=1)\n",
    "neg_edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272becef",
   "metadata": {},
   "source": [
    "# `Negative Sampling for Heterogeneous Graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da2e8c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['track', 'album', 'playlist', 'artist'],\n",
       " [('track', 'track_in_playlist', 'playlist'),\n",
       "  ('track', 'track_in_album', 'album'),\n",
       "  ('track', 'track_by_artist', 'artist'),\n",
       "  ('album', 'track_in_album', 'track'),\n",
       "  ('playlist', 'track_in_playlist', 'track'),\n",
       "  ('artist', 'track_by_artist', 'track')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a92991d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('playlist', tensor([   0,    1,    2,  ..., 9997, 9998, 9999]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_nodes = ('playlist', torch.arange(train_split['playlist'].num_nodes))\n",
    "train_input_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f02d1fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('playlist', 'track_in_playlist', 'track'): [20, 10],\n",
       " ('track', 'track_in_playlist', 'playlist'): [20, 10],\n",
       " ('track', 'track_in_album', 'album'): [10, 5],\n",
       " ('album', 'track_in_album', 'track'): [10, 5],\n",
       " ('track', 'track_by_artist', 'artist'): [10, 5],\n",
       " ('artist', 'track_by_artist', 'track'): [10, 5]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neighbors = {\n",
    "    ('playlist','track_in_playlist','track'): [20, 10],  \n",
    "    ('track','track_in_playlist','playlist'): [20, 10],  \n",
    "\n",
    "    ('track','track_in_album','album'): [10, 5],\n",
    "    ('album','track_in_album','track'): [10, 5],\n",
    "\n",
    "    ('track','track_by_artist','artist'): [10, 5],\n",
    "    ('artist','track_by_artist','track'): [10, 5],\n",
    "}\n",
    "num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87b60e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    train_split,\n",
    "    num_neighbors=num_neighbors,\n",
    "    input_nodes=train_input_nodes,\n",
    "    batch_size=512,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3d490ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  track={\n",
      "    num_nodes=5447,\n",
      "    n_id=[5447],\n",
      "    num_sampled_nodes=[3],\n",
      "  },\n",
      "  album={\n",
      "    num_nodes=2919,\n",
      "    n_id=[2919],\n",
      "    num_sampled_nodes=[3],\n",
      "  },\n",
      "  playlist={\n",
      "    num_nodes=6952,\n",
      "    n_id=[6952],\n",
      "    num_sampled_nodes=[3],\n",
      "    input_id=[512],\n",
      "    batch_size=512,\n",
      "  },\n",
      "  artist={\n",
      "    num_nodes=2045,\n",
      "    n_id=[2045],\n",
      "    num_sampled_nodes=[3],\n",
      "  },\n",
      "  (track, track_in_playlist, playlist)={\n",
      "    edge_index=[2, 6829],\n",
      "    e_id=[6829],\n",
      "    num_sampled_edges=[2],\n",
      "  },\n",
      "  (track, track_in_album, album)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[2],\n",
      "  },\n",
      "  (track, track_by_artist, artist)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[2],\n",
      "  },\n",
      "  (album, track_in_album, track)={\n",
      "    edge_index=[2, 3702],\n",
      "    e_id=[3702],\n",
      "    num_sampled_edges=[2],\n",
      "  },\n",
      "  (playlist, track_in_playlist, track)={\n",
      "    edge_index=[2, 30067],\n",
      "    pos_edge_label=[30067],\n",
      "    pos_edge_label_index=[2, 30067],\n",
      "    e_id=[30067],\n",
      "    num_sampled_edges=[2],\n",
      "  },\n",
      "  (artist, track_by_artist, track)={\n",
      "    edge_index=[2, 3748],\n",
      "    e_id=[3748],\n",
      "    num_sampled_edges=[2],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc71a3",
   "metadata": {},
   "source": [
    "# `Negative Sampling for Heterogenous Graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc6c7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def sample_negatives(batch, neg_ratio=3):\n",
    "    \"\"\"\n",
    "    Sample random negative edges for playlist-track link prediction.\n",
    "    \n",
    "    Args:\n",
    "        batch: HeteroData batch from neighbor loader\n",
    "        neg_ratio: Number of negatives per positive edge (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        neg_edge_index: [2, num_neg] negative edges (playlist_idx, track_idx)\n",
    "        neg_edge_label: [num_neg] all zeros\n",
    "    \"\"\"\n",
    "    # Get positive edges and batch nodes\n",
    "    pos_edge_index = batch['playlist', 'track_in_playlist', 'track'].pos_edge_label_index\n",
    "    batch_playlists = batch['playlist'].n_id\n",
    "    batch_tracks = batch['track'].n_id\n",
    "    \n",
    "    num_pos = pos_edge_index.size(1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "    \n",
    "    # Create set of positive edges for fast lookup\n",
    "    pos_set = set(zip(pos_edge_index[0].tolist(), pos_edge_index[1].tolist()))\n",
    "    \n",
    "    # Sample negatives\n",
    "    neg_edges = []\n",
    "    while len(neg_edges) < num_neg:\n",
    "        # Random playlist and track from batch\n",
    "        p_idx = batch_playlists[torch.randint(0, len(batch_playlists), (1,))].item()\n",
    "        t_idx = batch_tracks[torch.randint(0, len(batch_tracks), (1,))].item()\n",
    "        \n",
    "        # Keep if not a positive edge\n",
    "        if (p_idx, t_idx) not in pos_set:\n",
    "            neg_edges.append([p_idx, t_idx])\n",
    "    \n",
    "    neg_edge_index = torch.tensor(neg_edges, dtype=torch.long).t().to(pos_edge_index.device)\n",
    "    neg_edge_label = torch.zeros(num_neg, dtype=torch.float, device=pos_edge_index.device)\n",
    "    \n",
    "    return neg_edge_index, neg_edge_label\n",
    "neg_edge_index, neg_edge_label = sample_negatives(batch, neg_ratio=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1a4e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def sample_hard_negatives(batch, model, device=None, batch_size=500, frac_sample=1.0):\n",
    "    \"\"\"\n",
    "    Sample hard negative edges based on model embeddings.\n",
    "    \n",
    "    Args:\n",
    "        batch: HeteroData batch from neighbor loader\n",
    "        model: Your GNN model with get_embedding() method\n",
    "        device: Device to use (default: same as batch)\n",
    "        batch_size: Batch size for scoring (default: 500)\n",
    "        frac_sample: Fraction of tracks to consider for sampling (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        neg_edge_index: [2, num_neg] hard negative edges\n",
    "        neg_edge_label: [num_neg] all zeros\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = batch['playlist'].n_id.device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings from model using the batch\n",
    "        embed_dict = model.get_embedding(batch)\n",
    "        \n",
    "        # Extract playlist and track embeddings (LOCAL batch embeddings)\n",
    "        playlist_emb = embed_dict['playlist'].to(device)\n",
    "        track_emb = embed_dict['track'].to(device)\n",
    "        \n",
    "        # Get positive edges (GLOBAL IDs)\n",
    "        pos_edge_index = batch['playlist', 'track_in_playlist', 'track'].pos_edge_label_index\n",
    "        positive_playlists_global = pos_edge_index[0]\n",
    "        positive_tracks_global = pos_edge_index[1]\n",
    "        num_edges = positive_playlists_global.size(0)\n",
    "        \n",
    "        # Get batch node mappings\n",
    "        batch_playlists = batch['playlist'].n_id\n",
    "        batch_tracks = batch['track'].n_id\n",
    "        num_batch_playlists = len(batch_playlists)\n",
    "        num_batch_tracks = len(batch_tracks)\n",
    "        \n",
    "        # Create reverse mappings: global_id → local_idx\n",
    "        playlist_map = {global_id.item(): local_idx for local_idx, global_id in enumerate(batch_playlists.cpu())}\n",
    "        track_map = {global_id.item(): local_idx for local_idx, global_id in enumerate(batch_tracks.cpu())}\n",
    "        \n",
    "        # Map positive edges from global to local indices\n",
    "        pos_playlists_local = torch.tensor(\n",
    "            [playlist_map[pid.item()] for pid in positive_playlists_global.cpu()],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "        pos_tracks_local = torch.tensor(\n",
    "            [track_map[tid.item()] for tid in positive_tracks_global.cpu()],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Create positive edge mask (LOCAL indices)\n",
    "        positive_mask = torch.zeros(num_batch_playlists, num_batch_tracks, device=device, dtype=torch.bool)\n",
    "        positive_mask[pos_playlists_local, pos_tracks_local] = True\n",
    "        \n",
    "        neg_edges_list = []\n",
    "        neg_edge_label_list = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, num_edges, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_edges)\n",
    "            \n",
    "            # Get local playlist indices for this batch\n",
    "            playlists_local_batch = pos_playlists_local[batch_start:batch_end]\n",
    "            \n",
    "            # Compute similarity scores (using LOCAL embeddings)\n",
    "            batch_scores = torch.matmul(\n",
    "                playlist_emb[playlists_local_batch], \n",
    "                track_emb.t()\n",
    "            )\n",
    "            \n",
    "            # Mask out positive edges\n",
    "            batch_scores[positive_mask[playlists_local_batch]] = -float(\"inf\")\n",
    "            \n",
    "            # Select top-k highest scoring negative edges\n",
    "            k = int(frac_sample * 0.99 * num_batch_tracks)\n",
    "            k = max(1, k)  # Ensure at least 1\n",
    "            _, top_indices_local = torch.topk(batch_scores, k, dim=1)\n",
    "            \n",
    "            # Randomly select one from top-k for each playlist\n",
    "            selected_indices = torch.randint(0, k, size=(batch_end - batch_start,), device=device)\n",
    "            top_tracks_local = top_indices_local[torch.arange(batch_end - batch_start), selected_indices]\n",
    "            \n",
    "            # Map local indices back to global for output\n",
    "            playlists_global_batch = positive_playlists_global[batch_start:batch_end]\n",
    "            tracks_global_batch = batch_tracks[top_tracks_local]\n",
    "            \n",
    "            # Create negative edges (GLOBAL IDs)\n",
    "            neg_edges_batch = torch.stack(\n",
    "                (playlists_global_batch, tracks_global_batch), dim=0\n",
    "            )\n",
    "            neg_edge_label_batch = torch.zeros(neg_edges_batch.shape[1], device=device)\n",
    "            \n",
    "            neg_edges_list.append(neg_edges_batch)\n",
    "            neg_edge_label_list.append(neg_edge_label_batch)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        neg_edge_index = torch.cat(neg_edges_list, dim=1)\n",
    "        neg_edge_label = torch.cat(neg_edge_label_list)\n",
    "        \n",
    "        return neg_edge_index, neg_edge_label\n",
    "\n",
    "\n",
    "# Usage:\n",
    "neg_edge_index, neg_edge_label = sample_hard_negatives(batch, model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SAMPLING NEGATIVES EDGES FOR HOMOGENEOUS GRAPHS - ORIGINAL CODE\n",
    "\n",
    "\n",
    "# def sample_negative_edges_nocheck(data, num_playlists, num_tracks, device = None):\n",
    "#   # note computationally inefficient to check that these are indeed negative edges\n",
    "#   playlists = data.edge_label_index[0, :]\n",
    "#   tracks = torch.randint(num_playlists, num_playlists + num_tracks - 1, size = data.edge_label_index[1, :].size())\n",
    "\n",
    "#   if playlists.get_device() != -1: # on gpu\n",
    "#     tracks = tracks.to(device)\n",
    "\n",
    "#   neg_edge_index = torch.stack((playlists, tracks), dim = 0)\n",
    "#   neg_edge_label = torch.zeros(neg_edge_index.shape[1])\n",
    "\n",
    "#   if neg_edge_index.get_device() != -1: # on gpu\n",
    "#     neg_edge_label = neg_edge_label.to(device)\n",
    "\n",
    "#   return neg_edge_index, neg_edge_label\n",
    "\n",
    "# def sample_negative_edges(data, num_playlists, num_tracks, device=None):\n",
    "#     positive_playlists, positive_tracks = data.edge_label_index\n",
    "\n",
    "#     # Create a mask tensor with the shape (num_playlists, num_tracks)\n",
    "#     mask = torch.zeros(num_playlists, num_tracks, device=device, dtype=torch.bool)\n",
    "#     mask[positive_playlists, positive_tracks - num_playlists] = True\n",
    "\n",
    "#     # Flatten the mask tensor and get the indices of the negative edges\n",
    "#     flat_mask = mask.flatten()\n",
    "#     negative_indices = torch.where(~flat_mask)[0]\n",
    "\n",
    "#     # Sample negative edges from the negative_indices tensor\n",
    "#     sampled_negative_indices = negative_indices[\n",
    "#         torch.randint(0, negative_indices.size(0), size=(positive_playlists.size(0),), device=device)\n",
    "#     ]\n",
    "\n",
    "#     # Convert the indices back to playlists and tracks tensors\n",
    "#     playlists = torch.floor_divide(sampled_negative_indices, num_tracks)\n",
    "#     tracks = torch.remainder(sampled_negative_indices, num_tracks)\n",
    "#     tracks = tracks + num_playlists\n",
    "\n",
    "#     neg_edge_index = torch.stack((playlists, tracks), dim=0)\n",
    "#     neg_edge_label = torch.zeros(neg_edge_index.shape[1], device=device)\n",
    "\n",
    "#     return neg_edge_index, neg_edge_label\n",
    "\n",
    "# def sample_hard_negative_edges(data, model, num_playlists, num_tracks, device=None, batch_size=500, frac_sample = 1):\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model.get_embedding(data.edge_index)\n",
    "#         playlists_embeddings = embeddings[:num_playlists].to(device)\n",
    "#         tracks_embeddings = embeddings[num_playlists:].to(device)\n",
    "\n",
    "#     positive_playlists, positive_tracks = data.edge_label_index\n",
    "#     num_edges = positive_playlists.size(0)\n",
    "\n",
    "#     # Create a boolean mask for all the positive edges\n",
    "#     positive_mask = torch.zeros(num_playlists, num_tracks, device=device, dtype=torch.bool)\n",
    "#     positive_mask[positive_playlists, positive_tracks - num_playlists] = True\n",
    "\n",
    "#     neg_edges_list = []\n",
    "#     neg_edge_label_list = []\n",
    "\n",
    "#     for batch_start in range(0, num_edges, batch_size):\n",
    "#         batch_end = min(batch_start + batch_size, num_edges)\n",
    "\n",
    "#         batch_scores = torch.matmul(\n",
    "#             playlists_embeddings[positive_playlists[batch_start:batch_end]], tracks_embeddings.t()\n",
    "#         )\n",
    "\n",
    "#         # Set the scores of the positive edges to negative infinity\n",
    "#         batch_scores[positive_mask[positive_playlists[batch_start:batch_end]]] = -float(\"inf\")\n",
    "\n",
    "#         # Select the top k highest scoring negative edges for each playlist in the current batch\n",
    "#         # do 0.99 to filter out all pos edges which will be at the end\n",
    "#         _, top_indices = torch.topk(batch_scores, int(frac_sample * 0.99 * num_tracks), dim=1)\n",
    "#         selected_indices = torch.randint(0, int(frac_sample * 0.99 *num_tracks), size = (batch_end - batch_start, ))\n",
    "#         top_indices_selected = top_indices[torch.arange(batch_end - batch_start), selected_indices] + n_playlists\n",
    "\n",
    "#         # Create the negative edges tensor for the current batch\n",
    "#         neg_edges_batch = torch.stack(\n",
    "#             (positive_playlists[batch_start:batch_end], top_indices_selected), dim=0\n",
    "#         )\n",
    "#         neg_edge_label_batch = torch.zeros(neg_edges_batch.shape[1], device=device)\n",
    "\n",
    "#         neg_edges_list.append(neg_edges_batch)\n",
    "#         neg_edge_label_list.append(neg_edge_label_batch)\n",
    "\n",
    "#     # Concatenate the batch tensors\n",
    "#     neg_edges = torch.cat(neg_edges_list, dim=1)\n",
    "#     neg_edge_label = torch.cat(neg_edge_label_list)\n",
    "\n",
    "#     return neg_edges, neg_edge_label\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall_at_k(data, model, k = 300, batch_size = 64, device = None):\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model.get_embedding(data.edge_index)\n",
    "#         playlists_embeddings = embeddings[:n_playlists]\n",
    "#         tracks_embeddings = embeddings[n_playlists:]\n",
    "\n",
    "#     hits_list = []\n",
    "#     relevant_counts_list = []\n",
    "\n",
    "#     for batch_start in range(0, n_playlists, batch_size):\n",
    "#         batch_end = min(batch_start + batch_size, n_playlists)\n",
    "#         batch_playlists_embeddings = playlists_embeddings[batch_start:batch_end]\n",
    "\n",
    "#         # Calculate scores for all possible item pairs\n",
    "#         scores = torch.matmul(batch_playlists_embeddings, tracks_embeddings.t())\n",
    "\n",
    "#         # Set the scores of message passing edges to negative infinity\n",
    "#         mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
    "#         scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices] - n_playlists] = -float(\"inf\")\n",
    "\n",
    "#         # Find the top k highest scoring items for each playlist in the batch\n",
    "#         _, top_k_indices = torch.topk(scores, k, dim=1)\n",
    "\n",
    "#         # Ground truth supervision edges\n",
    "#         ground_truth_edges = data.edge_label_index\n",
    "\n",
    "#         # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
    "#         mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
    "#         gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
    "#         mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices] - n_playlists] = True\n",
    "\n",
    "#         # Check how many of the top k items are in the ground truth supervision edges\n",
    "#         hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
    "#         hits_list.append(hits)\n",
    "\n",
    "#         # Calculate the total number of relevant items for each playlist in the batch\n",
    "#         relevant_counts = torch.bincount(ground_truth_edges[0, gt_indices] - batch_start, minlength=batch_end - batch_start)\n",
    "#         relevant_counts_list.append(relevant_counts)\n",
    "\n",
    "#     # Compute recall@k\n",
    "#     hits_tensor = torch.cat(hits_list, dim=0)\n",
    "#     relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
    "#     # Handle division by zero case\n",
    "#     recall_at_k = torch.where(\n",
    "#         relevant_counts_tensor != 0,\n",
    "#         hits_tensor.true_divide(relevant_counts_tensor),\n",
    "#         torch.ones_like(hits_tensor)\n",
    "#     )\n",
    "#     # take average\n",
    "#     recall_at_k = torch.mean(recall_at_k)\n",
    "\n",
    "#     if recall_at_k.numel() == 1:\n",
    "#         return recall_at_k.item()\n",
    "#     else:\n",
    "#         raise ValueError(\"recall_at_k contains more than one item.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc71b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def recall_at_k(batch, model, k=300, score_batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    Calculate Recall@K for heterogeneous batch.\n",
    "    \n",
    "    Args:\n",
    "        batch: HeteroData batch from neighbor loader\n",
    "        model: Your GNN model with get_embedding() method\n",
    "        k: Top-k items to consider (default: 300)\n",
    "        score_batch_size: Batch size for scoring (default: 64)\n",
    "        device: Device to use\n",
    "        \n",
    "    Returns:\n",
    "        recall_at_k: Scalar recall value\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = batch['playlist'].n_id.device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings from model using the batch\n",
    "        embed_dict = model.get_embedding(batch)\n",
    "        \n",
    "        # Extract LOCAL batch embeddings\n",
    "        playlist_emb = embed_dict['playlist'].to(device)\n",
    "        track_emb = embed_dict['track'].to(device)\n",
    "        \n",
    "        num_batch_playlists = playlist_emb.size(0)\n",
    "        num_batch_tracks = track_emb.size(0)\n",
    "        \n",
    "        # Get edges (GLOBAL IDs)\n",
    "        mp_edge_index = batch['playlist', 'track_in_playlist', 'track'].edge_index\n",
    "        gt_edge_index = batch['playlist', 'track_in_playlist', 'track'].pos_edge_label_index\n",
    "        \n",
    "        # Get batch node mappings\n",
    "        batch_tracks = batch['track'].n_id\n",
    "        batch_playlists = batch['playlist'].n_id\n",
    "        \n",
    "        # Create reverse mappings: global_id → local_idx\n",
    "        track_global_to_local = {t.item(): i for i, t in enumerate(batch_tracks.cpu())}\n",
    "        playlist_global_to_local = {p.item(): i for i, p in enumerate(batch_playlists.cpu())}\n",
    "        \n",
    "        hits_list = []\n",
    "        relevant_counts_list = []\n",
    "        \n",
    "        # Process playlists in batches\n",
    "        for batch_start in range(0, num_batch_playlists, score_batch_size):\n",
    "            batch_end = min(batch_start + score_batch_size, num_batch_playlists)\n",
    "            batch_playlist_emb = playlist_emb[batch_start:batch_end]\n",
    "            \n",
    "            # Calculate scores for all tracks (using LOCAL embeddings)\n",
    "            scores = torch.matmul(batch_playlist_emb, track_emb.t())\n",
    "            \n",
    "            # Mask out message passing edges (exclude training edges)\n",
    "            for i in range(mp_edge_index.size(1)):\n",
    "                p_global = mp_edge_index[0, i].item()\n",
    "                t_global = mp_edge_index[1, i].item()\n",
    "                \n",
    "                # Check if both nodes are in this batch\n",
    "                if p_global in playlist_global_to_local and t_global in track_global_to_local:\n",
    "                    p_local = playlist_global_to_local[p_global]\n",
    "                    t_local = track_global_to_local[t_global]\n",
    "                    \n",
    "                    # Check if playlist is in current scoring batch\n",
    "                    if batch_start <= p_local < batch_end:\n",
    "                        scores[p_local - batch_start, t_local] = -float(\"inf\")\n",
    "            \n",
    "            # Get top-k predictions\n",
    "            actual_k = min(k, num_batch_tracks)\n",
    "            _, top_k_indices = torch.topk(scores, actual_k, dim=1)\n",
    "            \n",
    "            # Create ground truth mask (LOCAL indices)\n",
    "            mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
    "            \n",
    "            for i in range(gt_edge_index.size(1)):\n",
    "                p_global = gt_edge_index[0, i].item()\n",
    "                t_global = gt_edge_index[1, i].item()\n",
    "                \n",
    "                # Check if both nodes are in this batch\n",
    "                if p_global in playlist_global_to_local and t_global in track_global_to_local:\n",
    "                    p_local = playlist_global_to_local[p_global]\n",
    "                    t_local = track_global_to_local[t_global]\n",
    "                    \n",
    "                    # Check if playlist is in current scoring batch\n",
    "                    if batch_start <= p_local < batch_end:\n",
    "                        mask[p_local - batch_start, t_local] = True\n",
    "            \n",
    "            # Count hits (how many ground truth items are in top-k)\n",
    "            hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
    "            hits_list.append(hits)\n",
    "            \n",
    "            # Count total relevant items per playlist in this scoring batch\n",
    "            relevant_counts = torch.zeros(batch_end - batch_start, device=device)\n",
    "            for i in range(gt_edge_index.size(1)):\n",
    "                p_global = gt_edge_index[0, i].item()\n",
    "                \n",
    "                if p_global in playlist_global_to_local:\n",
    "                    p_local = playlist_global_to_local[p_global]\n",
    "                    if batch_start <= p_local < batch_end:\n",
    "                        relevant_counts[p_local - batch_start] += 1\n",
    "            \n",
    "            relevant_counts_list.append(relevant_counts)\n",
    "        \n",
    "        # Compute recall@k\n",
    "        hits_tensor = torch.cat(hits_list, dim=0)\n",
    "        relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
    "        \n",
    "        # Handle division by zero (playlists with no ground truth edges)\n",
    "        recall_at_k = torch.where(\n",
    "            relevant_counts_tensor != 0,\n",
    "            hits_tensor.float() / relevant_counts_tensor,\n",
    "            torch.zeros_like(hits_tensor, dtype=torch.float)  # Changed from ones to zeros\n",
    "        )\n",
    "        \n",
    "        # Average recall across all playlists\n",
    "        recall_at_k = torch.mean(recall_at_k)\n",
    "        \n",
    "        return recall_at_k.item()\n",
    "\n",
    "\n",
    "# Usage:\n",
    "recall = recall_at_k(batch, model, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5a860a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(labels, preds):\n",
    "  roc = roc_auc_score(labels.flatten().cpu().numpy(), preds.flatten().data.cpu().numpy())\n",
    "  return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcd6fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(datasets, model, optimizer, loss_fn, args, neg_samp = \"random\"):\n",
    "  print(f\"Beginning training for {model.name}\")\n",
    "\n",
    "  train_data = datasets[\"train\"]\n",
    "  val_data = datasets[\"val\"]\n",
    "\n",
    "  stats = {\n",
    "      'train': {\n",
    "        'loss': [],\n",
    "        'roc' : []\n",
    "      },\n",
    "      'val': {\n",
    "        'loss': [],\n",
    "        'recall': [],\n",
    "        'roc' : []\n",
    "      }\n",
    "\n",
    "  }\n",
    "  val_neg_edge, val_neg_label = None, None\n",
    "  for epoch in range(args[\"epochs\"]): # loop over each epoch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # obtain negative sample\n",
    "    if neg_samp == \"random\":\n",
    "      neg_edge_index, neg_edge_label = sample_negative_edges(train_data, n_playlists, n_tracks, args[\"device\"])\n",
    "    elif neg_samp == \"hard\":\n",
    "      if epoch % 5 == 0:\n",
    "        neg_edge_index, neg_edge_label = sample_hard_negative_edges(\n",
    "            train_data, model, n_playlists, n_tracks, args[\"device\"], batch_size = 500,\n",
    "            frac_sample = 1 - (0.5 * epoch / args[\"epochs\"])\n",
    "        )\n",
    "    # calculate embedding\n",
    "    embed = model.get_embedding(train_data.edge_index)\n",
    "    # calculate pos, negative scores using embedding\n",
    "    pos_scores = model.predict_link_embedding(embed, train_data.edge_label_index)\n",
    "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
    "\n",
    "    # concatenate pos, neg scores together and evaluate loss\n",
    "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
    "    labels = torch.cat((train_data.edge_label, neg_edge_label), dim = 0)\n",
    "\n",
    "    # calculate loss function\n",
    "    if loss_fn == \"BCE\":\n",
    "      loss = model.link_pred_loss(scores, labels)\n",
    "    elif loss_fn == \"BPR\":\n",
    "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
    "\n",
    "    train_roc = metrics(labels, scores)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_roc, val_neg_edge, val_neg_label = test(\n",
    "        model, val_data, loss_fn, neg_samp, epoch, val_neg_edge, val_neg_label\n",
    "    )\n",
    "\n",
    "    stats['train']['loss'].append(loss)\n",
    "    stats['train']['roc'].append(train_roc)\n",
    "    stats['val']['loss'].append(val_loss)\n",
    "    stats['val']['roc'].append(val_roc)\n",
    "\n",
    "    print(f\"Epoch {epoch}; Train loss {loss}; Val loss {val_loss}; Train ROC {train_roc}; Val ROC {val_roc}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      # calculate recall @ K\n",
    "      val_recall = recall_at_k(val_data, model, k = 300, device = args[\"device\"])\n",
    "      print(f\"Val recall {val_recall}\")\n",
    "      stats['val']['recall'].append(val_recall)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "\n",
    "      # save embeddings for future visualization\n",
    "      path = os.path.join(\"model_embeddings\", model.name)\n",
    "      if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "      torch.save(model.embedding.weight, os.path.join(\"model_embeddings\", model.name, f\"{model.name}_{loss_fn}_{neg_samp}_{epoch}.pt\"))\n",
    "\n",
    "  pickle.dump(stats, open(f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pkl\", \"wb\"))\n",
    "  return stats\n",
    "\n",
    "def test(model, data, loss_fn, neg_samp, epoch = 0, neg_edge_index = None, neg_edge_label = None):\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad(): # want to save RAM\n",
    "\n",
    "    # conduct negative sampling\n",
    "    if neg_samp == \"random\":\n",
    "      neg_edge_index, neg_edge_label = sample_negative_edges(data, n_playlists, n_tracks, args[\"device\"])\n",
    "    elif neg_samp == \"hard\":\n",
    "      if epoch % 5 == 0 or neg_edge_index is None:\n",
    "        neg_edge_index, neg_edge_label = sample_hard_negative_edges(\n",
    "            data, model, n_playlists, n_tracks, args[\"device\"], batch_size = 500,\n",
    "            frac_sample = 1 - (0.5 * epoch / args[\"epochs\"])\n",
    "        )\n",
    "    # obtain model embedding\n",
    "    embed = model.get_embedding(data.edge_index)\n",
    "    # calculate pos, neg scores using embedding\n",
    "    pos_scores = model.predict_link_embedding(embed, data.edge_label_index)\n",
    "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
    "    # concatenate pos, neg scores together and evaluate loss\n",
    "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
    "    labels = torch.cat((data.edge_label, neg_edge_label), dim = 0)\n",
    "    # calculate loss\n",
    "    if loss_fn == \"BCE\":\n",
    "      loss = model.link_pred_loss(scores, labels)\n",
    "    elif loss_fn == \"BPR\":\n",
    "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
    "\n",
    "    roc = metrics(labels, scores)\n",
    "\n",
    "  return loss, roc, neg_edge_index, neg_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c070427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the dataset splits\n",
    "datasets = {\n",
    "    'train':train_split,\n",
    "    'val':val_split,\n",
    "    'test': test_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e0365f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our arguments\n",
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_layers' :  3,\n",
    "    'emb_size' : 64,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79c2629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import to_hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acd31c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8749f812",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_playlists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize model and and optimizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mn_playlists\u001b[49m \u001b[38;5;241m+\u001b[39m n_tracks\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GCN(\n\u001b[1;32m      4\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m num_nodes, num_layers \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     embedding_dim \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], conv_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m to_hetero(model, train_split\u001b[38;5;241m.\u001b[39mmetadata(), aggr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_playlists' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize model and and optimizer\n",
    "num_nodes = n_playlists + n_tracks\n",
    "model = GCN(\n",
    "    num_nodes = num_nodes, num_layers = args['num_layers'],\n",
    "    embedding_dim = args[\"emb_size\"], conv_layer = \"SAGE\"\n",
    ")\n",
    "model = to_hetero(model, train_split.metadata(), aggr = \"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec2c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(35300, 64, num_layers=3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only move model to GPU\n",
    "model.to(args[\"device\"])\n",
    "\n",
    "# Convert index tensors (if you're using them separately)\n",
    "playlists_idx = torch.tensor(playlists_idx, dtype=torch.int64).to(args[\"device\"])\n",
    "tracks_idx = torch.tensor(tracks_idx, dtype=torch.int64).to(args[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75acf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save model_stats\n",
    "MODEL_STATS_DIR = \"model_stats\"\n",
    "if not os.path.exists(MODEL_STATS_DIR):\n",
    "  os.makedirs(MODEL_STATS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bbec7424",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m(datasets, model, optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBPR\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, neg_samp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(datasets, model, optimizer, \"BPR\", args, neg_samp = \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ba6d8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1410, device='cuda:0'),\n",
       " 0.9499165680823833,\n",
       " tensor([[12321, 14144,  3385,  ...,  5691, 21204,  1460],\n",
       "         [29629, 32538, 35098,  ..., 35196, 30857, 25586]], device='cuda:0'),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, datasets['test'], \"BPR\", neg_samp = \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f121d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(conv_layer, args, alpha = False):\n",
    "  num_nodes = n_playlists + n_tracks\n",
    "  model = GCN(\n",
    "      num_nodes = num_nodes, num_layers = args['num_layers'],\n",
    "      embedding_dim = args[\"emb_size\"], conv_layer = conv_layer,\n",
    "      alpha_learnable = alpha\n",
    "  )\n",
    "  model.to(args[\"device\"])\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "  return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a55b5682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training for LGCN_LGC_4_e64_nodes35300_\n",
      "Epoch 0; Train loss 0.6931434273719788; Val loss 0.6931453943252563; Train ROC 0.6034073015044848; Val ROC 0.6759018905552148\n",
      "Val recall 0.2589329481124878\n",
      "Epoch 0; Train loss 0.6931434273719788; Val loss 0.6931453943252563; Train ROC 0.6034073015044848; Val ROC 0.6759018905552148\n",
      "Val recall 0.2589329481124878\n",
      "Epoch 1; Train loss 0.693144679069519; Val loss 0.6931245923042297; Train ROC 0.7589306002473327; Val ROC 0.7777647373441927\n",
      "Epoch 2; Train loss 0.6931230425834656; Val loss 0.6929528117179871; Train ROC 0.8139802534992705; Val ROC 0.8013493128548925\n",
      "Epoch 1; Train loss 0.693144679069519; Val loss 0.6931245923042297; Train ROC 0.7589306002473327; Val ROC 0.7777647373441927\n",
      "Epoch 2; Train loss 0.6931230425834656; Val loss 0.6929528117179871; Train ROC 0.8139802534992705; Val ROC 0.8013493128548925\n",
      "Epoch 3; Train loss 0.6929451823234558; Val loss 0.6922587156295776; Train ROC 0.8156420458589091; Val ROC 0.7996414969775105\n",
      "Epoch 4; Train loss 0.6922293305397034; Val loss 0.6904830932617188; Train ROC 0.8072139858924556; Val ROC 0.7994193511409238\n",
      "Epoch 3; Train loss 0.6929451823234558; Val loss 0.6922587156295776; Train ROC 0.8156420458589091; Val ROC 0.7996414969775105\n",
      "Epoch 4; Train loss 0.6922293305397034; Val loss 0.6904830932617188; Train ROC 0.8072139858924556; Val ROC 0.7994193511409238\n",
      "Epoch 5; Train loss 0.6904475092887878; Val loss 0.6871398687362671; Train ROC 0.8024970191853027; Val ROC 0.7980478188628418\n",
      "Epoch 6; Train loss 0.6869509220123291; Val loss 0.6816027164459229; Train ROC 0.8043681870324679; Val ROC 0.8002050380747749\n",
      "Epoch 5; Train loss 0.6904475092887878; Val loss 0.6871398687362671; Train ROC 0.8024970191853027; Val ROC 0.7980478188628418\n",
      "Epoch 6; Train loss 0.6869509220123291; Val loss 0.6816027164459229; Train ROC 0.8043681870324679; Val ROC 0.8002050380747749\n",
      "Epoch 7; Train loss 0.6812371611595154; Val loss 0.6735457181930542; Train ROC 0.8066955257296078; Val ROC 0.802193430930345\n",
      "Epoch 8; Train loss 0.6729232668876648; Val loss 0.6627397537231445; Train ROC 0.8088143817709648; Val ROC 0.8039298658302056\n",
      "Epoch 7; Train loss 0.6812371611595154; Val loss 0.6735457181930542; Train ROC 0.8066955257296078; Val ROC 0.802193430930345\n",
      "Epoch 8; Train loss 0.6729232668876648; Val loss 0.6627397537231445; Train ROC 0.8088143817709648; Val ROC 0.8039298658302056\n",
      "Epoch 9; Train loss 0.6617752909660339; Val loss 0.649107813835144; Train ROC 0.8106441604533731; Val ROC 0.8054730568762192\n",
      "Epoch 9; Train loss 0.6617752909660339; Val loss 0.649107813835144; Train ROC 0.8106441604533731; Val ROC 0.8054730568762192\n",
      "Epoch 10; Train loss 0.6482645869255066; Val loss 0.6334921717643738; Train ROC 0.8098680873231823; Val ROC 0.8044000011056873\n",
      "Val recall 0.4114921987056732\n",
      "Epoch 10; Train loss 0.6482645869255066; Val loss 0.6334921717643738; Train ROC 0.8098680873231823; Val ROC 0.8044000011056873\n",
      "Val recall 0.4114921987056732\n",
      "Epoch 11; Train loss 0.6316260099411011; Val loss 0.6150034666061401; Train ROC 0.8115193358340562; Val ROC 0.8058693663271381\n",
      "Epoch 11; Train loss 0.6316260099411011; Val loss 0.6150034666061401; Train ROC 0.8115193358340562; Val ROC 0.8058693663271381\n",
      "Epoch 12; Train loss 0.6125074028968811; Val loss 0.5946041941642761; Train ROC 0.8131871623079668; Val ROC 0.807407728096706\n",
      "Epoch 13; Train loss 0.5913878679275513; Val loss 0.5729467868804932; Train ROC 0.8149277208852008; Val ROC 0.8091251794122971\n",
      "Epoch 12; Train loss 0.6125074028968811; Val loss 0.5946041941642761; Train ROC 0.8131871623079668; Val ROC 0.807407728096706\n",
      "Epoch 13; Train loss 0.5913878679275513; Val loss 0.5729467868804932; Train ROC 0.8149277208852008; Val ROC 0.8091251794122971\n",
      "Epoch 14; Train loss 0.5689405202865601; Val loss 0.5508337616920471; Train ROC 0.8168363198567093; Val ROC 0.8111190958308518\n",
      "Epoch 14; Train loss 0.5689405202865601; Val loss 0.5508337616920471; Train ROC 0.8168363198567093; Val ROC 0.8111190958308518\n",
      "Epoch 15; Train loss 0.5472096800804138; Val loss 0.5297122001647949; Train ROC 0.8172517750573447; Val ROC 0.8136823804802268\n",
      "Epoch 15; Train loss 0.5472096800804138; Val loss 0.5297122001647949; Train ROC 0.8172517750573447; Val ROC 0.8136823804802268\n",
      "Epoch 16; Train loss 0.524857759475708; Val loss 0.5093288421630859; Train ROC 0.8197378045102562; Val ROC 0.8162785143419883\n",
      "Epoch 17; Train loss 0.5036938190460205; Val loss 0.4907902777194977; Train ROC 0.822548166801467; Val ROC 0.8191699555293027\n",
      "Epoch 16; Train loss 0.524857759475708; Val loss 0.5093288421630859; Train ROC 0.8197378045102562; Val ROC 0.8162785143419883\n",
      "Epoch 17; Train loss 0.5036938190460205; Val loss 0.4907902777194977; Train ROC 0.822548166801467; Val ROC 0.8191699555293027\n",
      "Epoch 18; Train loss 0.4843738377094269; Val loss 0.47448551654815674; Train ROC 0.8256688368454053; Val ROC 0.8223310254577102\n",
      "Epoch 19; Train loss 0.4673192501068115; Val loss 0.46055859327316284; Train ROC 0.8290573080067872; Val ROC 0.8256816762566078\n",
      "Epoch 18; Train loss 0.4843738377094269; Val loss 0.47448551654815674; Train ROC 0.8256688368454053; Val ROC 0.8223310254577102\n",
      "Epoch 19; Train loss 0.4673192501068115; Val loss 0.46055859327316284; Train ROC 0.8290573080067872; Val ROC 0.8256816762566078\n",
      "Epoch 20; Train loss 0.45471590757369995; Val loss 0.4513704180717468; Train ROC 0.8310432323996731; Val ROC 0.8257130001030236\n",
      "Val recall 0.4227442443370819\n",
      "Epoch 20; Train loss 0.45471590757369995; Val loss 0.4513704180717468; Train ROC 0.8310432323996731; Val ROC 0.8257130001030236\n",
      "Val recall 0.4227442443370819\n",
      "Epoch 21; Train loss 0.4426499903202057; Val loss 0.4420856535434723; Train ROC 0.8346459118585715; Val ROC 0.8290389018119678\n",
      "Epoch 22; Train loss 0.43277883529663086; Val loss 0.4347459077835083; Train ROC 0.8382034740564461; Val ROC 0.832200209215815\n",
      "Epoch 21; Train loss 0.4426499903202057; Val loss 0.4420856535434723; Train ROC 0.8346459118585715; Val ROC 0.8290389018119678\n",
      "Epoch 22; Train loss 0.43277883529663086; Val loss 0.4347459077835083; Train ROC 0.8382034740564461; Val ROC 0.832200209215815\n",
      "Epoch 23; Train loss 0.42490243911743164; Val loss 0.4291294515132904; Train ROC 0.8416003470455451; Val ROC 0.8351314856644141\n",
      "Epoch 24; Train loss 0.41881418228149414; Val loss 0.42501959204673767; Train ROC 0.8447558084785183; Val ROC 0.8378016864812085\n",
      "Epoch 23; Train loss 0.42490243911743164; Val loss 0.4291294515132904; Train ROC 0.8416003470455451; Val ROC 0.8351314856644141\n",
      "Epoch 24; Train loss 0.41881418228149414; Val loss 0.42501959204673767; Train ROC 0.8447558084785183; Val ROC 0.8378016864812085\n",
      "Epoch 25; Train loss 0.41677889227867126; Val loss 0.42365923523902893; Train ROC 0.8450899025592125; Val ROC 0.839254526227783\n",
      "Epoch 26; Train loss 0.4137425422668457; Val loss 0.4219683110713959; Train ROC 0.8476813987624112; Val ROC 0.8415062767739254\n",
      "Epoch 25; Train loss 0.41677889227867126; Val loss 0.42365923523902893; Train ROC 0.8450899025592125; Val ROC 0.839254526227783\n",
      "Epoch 26; Train loss 0.4137425422668457; Val loss 0.4219683110713959; Train ROC 0.8476813987624112; Val ROC 0.8415062767739254\n",
      "Epoch 27; Train loss 0.411825567483902; Val loss 0.4211812913417816; Train ROC 0.8500773709753966; Val ROC 0.8435605190611684\n",
      "Epoch 28; Train loss 0.41083580255508423; Val loss 0.42110615968704224; Train ROC 0.8522855428640016; Val ROC 0.8454371438319911\n",
      "Epoch 27; Train loss 0.411825567483902; Val loss 0.4211812913417816; Train ROC 0.8500773709753966; Val ROC 0.8435605190611684\n",
      "Epoch 28; Train loss 0.41083580255508423; Val loss 0.42110615968704224; Train ROC 0.8522855428640016; Val ROC 0.8454371438319911\n",
      "Epoch 29; Train loss 0.4105883538722992; Val loss 0.42155909538269043; Train ROC 0.854313782280596; Val ROC 0.8471534144035867\n",
      "Epoch 29; Train loss 0.4105883538722992; Val loss 0.42155909538269043; Train ROC 0.854313782280596; Val ROC 0.8471534144035867\n",
      "Epoch 30; Train loss 0.41312935948371887; Val loss 0.42375504970550537; Train ROC 0.8544136874622523; Val ROC 0.8474512185547793\n",
      "Val recall 0.4427902400493622\n",
      "Epoch 30; Train loss 0.41312935948371887; Val loss 0.42375504970550537; Train ROC 0.8544136874622523; Val ROC 0.8474512185547793\n",
      "Val recall 0.4427902400493622\n",
      "Epoch 31; Train loss 0.4139411449432373; Val loss 0.4248376488685608; Train ROC 0.8560703492681938; Val ROC 0.848939808389441\n",
      "Epoch 32; Train loss 0.41495710611343384; Val loss 0.42600634694099426; Train ROC 0.8576550257512161; Val ROC 0.8503319507064252\n",
      "Epoch 31; Train loss 0.4139411449432373; Val loss 0.4248376488685608; Train ROC 0.8560703492681938; Val ROC 0.848939808389441\n",
      "Epoch 32; Train loss 0.41495710611343384; Val loss 0.42600634694099426; Train ROC 0.8576550257512161; Val ROC 0.8503319507064252\n",
      "Epoch 33; Train loss 0.4160420894622803; Val loss 0.4271341860294342; Train ROC 0.8591668873958396; Val ROC 0.8516347554195446\n",
      "Epoch 34; Train loss 0.41707760095596313; Val loss 0.42811524868011475; Train ROC 0.8606013682158007; Val ROC 0.8528515648847778\n",
      "Epoch 33; Train loss 0.4160420894622803; Val loss 0.4271341860294342; Train ROC 0.8591668873958396; Val ROC 0.8516347554195446\n",
      "Epoch 34; Train loss 0.41707760095596313; Val loss 0.42811524868011475; Train ROC 0.8606013682158007; Val ROC 0.8528515648847778\n",
      "Epoch 35; Train loss 0.4196961224079132; Val loss 0.43028509616851807; Train ROC 0.860473450409248; Val ROC 0.8527905174183519\n",
      "Epoch 36; Train loss 0.42050135135650635; Val loss 0.4308376908302307; Train ROC 0.8616422448119014; Val ROC 0.8538221426760955\n",
      "Epoch 35; Train loss 0.4196961224079132; Val loss 0.43028509616851807; Train ROC 0.860473450409248; Val ROC 0.8527905174183519\n",
      "Epoch 36; Train loss 0.42050135135650635; Val loss 0.4308376908302307; Train ROC 0.8616422448119014; Val ROC 0.8538221426760955\n",
      "Epoch 37; Train loss 0.42100441455841064; Val loss 0.4311073124408722; Train ROC 0.8627725086196919; Val ROC 0.854775496407953\n",
      "Epoch 38; Train loss 0.4211828410625458; Val loss 0.4310801923274994; Train ROC 0.863856592581734; Val ROC 0.8556582029732496\n",
      "Epoch 37; Train loss 0.42100441455841064; Val loss 0.4311073124408722; Train ROC 0.8627725086196919; Val ROC 0.854775496407953\n",
      "Epoch 38; Train loss 0.4211828410625458; Val loss 0.4310801923274994; Train ROC 0.863856592581734; Val ROC 0.8556582029732496\n",
      "Epoch 39; Train loss 0.42103493213653564; Val loss 0.4307611882686615; Train ROC 0.8648869661698009; Val ROC 0.8564772628379469\n",
      "Epoch 39; Train loss 0.42103493213653564; Val loss 0.4307611882686615; Train ROC 0.8648869661698009; Val ROC 0.8564772628379469\n",
      "Epoch 40; Train loss 0.4228224754333496; Val loss 0.43234145641326904; Train ROC 0.8636073682128873; Val ROC 0.8552113179207197\n",
      "Val recall 0.45735597610473633\n",
      "Epoch 40; Train loss 0.4228224754333496; Val loss 0.43234145641326904; Train ROC 0.8636073682128873; Val ROC 0.8552113179207197\n",
      "Val recall 0.45735597610473633\n",
      "Epoch 41; Train loss 0.42227602005004883; Val loss 0.4316645562648773; Train ROC 0.8644219326993866; Val ROC 0.8559138287856894\n",
      "Epoch 42; Train loss 0.4214940071105957; Val loss 0.4308389723300934; Train ROC 0.865221024326627; Val ROC 0.8565661459658702\n",
      "Epoch 41; Train loss 0.42227602005004883; Val loss 0.4316645562648773; Train ROC 0.8644219326993866; Val ROC 0.8559138287856894\n",
      "Epoch 42; Train loss 0.4214940071105957; Val loss 0.4308389723300934; Train ROC 0.865221024326627; Val ROC 0.8565661459658702\n",
      "Epoch 43; Train loss 0.4205223023891449; Val loss 0.42990005016326904; Train ROC 0.8659988310907581; Val ROC 0.8571750466657904\n",
      "Epoch 44; Train loss 0.41941070556640625; Val loss 0.4288848340511322; Train ROC 0.8667491235425442; Val ROC 0.8577486214303505\n",
      "Epoch 43; Train loss 0.4205223023891449; Val loss 0.42990005016326904; Train ROC 0.8659988310907581; Val ROC 0.8571750466657904\n",
      "Epoch 44; Train loss 0.41941070556640625; Val loss 0.4288848340511322; Train ROC 0.8667491235425442; Val ROC 0.8577486214303505\n",
      "Epoch 45; Train loss 0.4203106462955475; Val loss 0.4275154769420624; Train ROC 0.8658137952259002; Val ROC 0.8581145466483788\n",
      "Epoch 46; Train loss 0.4192846417427063; Val loss 0.4266073703765869; Train ROC 0.8663862061808447; Val ROC 0.8586149979633619\n",
      "Epoch 45; Train loss 0.4203106462955475; Val loss 0.4275154769420624; Train ROC 0.8658137952259002; Val ROC 0.8581145466483788\n",
      "Epoch 46; Train loss 0.4192846417427063; Val loss 0.4266073703765869; Train ROC 0.8663862061808447; Val ROC 0.8586149979633619\n",
      "Epoch 47; Train loss 0.41825753450393677; Val loss 0.4257684350013733; Train ROC 0.8669681015062805; Val ROC 0.859084801206417\n",
      "Epoch 48; Train loss 0.4172624945640564; Val loss 0.4250115752220154; Train ROC 0.8675500070461721; Val ROC 0.8595251870397767\n",
      "Epoch 47; Train loss 0.41825753450393677; Val loss 0.4257684350013733; Train ROC 0.8669681015062805; Val ROC 0.859084801206417\n",
      "Epoch 48; Train loss 0.4172624945640564; Val loss 0.4250115752220154; Train ROC 0.8675500070461721; Val ROC 0.8595251870397767\n",
      "Epoch 49; Train loss 0.4163269102573395; Val loss 0.4243439733982086; Train ROC 0.868120966567754; Val ROC 0.8599368930688579\n",
      "Epoch 49; Train loss 0.4163269102573395; Val loss 0.4243439733982086; Train ROC 0.868120966567754; Val ROC 0.8599368930688579\n",
      "Epoch 50; Train loss 0.41714465618133545; Val loss 0.42563721537590027; Train ROC 0.8669003424740747; Val ROC 0.8589422963137285\n",
      "Val recall 0.4608980715274811\n",
      "Epoch 50; Train loss 0.41714465618133545; Val loss 0.42563721537590027; Train ROC 0.8669003424740747; Val ROC 0.8589422963137285\n",
      "Val recall 0.4608980715274811\n",
      "Epoch 51; Train loss 0.41655421257019043; Val loss 0.42524683475494385; Train ROC 0.8673185236540306; Val ROC 0.8592834948457857\n",
      "Epoch 52; Train loss 0.41603410243988037; Val loss 0.42496082186698914; Train ROC 0.8677539318477847; Val ROC 0.8595980541972765\n",
      "Epoch 51; Train loss 0.41655421257019043; Val loss 0.42524683475494385; Train ROC 0.8673185236540306; Val ROC 0.8592834948457857\n",
      "Epoch 52; Train loss 0.41603410243988037; Val loss 0.42496082186698914; Train ROC 0.8677539318477847; Val ROC 0.8595980541972765\n",
      "Epoch 53; Train loss 0.41558659076690674; Val loss 0.4247640073299408; Train ROC 0.8681938992849871; Val ROC 0.8598837893369498\n",
      "Epoch 54; Train loss 0.41521015763282776; Val loss 0.4246392250061035; Train ROC 0.8686246934722565; Val ROC 0.860138770848774\n",
      "Epoch 53; Train loss 0.41558659076690674; Val loss 0.4247640073299408; Train ROC 0.8681938992849871; Val ROC 0.8598837893369498\n",
      "Epoch 54; Train loss 0.41521015763282776; Val loss 0.4246392250061035; Train ROC 0.8686246934722565; Val ROC 0.860138770848774\n",
      "Epoch 55; Train loss 0.41679224371910095; Val loss 0.4263988137245178; Train ROC 0.8673133518244818; Val ROC 0.8585020327208259\n",
      "Epoch 56; Train loss 0.4167044758796692; Val loss 0.4264463186264038; Train ROC 0.8676008224182269; Val ROC 0.8587285988088955\n",
      "Epoch 55; Train loss 0.41679224371910095; Val loss 0.4263988137245178; Train ROC 0.8673133518244818; Val ROC 0.8585020327208259\n",
      "Epoch 56; Train loss 0.4167044758796692; Val loss 0.4264463186264038; Train ROC 0.8676008224182269; Val ROC 0.8587285988088955\n",
      "Epoch 57; Train loss 0.41664019227027893; Val loss 0.4265449047088623; Train ROC 0.867917030732726; Val ROC 0.8589325647956031\n",
      "Epoch 58; Train loss 0.4165899157524109; Val loss 0.42667248845100403; Train ROC 0.8682452083510988; Val ROC 0.8591095453891537\n",
      "Epoch 57; Train loss 0.41664019227027893; Val loss 0.4265449047088623; Train ROC 0.867917030732726; Val ROC 0.8589325647956031\n",
      "Epoch 58; Train loss 0.4165899157524109; Val loss 0.42667248845100403; Train ROC 0.8682452083510988; Val ROC 0.8591095453891537\n",
      "Epoch 59; Train loss 0.41654515266418457; Val loss 0.42680877447128296; Train ROC 0.8685685941338565; Val ROC 0.8592558743572745\n",
      "Epoch 59; Train loss 0.41654515266418457; Val loss 0.42680877447128296; Train ROC 0.8685685941338565; Val ROC 0.8592558743572745\n",
      "Epoch 60; Train loss 0.418667733669281; Val loss 0.42807742953300476; Train ROC 0.8669599651596321; Val ROC 0.8586387096677299\n",
      "Val recall 0.46144983172416687\n",
      "Epoch 60; Train loss 0.418667733669281; Val loss 0.42807742953300476; Train ROC 0.8669599651596321; Val ROC 0.8586387096677299\n",
      "Val recall 0.46144983172416687\n",
      "Epoch 61; Train loss 0.418806254863739; Val loss 0.42830199003219604; Train ROC 0.8671404719473317; Val ROC 0.858755027216049\n",
      "Epoch 62; Train loss 0.41892209649086; Val loss 0.4285304844379425; Train ROC 0.8673563651017684; Val ROC 0.858857267550609\n",
      "Epoch 61; Train loss 0.418806254863739; Val loss 0.42830199003219604; Train ROC 0.8671404719473317; Val ROC 0.858755027216049\n",
      "Epoch 62; Train loss 0.41892209649086; Val loss 0.4285304844379425; Train ROC 0.8673563651017684; Val ROC 0.858857267550609\n",
      "Epoch 63; Train loss 0.41900551319122314; Val loss 0.4287417232990265; Train ROC 0.8675932506716462; Val ROC 0.8589434728612078\n",
      "Epoch 64; Train loss 0.4190487265586853; Val loss 0.4289175271987915; Train ROC 0.8678361570566451; Val ROC 0.8590126839054536\n",
      "Epoch 63; Train loss 0.41900551319122314; Val loss 0.4287417232990265; Train ROC 0.8675932506716462; Val ROC 0.8589434728612078\n",
      "Epoch 64; Train loss 0.4190487265586853; Val loss 0.4289175271987915; Train ROC 0.8678361570566451; Val ROC 0.8590126839054536\n",
      "Epoch 65; Train loss 0.42111095786094666; Val loss 0.43025004863739014; Train ROC 0.8661248473068162; Val ROC 0.8572491448577948\n",
      "Epoch 66; Train loss 0.4212401509284973; Val loss 0.4304310083389282; Train ROC 0.8662412204840999; Val ROC 0.8572950756604605\n",
      "Epoch 65; Train loss 0.42111095786094666; Val loss 0.43025004863739014; Train ROC 0.8661248473068162; Val ROC 0.8572491448577948\n",
      "Epoch 66; Train loss 0.4212401509284973; Val loss 0.4304310083389282; Train ROC 0.8662412204840999; Val ROC 0.8572950756604605\n",
      "Epoch 67; Train loss 0.42130452394485474; Val loss 0.43058642745018005; Train ROC 0.86639392874489; Val ROC 0.8573297503912587\n",
      "Epoch 68; Train loss 0.4213038384914398; Val loss 0.4307050108909607; Train ROC 0.8665696306682209; Val ROC 0.8573529695464535\n",
      "Epoch 67; Train loss 0.42130452394485474; Val loss 0.43058642745018005; Train ROC 0.86639392874489; Val ROC 0.8573297503912587\n",
      "Epoch 68; Train loss 0.4213038384914398; Val loss 0.4307050108909607; Train ROC 0.8665696306682209; Val ROC 0.8573529695464535\n",
      "Epoch 69; Train loss 0.4212411642074585; Val loss 0.4307788610458374; Train ROC 0.866754170480047; Val ROC 0.8573642503236181\n",
      "Epoch 69; Train loss 0.4212411642074585; Val loss 0.4307788610458374; Train ROC 0.866754170480047; Val ROC 0.8573642503236181\n",
      "Epoch 70; Train loss 0.4227435886859894; Val loss 0.431831032037735; Train ROC 0.8651321150079001; Val ROC 0.856077441985899\n",
      "Val recall 0.46155494451522827\n",
      "Epoch 70; Train loss 0.4227435886859894; Val loss 0.431831032037735; Train ROC 0.8651321150079001; Val ROC 0.856077441985899\n",
      "Val recall 0.46155494451522827\n",
      "Epoch 71; Train loss 0.4227423667907715; Val loss 0.4318799674510956; Train ROC 0.8652024783783528; Val ROC 0.8560986340206876\n",
      "Epoch 72; Train loss 0.4226681888103485; Val loss 0.43190649151802063; Train ROC 0.8653260921163832; Val ROC 0.8561230314493478\n",
      "Epoch 71; Train loss 0.4227423667907715; Val loss 0.4318799674510956; Train ROC 0.8652024783783528; Val ROC 0.8560986340206876\n",
      "Epoch 72; Train loss 0.4226681888103485; Val loss 0.43190649151802063; Train ROC 0.8653260921163832; Val ROC 0.8561230314493478\n",
      "Epoch 73; Train loss 0.4225319027900696; Val loss 0.43190836906433105; Train ROC 0.8654876617948152; Val ROC 0.8561481696646546\n",
      "Epoch 74; Train loss 0.4223460853099823; Val loss 0.43188467621803284; Train ROC 0.8656710105074916; Val ROC 0.8561702442200421\n",
      "Epoch 73; Train loss 0.4225319027900696; Val loss 0.43190836906433105; Train ROC 0.8654876617948152; Val ROC 0.8561481696646546\n",
      "Epoch 74; Train loss 0.4223460853099823; Val loss 0.43188467621803284; Train ROC 0.8656710105074916; Val ROC 0.8561702442200421\n",
      "Epoch 75; Train loss 0.4240180253982544; Val loss 0.4331722557544708; Train ROC 0.8641021220975282; Val ROC 0.8552303051261765\n",
      "Epoch 76; Train loss 0.4239637553691864; Val loss 0.43321138620376587; Train ROC 0.8641734865546236; Val ROC 0.8552479720873156\n",
      "Epoch 75; Train loss 0.4240180253982544; Val loss 0.4331722557544708; Train ROC 0.8641021220975282; Val ROC 0.8552303051261765\n",
      "Epoch 76; Train loss 0.4239637553691864; Val loss 0.43321138620376587; Train ROC 0.8641734865546236; Val ROC 0.8552479720873156\n",
      "Epoch 77; Train loss 0.42388129234313965; Val loss 0.4332650899887085; Train ROC 0.864291426490879; Val ROC 0.8552633269033727\n",
      "Epoch 78; Train loss 0.4237755537033081; Val loss 0.43332386016845703; Train ROC 0.8644424521900098; Val ROC 0.855274692321563\n",
      "Epoch 77; Train loss 0.42388129234313965; Val loss 0.4332650899887085; Train ROC 0.864291426490879; Val ROC 0.8552633269033727\n",
      "Epoch 78; Train loss 0.4237755537033081; Val loss 0.43332386016845703; Train ROC 0.8644424521900098; Val ROC 0.855274692321563\n",
      "Epoch 79; Train loss 0.4236511290073395; Val loss 0.4333782494068146; Train ROC 0.8646118581761217; Val ROC 0.8552799682900889\n",
      "Epoch 79; Train loss 0.4236511290073395; Val loss 0.4333782494068146; Train ROC 0.8646118581761217; Val ROC 0.8552799682900889\n",
      "Epoch 80; Train loss 0.4255140423774719; Val loss 0.43479201197624207; Train ROC 0.8626060166222989; Val ROC 0.8542037011016543\n",
      "Val recall 0.46179354190826416\n",
      "Epoch 80; Train loss 0.4255140423774719; Val loss 0.43479201197624207; Train ROC 0.8626060166222989; Val ROC 0.8542037011016543\n",
      "Val recall 0.46179354190826416\n",
      "Epoch 81; Train loss 0.42555713653564453; Val loss 0.43491747975349426; Train ROC 0.86267490332861; Val ROC 0.8542360049023002\n",
      "Epoch 82; Train loss 0.4255709946155548; Val loss 0.4350559413433075; Train ROC 0.8628112265479623; Val ROC 0.8542756436718643\n",
      "Epoch 81; Train loss 0.42555713653564453; Val loss 0.43491747975349426; Train ROC 0.86267490332861; Val ROC 0.8542360049023002\n",
      "Epoch 82; Train loss 0.4255709946155548; Val loss 0.4350559413433075; Train ROC 0.8628112265479623; Val ROC 0.8542756436718643\n",
      "Epoch 83; Train loss 0.42555513978004456; Val loss 0.4351934492588043; Train ROC 0.8629970388218295; Val ROC 0.8543164117442568\n",
      "Epoch 84; Train loss 0.4255101978778839; Val loss 0.43531739711761475; Train ROC 0.8632117497722251; Val ROC 0.8543520848736509\n",
      "Epoch 83; Train loss 0.42555513978004456; Val loss 0.4351934492588043; Train ROC 0.8629970388218295; Val ROC 0.8543164117442568\n",
      "Epoch 84; Train loss 0.4255101978778839; Val loss 0.43531739711761475; Train ROC 0.8632117497722251; Val ROC 0.8543520848736509\n",
      "Epoch 85; Train loss 0.42759963870048523; Val loss 0.4362180233001709; Train ROC 0.8611779417551404; Val ROC 0.8526703840522614\n",
      "Epoch 86; Train loss 0.4277075529098511; Val loss 0.43641752004623413; Train ROC 0.8612855006507759; Val ROC 0.8527124650250686\n",
      "Epoch 85; Train loss 0.42759963870048523; Val loss 0.4362180233001709; Train ROC 0.8611779417551404; Val ROC 0.8526703840522614\n",
      "Epoch 86; Train loss 0.4277075529098511; Val loss 0.43641752004623413; Train ROC 0.8612855006507759; Val ROC 0.8527124650250686\n",
      "Epoch 87; Train loss 0.42778512835502625; Val loss 0.43662405014038086; Train ROC 0.8614435656751428; Val ROC 0.8527496705259081\n",
      "Epoch 88; Train loss 0.4278285503387451; Val loss 0.4368200898170471; Train ROC 0.8616337765863341; Val ROC 0.8527773291891804\n",
      "Epoch 87; Train loss 0.42778512835502625; Val loss 0.43662405014038086; Train ROC 0.8614435656751428; Val ROC 0.8527496705259081\n",
      "Epoch 88; Train loss 0.4278285503387451; Val loss 0.4368200898170471; Train ROC 0.8616337765863341; Val ROC 0.8527773291891804\n",
      "Epoch 89; Train loss 0.4278351068496704; Val loss 0.43699008226394653; Train ROC 0.8618365465110336; Val ROC 0.8527920837680423\n",
      "Epoch 89; Train loss 0.4278351068496704; Val loss 0.43699008226394653; Train ROC 0.8618365465110336; Val ROC 0.8527920837680423\n",
      "Epoch 90; Train loss 0.42896944284439087; Val loss 0.4384402632713318; Train ROC 0.8605453333109926; Val ROC 0.8514228137844904\n",
      "Val recall 0.4619694948196411\n",
      "Epoch 90; Train loss 0.42896944284439087; Val loss 0.4384402632713318; Train ROC 0.8605453333109926; Val ROC 0.8514228137844904\n",
      "Val recall 0.4619694948196411\n",
      "Epoch 91; Train loss 0.4290598928928375; Val loss 0.43857628107070923; Train ROC 0.8606070008132083; Val ROC 0.8514044024832029\n",
      "Epoch 92; Train loss 0.4290642738342285; Val loss 0.43867504596710205; Train ROC 0.8607079939688768; Val ROC 0.851374339723763\n",
      "Epoch 91; Train loss 0.4290598928928375; Val loss 0.43857628107070923; Train ROC 0.8606070008132083; Val ROC 0.8514044024832029\n",
      "Epoch 92; Train loss 0.4290642738342285; Val loss 0.43867504596710205; Train ROC 0.8607079939688768; Val ROC 0.851374339723763\n",
      "Epoch 93; Train loss 0.42899006605148315; Val loss 0.4387322962284088; Train ROC 0.8608340560357686; Val ROC 0.8513318249530101\n",
      "Epoch 94; Train loss 0.42884817719459534; Val loss 0.43874678015708923; Train ROC 0.8609699136216815; Val ROC 0.8512767311924849\n",
      "Epoch 93; Train loss 0.42899006605148315; Val loss 0.4387322962284088; Train ROC 0.8608340560357686; Val ROC 0.8513318249530101\n",
      "Epoch 94; Train loss 0.42884817719459534; Val loss 0.43874678015708923; Train ROC 0.8609699136216815; Val ROC 0.8512767311924849\n",
      "Epoch 95; Train loss 0.4308214485645294; Val loss 0.4405938386917114; Train ROC 0.8588775353203877; Val ROC 0.8489240035985449\n",
      "Epoch 96; Train loss 0.4307961165904999; Val loss 0.44065505266189575; Train ROC 0.8588931705013892; Val ROC 0.8488962172122703\n",
      "Epoch 95; Train loss 0.4308214485645294; Val loss 0.4405938386917114; Train ROC 0.8588775353203877; Val ROC 0.8489240035985449\n",
      "Epoch 96; Train loss 0.4307961165904999; Val loss 0.44065505266189575; Train ROC 0.8588931705013892; Val ROC 0.8488962172122703\n",
      "Epoch 97; Train loss 0.4307318925857544; Val loss 0.44072437286376953; Train ROC 0.8589764041348903; Val ROC 0.8488829041705829\n",
      "Epoch 98; Train loss 0.43063637614250183; Val loss 0.44079431891441345; Train ROC 0.8591110641880847; Val ROC 0.8488776475263371\n",
      "Epoch 97; Train loss 0.4307318925857544; Val loss 0.44072437286376953; Train ROC 0.8589764041348903; Val ROC 0.8488829041705829\n",
      "Epoch 98; Train loss 0.43063637614250183; Val loss 0.44079431891441345; Train ROC 0.8591110641880847; Val ROC 0.8488776475263371\n",
      "Epoch 99; Train loss 0.43051666021347046; Val loss 0.44085702300071716; Train ROC 0.8592783133574937; Val ROC 0.8488769257255423\n",
      "Epoch 99; Train loss 0.43051666021347046; Val loss 0.44085702300071716; Train ROC 0.8592783133574937; Val ROC 0.8488769257255423\n",
      "Epoch 100; Train loss 0.43240219354629517; Val loss 0.4414217174053192; Train ROC 0.8574576555007554; Val ROC 0.8486730176101476\n",
      "Val recall 0.46197178959846497\n",
      "Epoch 100; Train loss 0.43240219354629517; Val loss 0.4414217174053192; Train ROC 0.8574576555007554; Val ROC 0.8486730176101476\n",
      "Val recall 0.46197178959846497\n",
      "Epoch 101; Train loss 0.43246740102767944; Val loss 0.44159215688705444; Train ROC 0.8575233143456512; Val ROC 0.8486866532421001\n",
      "Epoch 102; Train loss 0.43251365423202515; Val loss 0.4417809247970581; Train ROC 0.8576558151998575; Val ROC 0.8487073878566176\n",
      "Epoch 101; Train loss 0.43246740102767944; Val loss 0.44159215688705444; Train ROC 0.8575233143456512; Val ROC 0.8486866532421001\n",
      "Epoch 102; Train loss 0.43251365423202515; Val loss 0.4417809247970581; Train ROC 0.8576558151998575; Val ROC 0.8487073878566176\n",
      "Epoch 103; Train loss 0.4325374960899353; Val loss 0.44196996092796326; Train ROC 0.857835317541162; Val ROC 0.848728287962784\n",
      "Epoch 104; Train loss 0.43253573775291443; Val loss 0.44214239716529846; Train ROC 0.8580401689616332; Val ROC 0.8487439069901505\n",
      "Epoch 103; Train loss 0.4325374960899353; Val loss 0.44196996092796326; Train ROC 0.857835317541162; Val ROC 0.848728287962784\n",
      "Epoch 104; Train loss 0.43253573775291443; Val loss 0.44214239716529846; Train ROC 0.8580401689616332; Val ROC 0.8487439069901505\n",
      "Epoch 105; Train loss 0.43448734283447266; Val loss 0.44420549273490906; Train ROC 0.8559654165305499; Val ROC 0.8470245967193893\n",
      "Epoch 106; Train loss 0.43462833762168884; Val loss 0.4444119930267334; Train ROC 0.8560539323339698; Val ROC 0.8470516006922836\n",
      "Epoch 105; Train loss 0.43448734283447266; Val loss 0.44420549273490906; Train ROC 0.8559654165305499; Val ROC 0.8470245967193893\n",
      "Epoch 106; Train loss 0.43462833762168884; Val loss 0.4444119930267334; Train ROC 0.8560539323339698; Val ROC 0.8470516006922836\n",
      "Epoch 107; Train loss 0.4347165524959564; Val loss 0.44460585713386536; Train ROC 0.8562049233912845; Val ROC 0.8470802634397541\n",
      "Epoch 108; Train loss 0.43474891781806946; Val loss 0.4447718858718872; Train ROC 0.8563989188494254; Val ROC 0.8471042044295864\n",
      "Epoch 107; Train loss 0.4347165524959564; Val loss 0.44460585713386536; Train ROC 0.8562049233912845; Val ROC 0.8470802634397541\n",
      "Epoch 108; Train loss 0.43474891781806946; Val loss 0.4447718858718872; Train ROC 0.8563989188494254; Val ROC 0.8471042044295864\n",
      "Epoch 109; Train loss 0.43472519516944885; Val loss 0.44489786028862; Train ROC 0.8566149019728174; Val ROC 0.8471188863478001\n",
      "Epoch 109; Train loss 0.43472519516944885; Val loss 0.44489786028862; Train ROC 0.8566149019728174; Val ROC 0.8471188863478001\n",
      "Epoch 110; Train loss 0.43666523694992065; Val loss 0.44554850459098816; Train ROC 0.8545558955603864; Val ROC 0.845287056578983\n",
      "Val recall 0.46245986223220825\n",
      "Epoch 110; Train loss 0.43666523694992065; Val loss 0.44554850459098816; Train ROC 0.8545558955603864; Val ROC 0.845287056578983\n",
      "Val recall 0.46245986223220825\n",
      "Epoch 111; Train loss 0.43675896525382996; Val loss 0.44570937752723694; Train ROC 0.8546384045688926; Val ROC 0.8453083172132753\n",
      "Epoch 112; Train loss 0.4368003308773041; Val loss 0.44586095213890076; Train ROC 0.8547772181404334; Val ROC 0.8453282320180376\n",
      "Epoch 111; Train loss 0.43675896525382996; Val loss 0.44570937752723694; Train ROC 0.8546384045688926; Val ROC 0.8453083172132753\n",
      "Epoch 112; Train loss 0.4368003308773041; Val loss 0.44586095213890076; Train ROC 0.8547772181404334; Val ROC 0.8453282320180376\n",
      "Epoch 113; Train loss 0.4367891252040863; Val loss 0.44599026441574097; Train ROC 0.8549547239836689; Val ROC 0.845341345183717\n",
      "Epoch 114; Train loss 0.43672722578048706; Val loss 0.4460866153240204; Train ROC 0.8551511391521671; Val ROC 0.8453444435325813\n",
      "Epoch 113; Train loss 0.4367891252040863; Val loss 0.44599026441574097; Train ROC 0.8549547239836689; Val ROC 0.845341345183717\n",
      "Epoch 114; Train loss 0.43672722578048706; Val loss 0.4460866153240204; Train ROC 0.8551511391521671; Val ROC 0.8453444435325813\n",
      "Epoch 115; Train loss 0.4381953775882721; Val loss 0.44720450043678284; Train ROC 0.8533662767361423; Val ROC 0.8440967347120881\n",
      "Epoch 116; Train loss 0.4382372498512268; Val loss 0.44730344414711; Train ROC 0.853416074926754; Val ROC 0.8440835892603078\n",
      "Epoch 115; Train loss 0.4381953775882721; Val loss 0.44720450043678284; Train ROC 0.8533662767361423; Val ROC 0.8440967347120881\n",
      "Epoch 116; Train loss 0.4382372498512268; Val loss 0.44730344414711; Train ROC 0.853416074926754; Val ROC 0.8440835892603078\n",
      "Epoch 117; Train loss 0.43820998072624207; Val loss 0.4473809003829956; Train ROC 0.8535161561861802; Val ROC 0.8440652981689053\n",
      "Epoch 118; Train loss 0.4381198585033417; Val loss 0.4474307894706726; Train ROC 0.8536513436910204; Val ROC 0.8440388629254916\n",
      "Epoch 117; Train loss 0.43820998072624207; Val loss 0.4473809003829956; Train ROC 0.8535161561861802; Val ROC 0.8440652981689053\n",
      "Epoch 118; Train loss 0.4381198585033417; Val loss 0.4474307894706726; Train ROC 0.8536513436910204; Val ROC 0.8440388629254916\n",
      "Epoch 119; Train loss 0.43797561526298523; Val loss 0.4474489092826843; Train ROC 0.8538048792947945; Val ROC 0.8440027114609308\n",
      "Epoch 119; Train loss 0.43797561526298523; Val loss 0.4474489092826843; Train ROC 0.8538048792947945; Val ROC 0.8440027114609308\n",
      "Epoch 120; Train loss 0.44001051783561707; Val loss 0.4488937556743622; Train ROC 0.8515026693177599; Val ROC 0.8421031626109231\n",
      "Val recall 0.4623333215713501\n",
      "Epoch 120; Train loss 0.44001051783561707; Val loss 0.4488937556743622; Train ROC 0.8515026693177599; Val ROC 0.8421031626109231\n",
      "Val recall 0.4623333215713501\n",
      "Epoch 121; Train loss 0.4400308132171631; Val loss 0.4490165710449219; Train ROC 0.8515350343624021; Val ROC 0.8420858233164801\n",
      "Epoch 122; Train loss 0.44002988934516907; Val loss 0.4491591155529022; Train ROC 0.8516472034572169; Val ROC 0.8420839286909221\n",
      "Epoch 121; Train loss 0.4400308132171631; Val loss 0.4490165710449219; Train ROC 0.8515350343624021; Val ROC 0.8420858233164801\n",
      "Epoch 122; Train loss 0.44002988934516907; Val loss 0.4491591155529022; Train ROC 0.8516472034572169; Val ROC 0.8420839286909221\n",
      "Epoch 123; Train loss 0.44000670313835144; Val loss 0.4493050277233124; Train ROC 0.8518206334045144; Val ROC 0.8420931281313986\n",
      "Epoch 124; Train loss 0.4399598240852356; Val loss 0.4494383633136749; Train ROC 0.8520335397698925; Val ROC 0.8421084418960507\n",
      "Epoch 123; Train loss 0.44000670313835144; Val loss 0.4493050277233124; Train ROC 0.8518206334045144; Val ROC 0.8420931281313986\n",
      "Epoch 124; Train loss 0.4399598240852356; Val loss 0.4494383633136749; Train ROC 0.8520335397698925; Val ROC 0.8421084418960507\n",
      "Epoch 125; Train loss 0.4419844448566437; Val loss 0.4513206481933594; Train ROC 0.8497769049390663; Val ROC 0.8400156686856909\n",
      "Epoch 126; Train loss 0.4421057403087616; Val loss 0.45153093338012695; Train ROC 0.8498806441471013; Val ROC 0.8400554482754494\n",
      "Epoch 125; Train loss 0.4419844448566437; Val loss 0.4513206481933594; Train ROC 0.8497769049390663; Val ROC 0.8400156686856909\n",
      "Epoch 126; Train loss 0.4421057403087616; Val loss 0.45153093338012695; Train ROC 0.8498806441471013; Val ROC 0.8400554482754494\n",
      "Epoch 127; Train loss 0.44218701124191284; Val loss 0.4517354965209961; Train ROC 0.8500568947466399; Val ROC 0.840108158380902\n",
      "Epoch 128; Train loss 0.4422230124473572; Val loss 0.45191678404808044; Train ROC 0.8502845061170496; Val ROC 0.8401660628258711\n",
      "Epoch 127; Train loss 0.44218701124191284; Val loss 0.4517354965209961; Train ROC 0.8500568947466399; Val ROC 0.840108158380902\n",
      "Epoch 128; Train loss 0.4422230124473572; Val loss 0.45191678404808044; Train ROC 0.8502845061170496; Val ROC 0.8401660628258711\n",
      "Epoch 129; Train loss 0.4422108829021454; Val loss 0.4520604908466339; Train ROC 0.8505398322800705; Val ROC 0.8402206186539303\n",
      "Epoch 129; Train loss 0.4422108829021454; Val loss 0.4520604908466339; Train ROC 0.8505398322800705; Val ROC 0.8402206186539303\n",
      "Epoch 130; Train loss 0.44415563344955444; Val loss 0.45232781767845154; Train ROC 0.8483095761309934; Val ROC 0.8395973130397119\n",
      "Val recall 0.46233347058296204\n",
      "Epoch 130; Train loss 0.44415563344955444; Val loss 0.45232781767845154; Train ROC 0.8483095761309934; Val ROC 0.8395973130397119\n",
      "Val recall 0.46233347058296204\n",
      "Epoch 131; Train loss 0.4442659318447113; Val loss 0.4525025486946106; Train ROC 0.84843778766105; Val ROC 0.839670718061992\n",
      "Epoch 132; Train loss 0.4443161189556122; Val loss 0.4526592791080475; Train ROC 0.8486362043330585; Val ROC 0.8397505122125763\n",
      "Epoch 131; Train loss 0.4442659318447113; Val loss 0.4525025486946106; Train ROC 0.84843778766105; Val ROC 0.839670718061992\n",
      "Epoch 132; Train loss 0.4443161189556122; Val loss 0.4526592791080475; Train ROC 0.8486362043330585; Val ROC 0.8397505122125763\n",
      "Epoch 133; Train loss 0.44430604577064514; Val loss 0.45278555154800415; Train ROC 0.8488824729307298; Val ROC 0.8398265412758559\n",
      "Epoch 134; Train loss 0.4442381262779236; Val loss 0.4528716504573822; Train ROC 0.8491523523566871; Val ROC 0.8398920610102601\n",
      "Epoch 133; Train loss 0.44430604577064514; Val loss 0.45278555154800415; Train ROC 0.8488824729307298; Val ROC 0.8398265412758559\n",
      "Epoch 134; Train loss 0.4442381262779236; Val loss 0.4528716504573822; Train ROC 0.8491523523566871; Val ROC 0.8398920610102601\n",
      "Epoch 135; Train loss 0.44597455859184265; Val loss 0.45493531227111816; Train ROC 0.8472675975095934; Val ROC 0.8377357979610173\n",
      "Epoch 136; Train loss 0.44603484869003296; Val loss 0.45507508516311646; Train ROC 0.847384636567482; Val ROC 0.8377765343564807\n",
      "Epoch 135; Train loss 0.44597455859184265; Val loss 0.45493531227111816; Train ROC 0.8472675975095934; Val ROC 0.8377357979610173\n",
      "Epoch 136; Train loss 0.44603484869003296; Val loss 0.45507508516311646; Train ROC 0.847384636567482; Val ROC 0.8377765343564807\n",
      "Epoch 137; Train loss 0.44604435563087463; Val loss 0.4552091956138611; Train ROC 0.8475564885498981; Val ROC 0.8378076366336071\n",
      "Epoch 138; Train loss 0.44600388407707214; Val loss 0.4553243815898895; Train ROC 0.8477636553263731; Val ROC 0.8378254668527656\n",
      "Epoch 137; Train loss 0.44604435563087463; Val loss 0.4552091956138611; Train ROC 0.8475564885498981; Val ROC 0.8378076366336071\n",
      "Epoch 138; Train loss 0.44600388407707214; Val loss 0.4553243815898895; Train ROC 0.8477636553263731; Val ROC 0.8378254668527656\n",
      "Epoch 139; Train loss 0.44591575860977173; Val loss 0.4554094672203064; Train ROC 0.8479852682857112; Val ROC 0.8378240845067759\n",
      "Epoch 139; Train loss 0.44591575860977173; Val loss 0.4554094672203064; Train ROC 0.8479852682857112; Val ROC 0.8378240845067759\n",
      "Epoch 140; Train loss 0.4477330148220062; Val loss 0.45666584372520447; Train ROC 0.8457819313189852; Val ROC 0.8363083639576606\n",
      "Val recall 0.4629266560077667\n",
      "Epoch 140; Train loss 0.4477330148220062; Val loss 0.45666584372520447; Train ROC 0.8457819313189852; Val ROC 0.8363083639576606\n",
      "Val recall 0.4629266560077667\n",
      "Epoch 141; Train loss 0.4477754831314087; Val loss 0.4567858576774597; Train ROC 0.8458547544759574; Val ROC 0.8363018067656491\n",
      "Epoch 141; Train loss 0.4477754831314087; Val loss 0.4567858576774597; Train ROC 0.8458547544759574; Val ROC 0.8363018067656491\n",
      "Epoch 142; Train loss 0.4477589428424835; Val loss 0.4568938612937927; Train ROC 0.8459991992463036; Val ROC 0.8363024504570932\n",
      "Epoch 143; Train loss 0.4476871192455292; Val loss 0.45697927474975586; Train ROC 0.8461954680946202; Val ROC 0.8363043465378948\n",
      "Epoch 142; Train loss 0.4477589428424835; Val loss 0.4568938612937927; Train ROC 0.8459991992463036; Val ROC 0.8363024504570932\n",
      "Epoch 143; Train loss 0.4476871192455292; Val loss 0.45697927474975586; Train ROC 0.8461954680946202; Val ROC 0.8363043465378948\n",
      "Epoch 144; Train loss 0.44756564497947693; Val loss 0.45703384280204773; Train ROC 0.8464214547946134; Val ROC 0.8363046690943171\n",
      "Epoch 144; Train loss 0.44756564497947693; Val loss 0.45703384280204773; Train ROC 0.8464214547946134; Val ROC 0.8363046690943171\n",
      "Epoch 145; Train loss 0.449454128742218; Val loss 0.45860999822616577; Train ROC 0.8440710926726832; Val ROC 0.833841693400584\n",
      "Epoch 146; Train loss 0.44949448108673096; Val loss 0.45874202251434326; Train ROC 0.844148420059873; Val ROC 0.8338428595921442\n",
      "Epoch 145; Train loss 0.449454128742218; Val loss 0.45860999822616577; Train ROC 0.8440710926726832; Val ROC 0.833841693400584\n",
      "Epoch 146; Train loss 0.44949448108673096; Val loss 0.45874202251434326; Train ROC 0.844148420059873; Val ROC 0.8338428595921442\n",
      "Epoch 147; Train loss 0.44949910044670105; Val loss 0.45887938141822815; Train ROC 0.8442935119540181; Val ROC 0.8338498031343936\n",
      "Epoch 148; Train loss 0.44946780800819397; Val loss 0.45900759100914; Train ROC 0.8444877524736392; Val ROC 0.83385829708634\n",
      "Epoch 147; Train loss 0.44949910044670105; Val loss 0.45887938141822815; Train ROC 0.8442935119540181; Val ROC 0.8338498031343936\n",
      "Epoch 148; Train loss 0.44946780800819397; Val loss 0.45900759100914; Train ROC 0.8444877524736392; Val ROC 0.83385829708634\n",
      "Epoch 149; Train loss 0.44940099120140076; Val loss 0.4591134786605835; Train ROC 0.844709549053873; Val ROC 0.8338633060684411\n",
      "Epoch 149; Train loss 0.44940099120140076; Val loss 0.4591134786605835; Train ROC 0.844709549053873; Val ROC 0.8338633060684411\n",
      "Epoch 150; Train loss 0.45039990544319153; Val loss 0.45984071493148804; Train ROC 0.8431805834413432; Val ROC 0.8330675153110189\n",
      "Val recall 0.46400555968284607\n",
      "Epoch 150; Train loss 0.45039990544319153; Val loss 0.45984071493148804; Train ROC 0.8431805834413432; Val ROC 0.8330675153110189\n",
      "Val recall 0.46400555968284607\n",
      "Epoch 151; Train loss 0.4504448473453522; Val loss 0.4599294662475586; Train ROC 0.8432425785987; Val ROC 0.8330533511210761\n",
      "Epoch 152; Train loss 0.450399786233902; Val loss 0.45998355746269226; Train ROC 0.8433686502335833; Val ROC 0.8330404946535882\n",
      "Epoch 151; Train loss 0.4504448473453522; Val loss 0.4599294662475586; Train ROC 0.8432425785987; Val ROC 0.8330533511210761\n",
      "Epoch 152; Train loss 0.450399786233902; Val loss 0.45998355746269226; Train ROC 0.8433686502335833; Val ROC 0.8330404946535882\n",
      "Epoch 153; Train loss 0.45027413964271545; Val loss 0.4599999785423279; Train ROC 0.8435412923546177; Val ROC 0.833025134558043\n",
      "Epoch 154; Train loss 0.45008087158203125; Val loss 0.4599783420562744; Train ROC 0.8437406954559205; Val ROC 0.8330046176885701\n",
      "Epoch 153; Train loss 0.45027413964271545; Val loss 0.4599999785423279; Train ROC 0.8435412923546177; Val ROC 0.833025134558043\n",
      "Epoch 154; Train loss 0.45008087158203125; Val loss 0.4599783420562744; Train ROC 0.8437406954559205; Val ROC 0.8330046176885701\n",
      "Epoch 155; Train loss 0.45227131247520447; Val loss 0.46118977665901184; Train ROC 0.8409521770847606; Val ROC 0.8313605375862292\n",
      "Epoch 156; Train loss 0.4522521495819092; Val loss 0.46129336953163147; Train ROC 0.8410374353313381; Val ROC 0.8314113445038768\n",
      "Epoch 155; Train loss 0.45227131247520447; Val loss 0.46118977665901184; Train ROC 0.8409521770847606; Val ROC 0.8313605375862292\n",
      "Epoch 156; Train loss 0.4522521495819092; Val loss 0.46129336953163147; Train ROC 0.8410374353313381; Val ROC 0.8314113445038768\n",
      "Epoch 157; Train loss 0.4522138237953186; Val loss 0.4614216089248657; Train ROC 0.8412318524337793; Val ROC 0.8315022956682393\n",
      "Epoch 158; Train loss 0.4521557688713074; Val loss 0.46155592799186707; Train ROC 0.841510398377534; Val ROC 0.8316218402021829\n",
      "Epoch 157; Train loss 0.4522138237953186; Val loss 0.4614216089248657; Train ROC 0.8412318524337793; Val ROC 0.8315022956682393\n",
      "Epoch 158; Train loss 0.4521557688713074; Val loss 0.46155592799186707; Train ROC 0.841510398377534; Val ROC 0.8316218402021829\n",
      "Epoch 159; Train loss 0.4520757496356964; Val loss 0.46167799830436707; Train ROC 0.841844411824164; Val ROC 0.8317577578842315\n",
      "Epoch 159; Train loss 0.4520757496356964; Val loss 0.46167799830436707; Train ROC 0.841844411824164; Val ROC 0.8317577578842315\n",
      "Epoch 160; Train loss 0.4541985094547272; Val loss 0.46264588832855225; Train ROC 0.8394451072384559; Val ROC 0.8304739868766765\n",
      "Val recall 0.4645437002182007\n",
      "Epoch 160; Train loss 0.4541985094547272; Val loss 0.46264588832855225; Train ROC 0.8394451072384559; Val ROC 0.8304739868766765\n",
      "Val recall 0.4645437002182007\n",
      "Epoch 161; Train loss 0.45431897044181824; Val loss 0.46286752820014954; Train ROC 0.8396587955015786; Val ROC 0.830625156052491\n",
      "Epoch 162; Train loss 0.45440763235092163; Val loss 0.4630875885486603; Train ROC 0.8399521909228391; Val ROC 0.8307794961362892\n",
      "Epoch 161; Train loss 0.45431897044181824; Val loss 0.46286752820014954; Train ROC 0.8396587955015786; Val ROC 0.830625156052491\n",
      "Epoch 162; Train loss 0.45440763235092163; Val loss 0.4630875885486603; Train ROC 0.8399521909228391; Val ROC 0.8307794961362892\n",
      "Epoch 163; Train loss 0.45445317029953003; Val loss 0.46328234672546387; Train ROC 0.8402973515409273; Val ROC 0.8309235146176287\n",
      "Epoch 164; Train loss 0.454446941614151; Val loss 0.4634319543838501; Train ROC 0.8406647459067262; Val ROC 0.83104724869619\n",
      "Epoch 163; Train loss 0.45445317029953003; Val loss 0.46328234672546387; Train ROC 0.8402973515409273; Val ROC 0.8309235146176287\n",
      "Epoch 164; Train loss 0.454446941614151; Val loss 0.4634319543838501; Train ROC 0.8406647459067262; Val ROC 0.83104724869619\n",
      "Epoch 165; Train loss 0.4564664363861084; Val loss 0.46570175886154175; Train ROC 0.83846633055189; Val ROC 0.8284316096638005\n",
      "Epoch 166; Train loss 0.4565856158733368; Val loss 0.46588554978370667; Train ROC 0.8386557902211373; Val ROC 0.8285193005749945\n",
      "Epoch 165; Train loss 0.4564664363861084; Val loss 0.46570175886154175; Train ROC 0.83846633055189; Val ROC 0.8284316096638005\n",
      "Epoch 166; Train loss 0.4565856158733368; Val loss 0.46588554978370667; Train ROC 0.8386557902211373; Val ROC 0.8285193005749945\n",
      "Epoch 167; Train loss 0.4566337764263153; Val loss 0.46604159474372864; Train ROC 0.8389095943182888; Val ROC 0.8285989152229806\n",
      "Epoch 168; Train loss 0.45660823583602905; Val loss 0.466155081987381; Train ROC 0.839205326396261; Val ROC 0.8286636748126148\n",
      "Epoch 167; Train loss 0.4566337764263153; Val loss 0.46604159474372864; Train ROC 0.8389095943182888; Val ROC 0.8285989152229806\n",
      "Epoch 168; Train loss 0.45660823583602905; Val loss 0.466155081987381; Train ROC 0.839205326396261; Val ROC 0.8286636748126148\n",
      "Epoch 169; Train loss 0.4565102159976959; Val loss 0.4662155210971832; Train ROC 0.8395184311182365; Val ROC 0.8287114623029779\n",
      "Epoch 169; Train loss 0.4565102159976959; Val loss 0.4662155210971832; Train ROC 0.8395184311182365; Val ROC 0.8287114623029779\n",
      "Epoch 170; Train loss 0.45828327536582947; Val loss 0.4674314856529236; Train ROC 0.8369539269101607; Val ROC 0.8265454559582899\n",
      "Val recall 0.46634915471076965\n",
      "Epoch 170; Train loss 0.45828327536582947; Val loss 0.4674314856529236; Train ROC 0.8369539269101607; Val ROC 0.8265454559582899\n",
      "Val recall 0.46634915471076965\n",
      "Epoch 171; Train loss 0.45830175280570984; Val loss 0.46751341223716736; Train ROC 0.8370698159909049; Val ROC 0.8265668649597373\n",
      "Epoch 172; Train loss 0.4582488238811493; Val loss 0.46757352352142334; Train ROC 0.8372515808148435; Val ROC 0.8265920100451473\n",
      "Epoch 171; Train loss 0.45830175280570984; Val loss 0.46751341223716736; Train ROC 0.8370698159909049; Val ROC 0.8265668649597373\n",
      "Epoch 172; Train loss 0.4582488238811493; Val loss 0.46757352352142334; Train ROC 0.8372515808148435; Val ROC 0.8265920100451473\n",
      "Epoch 173; Train loss 0.45813095569610596; Val loss 0.46760520339012146; Train ROC 0.8374799591614696; Val ROC 0.8266145277052706\n",
      "Epoch 174; Train loss 0.4579569101333618; Val loss 0.4676038324832916; Train ROC 0.8377331549382563; Val ROC 0.8266324583023903\n",
      "Epoch 173; Train loss 0.45813095569610596; Val loss 0.46760520339012146; Train ROC 0.8374799591614696; Val ROC 0.8266145277052706\n",
      "Epoch 174; Train loss 0.4579569101333618; Val loss 0.4676038324832916; Train ROC 0.8377331549382563; Val ROC 0.8266324583023903\n",
      "Epoch 175; Train loss 0.45967110991477966; Val loss 0.4684741497039795; Train ROC 0.8354875947178657; Val ROC 0.8257700851443521\n",
      "Epoch 176; Train loss 0.4596642851829529; Val loss 0.46856242418289185; Train ROC 0.8355616010874265; Val ROC 0.825775578857923\n",
      "Epoch 175; Train loss 0.45967110991477966; Val loss 0.4684741497039795; Train ROC 0.8354875947178657; Val ROC 0.8257700851443521\n",
      "Epoch 176; Train loss 0.4596642851829529; Val loss 0.46856242418289185; Train ROC 0.8355616010874265; Val ROC 0.825775578857923\n",
      "Epoch 177; Train loss 0.45961669087409973; Val loss 0.46865391731262207; Train ROC 0.8357170352203112; Val ROC 0.8257969299880575\n",
      "Epoch 178; Train loss 0.45953017473220825; Val loss 0.4687351882457733; Train ROC 0.8359353979841152; Val ROC 0.8258294453066596\n",
      "Epoch 177; Train loss 0.45961669087409973; Val loss 0.46865391731262207; Train ROC 0.8357170352203112; Val ROC 0.8257969299880575\n",
      "Epoch 178; Train loss 0.45953017473220825; Val loss 0.4687351882457733; Train ROC 0.8359353979841152; Val ROC 0.8258294453066596\n",
      "Epoch 179; Train loss 0.45940694212913513; Val loss 0.46879374980926514; Train ROC 0.8361946118736135; Val ROC 0.8258691776517675\n",
      "Epoch 179; Train loss 0.45940694212913513; Val loss 0.46879374980926514; Train ROC 0.8361946118736135; Val ROC 0.8258691776517675\n",
      "Epoch 180; Train loss 0.4611353874206543; Val loss 0.4707620441913605; Train ROC 0.8337973459118655; Val ROC 0.8230102389227635\n",
      "Val recall 0.46631136536598206\n",
      "Epoch 180; Train loss 0.4611353874206543; Val loss 0.4707620441913605; Train ROC 0.8337973459118655; Val ROC 0.8230102389227635\n",
      "Val recall 0.46631136536598206\n",
      "Epoch 181; Train loss 0.4611819088459015; Val loss 0.47089168429374695; Train ROC 0.8339179129431755; Val ROC 0.8230813972182348\n",
      "Epoch 182; Train loss 0.46117639541625977; Val loss 0.47100889682769775; Train ROC 0.8341344449649974; Val ROC 0.8231767288348617\n",
      "Epoch 181; Train loss 0.4611819088459015; Val loss 0.47089168429374695; Train ROC 0.8339179129431755; Val ROC 0.8230813972182348\n",
      "Epoch 182; Train loss 0.46117639541625977; Val loss 0.47100889682769775; Train ROC 0.8341344449649974; Val ROC 0.8231767288348617\n",
      "Epoch 183; Train loss 0.4611174464225769; Val loss 0.47109946608543396; Train ROC 0.8344207352909883; Val ROC 0.8232861085603769\n",
      "Epoch 184; Train loss 0.4610058069229126; Val loss 0.4711517095565796; Train ROC 0.8347477395864421; Val ROC 0.8233954279102068\n",
      "Epoch 183; Train loss 0.4611174464225769; Val loss 0.47109946608543396; Train ROC 0.8344207352909883; Val ROC 0.8232861085603769\n",
      "Epoch 184; Train loss 0.4610058069229126; Val loss 0.4711517095565796; Train ROC 0.8347477395864421; Val ROC 0.8233954279102068\n",
      "Epoch 185; Train loss 0.4631059467792511; Val loss 0.47177398204803467; Train ROC 0.8320599813203531; Val ROC 0.8218720096642607\n",
      "Epoch 186; Train loss 0.4631620943546295; Val loss 0.47191962599754333; Train ROC 0.8322400526346875; Val ROC 0.8220096728601689\n",
      "Epoch 185; Train loss 0.4631059467792511; Val loss 0.47177398204803467; Train ROC 0.8320599813203531; Val ROC 0.8218720096642607\n",
      "Epoch 186; Train loss 0.4631620943546295; Val loss 0.47191962599754333; Train ROC 0.8322400526346875; Val ROC 0.8220096728601689\n",
      "Epoch 187; Train loss 0.46317020058631897; Val loss 0.47205856442451477; Train ROC 0.8325271726983796; Val ROC 0.8221749355281645\n",
      "Epoch 188; Train loss 0.46312665939331055; Val loss 0.4721730053424835; Train ROC 0.8328941123507123; Val ROC 0.82235613331596\n",
      "Epoch 187; Train loss 0.46317020058631897; Val loss 0.47205856442451477; Train ROC 0.8325271726983796; Val ROC 0.8221749355281645\n",
      "Epoch 188; Train loss 0.46312665939331055; Val loss 0.4721730053424835; Train ROC 0.8328941123507123; Val ROC 0.82235613331596\n",
      "Epoch 189; Train loss 0.4630297124385834; Val loss 0.4722474217414856; Train ROC 0.8333095944231529; Val ROC 0.8225409722246166\n",
      "Epoch 189; Train loss 0.4630297124385834; Val loss 0.4722474217414856; Train ROC 0.8333095944231529; Val ROC 0.8225409722246166\n",
      "Epoch 190; Train loss 0.4651338458061218; Val loss 0.47358331084251404; Train ROC 0.8307416162851886; Val ROC 0.8209917313927863\n",
      "Val recall 0.46788454055786133\n",
      "Epoch 190; Train loss 0.4651338458061218; Val loss 0.47358331084251404; Train ROC 0.8307416162851886; Val ROC 0.8209917313927863\n",
      "Val recall 0.46788454055786133\n",
      "Epoch 191; Train loss 0.4652075469493866; Val loss 0.4737466275691986; Train ROC 0.8309864743011061; Val ROC 0.8211484379056349\n",
      "Epoch 192; Train loss 0.465230256319046; Val loss 0.47390004992485046; Train ROC 0.8313115779493445; Val ROC 0.8213026956497232\n",
      "Epoch 191; Train loss 0.4652075469493866; Val loss 0.4737466275691986; Train ROC 0.8309864743011061; Val ROC 0.8211484379056349\n",
      "Epoch 192; Train loss 0.465230256319046; Val loss 0.47390004992485046; Train ROC 0.8313115779493445; Val ROC 0.8213026956497232\n",
      "Epoch 193; Train loss 0.4651968777179718; Val loss 0.4740246832370758; Train ROC 0.8316890989485143; Val ROC 0.8214444898082361\n",
      "Epoch 194; Train loss 0.46510496735572815; Val loss 0.4741048812866211; Train ROC 0.8320895364011814; Val ROC 0.8215670883568803\n",
      "Epoch 193; Train loss 0.4651968777179718; Val loss 0.4740246832370758; Train ROC 0.8316890989485143; Val ROC 0.8214444898082361\n",
      "Epoch 194; Train loss 0.46510496735572815; Val loss 0.4741048812866211; Train ROC 0.8320895364011814; Val ROC 0.8215670883568803\n",
      "Epoch 195; Train loss 0.46702203154563904; Val loss 0.47675850987434387; Train ROC 0.829261868785341; Val ROC 0.8189517979737682\n",
      "Epoch 196; Train loss 0.4670654237270355; Val loss 0.4768731892108917; Train ROC 0.8294737173057015; Val ROC 0.8190727714891805\n",
      "Epoch 195; Train loss 0.46702203154563904; Val loss 0.47675850987434387; Train ROC 0.829261868785341; Val ROC 0.8189517979737682\n",
      "Epoch 196; Train loss 0.4670654237270355; Val loss 0.4768731892108917; Train ROC 0.8294737173057015; Val ROC 0.8190727714891805\n",
      "Epoch 197; Train loss 0.4670383930206299; Val loss 0.4769611358642578; Train ROC 0.829778308995565; Val ROC 0.81921364254949\n",
      "Epoch 198; Train loss 0.4669410288333893; Val loss 0.47700968384742737; Train ROC 0.8301503516335965; Val ROC 0.8193668437867696\n",
      "Epoch 197; Train loss 0.4670383930206299; Val loss 0.4769611358642578; Train ROC 0.829778308995565; Val ROC 0.81921364254949\n",
      "Epoch 198; Train loss 0.4669410288333893; Val loss 0.47700968384742737; Train ROC 0.8301503516335965; Val ROC 0.8193668437867696\n",
      "Epoch 199; Train loss 0.4667764902114868; Val loss 0.4770093262195587; Train ROC 0.8305619515848456; Val ROC 0.81952269322886\n",
      "Epoch 199; Train loss 0.4667764902114868; Val loss 0.4770093262195587; Train ROC 0.8305619515848456; Val ROC 0.81952269322886\n",
      "Epoch 200; Train loss 0.46836158633232117; Val loss 0.47729790210723877; Train ROC 0.8284243613371759; Val ROC 0.8178693232578857\n",
      "Val recall 0.46923014521598816\n",
      "Epoch 200; Train loss 0.46836158633232117; Val loss 0.47729790210723877; Train ROC 0.8284243613371759; Val ROC 0.8178693232578857\n",
      "Val recall 0.46923014521598816\n",
      "Epoch 201; Train loss 0.4683333933353424; Val loss 0.4773344099521637; Train ROC 0.8286576158830862; Val ROC 0.8180308102796001\n",
      "Epoch 202; Train loss 0.4682328402996063; Val loss 0.4773508608341217; Train ROC 0.8289817605317226; Val ROC 0.8182047093103513\n",
      "Epoch 201; Train loss 0.4683333933353424; Val loss 0.4773344099521637; Train ROC 0.8286576158830862; Val ROC 0.8180308102796001\n",
      "Epoch 202; Train loss 0.4682328402996063; Val loss 0.4773508608341217; Train ROC 0.8289817605317226; Val ROC 0.8182047093103513\n",
      "Epoch 203; Train loss 0.4680665135383606; Val loss 0.4773399531841278; Train ROC 0.8293691282045571; Val ROC 0.8183804029609711\n",
      "Epoch 204; Train loss 0.4678425192832947; Val loss 0.47729581594467163; Train ROC 0.8297900069053921; Val ROC 0.8185494063832457\n",
      "Epoch 203; Train loss 0.4680665135383606; Val loss 0.4773399531841278; Train ROC 0.8293691282045571; Val ROC 0.8183804029609711\n",
      "Epoch 204; Train loss 0.4678425192832947; Val loss 0.47729581594467163; Train ROC 0.8297900069053921; Val ROC 0.8185494063832457\n",
      "Epoch 205; Train loss 0.46968260407447815; Val loss 0.4791889190673828; Train ROC 0.8272730385886656; Val ROC 0.8161385839443334\n",
      "Epoch 206; Train loss 0.4696445167064667; Val loss 0.47925153374671936; Train ROC 0.8274950140982636; Val ROC 0.8162823746969806\n",
      "Epoch 205; Train loss 0.46968260407447815; Val loss 0.4791889190673828; Train ROC 0.8272730385886656; Val ROC 0.8161385839443334\n",
      "Epoch 206; Train loss 0.4696445167064667; Val loss 0.47925153374671936; Train ROC 0.8274950140982636; Val ROC 0.8162823746969806\n",
      "Epoch 207; Train loss 0.46956828236579895; Val loss 0.47931814193725586; Train ROC 0.8278097708473097; Val ROC 0.8164457730964751\n",
      "Epoch 208; Train loss 0.4694531261920929; Val loss 0.47937309741973877; Train ROC 0.8281948545239037; Val ROC 0.8166224364469343\n",
      "Epoch 207; Train loss 0.46956828236579895; Val loss 0.47931814193725586; Train ROC 0.8278097708473097; Val ROC 0.8164457730964751\n",
      "Epoch 208; Train loss 0.4694531261920929; Val loss 0.47937309741973877; Train ROC 0.8281948545239037; Val ROC 0.8166224364469343\n",
      "Epoch 209; Train loss 0.46929866075515747; Val loss 0.4794020354747772; Train ROC 0.8286239775574438; Val ROC 0.8168056520145027\n",
      "Epoch 209; Train loss 0.46929866075515747; Val loss 0.4794020354747772; Train ROC 0.8286239775574438; Val ROC 0.8168056520145027\n",
      "Epoch 210; Train loss 0.47143465280532837; Val loss 0.4798327088356018; Train ROC 0.8255554106389826; Val ROC 0.8150852793144302\n",
      "Val recall 0.46989205479621887\n",
      "Epoch 210; Train loss 0.47143465280532837; Val loss 0.4798327088356018; Train ROC 0.8255554106389826; Val ROC 0.8150852793144302\n",
      "Val recall 0.46989205479621887\n",
      "Epoch 211; Train loss 0.4714870750904083; Val loss 0.4799948036670685; Train ROC 0.8258165991897118; Val ROC 0.8152640167253601\n",
      "Epoch 212; Train loss 0.47150054574012756; Val loss 0.4801549017429352; Train ROC 0.8261864119667226; Val ROC 0.8154689136618177\n",
      "Epoch 211; Train loss 0.4714870750904083; Val loss 0.4799948036670685; Train ROC 0.8258165991897118; Val ROC 0.8152640167253601\n",
      "Epoch 212; Train loss 0.47150054574012756; Val loss 0.4801549017429352; Train ROC 0.8261864119667226; Val ROC 0.8154689136618177\n",
      "Epoch 213; Train loss 0.47146666049957275; Val loss 0.4802902042865753; Train ROC 0.8266332877920464; Val ROC 0.8156835263794647\n",
      "Epoch 214; Train loss 0.471378892660141; Val loss 0.4803810715675354; Train ROC 0.8271215219864659; Val ROC 0.8158955197264288\n",
      "Epoch 213; Train loss 0.47146666049957275; Val loss 0.4802902042865753; Train ROC 0.8266332877920464; Val ROC 0.8156835263794647\n",
      "Epoch 214; Train loss 0.471378892660141; Val loss 0.4803810715675354; Train ROC 0.8271215219864659; Val ROC 0.8158955197264288\n",
      "Epoch 215; Train loss 0.47295525670051575; Val loss 0.4826016128063202; Train ROC 0.8247833211327431; Val ROC 0.8146426601898824\n",
      "Epoch 216; Train loss 0.47299039363861084; Val loss 0.4826875925064087; Train ROC 0.8250588344723476; Val ROC 0.8148309720880071\n",
      "Epoch 215; Train loss 0.47295525670051575; Val loss 0.4826016128063202; Train ROC 0.8247833211327431; Val ROC 0.8146426601898824\n",
      "Epoch 216; Train loss 0.47299039363861084; Val loss 0.4826875925064087; Train ROC 0.8250588344723476; Val ROC 0.8148309720880071\n",
      "Epoch 217; Train loss 0.4729280173778534; Val loss 0.4827245771884918; Train ROC 0.8254232018563759; Val ROC 0.8150301682601969\n",
      "Epoch 218; Train loss 0.4727724492549896; Val loss 0.4827052056789398; Train ROC 0.8258475506114176; Val ROC 0.8152274825655916\n",
      "Epoch 217; Train loss 0.4729280173778534; Val loss 0.4827245771884918; Train ROC 0.8254232018563759; Val ROC 0.8150301682601969\n",
      "Epoch 218; Train loss 0.4727724492549896; Val loss 0.4827052056789398; Train ROC 0.8258475506114176; Val ROC 0.8152274825655916\n",
      "Epoch 219; Train loss 0.4725321829319; Val loss 0.4826262593269348; Train ROC 0.8263007661222295; Val ROC 0.8154128974799465\n",
      "Epoch 219; Train loss 0.4725321829319; Val loss 0.4826262593269348; Train ROC 0.8263007661222295; Val ROC 0.8154128974799465\n",
      "Epoch 220; Train loss 0.4749375283718109; Val loss 0.48314857482910156; Train ROC 0.8230564385919918; Val ROC 0.8135945567357562\n",
      "Val recall 0.47247549891471863\n",
      "Epoch 220; Train loss 0.4749375283718109; Val loss 0.48314857482910156; Train ROC 0.8230564385919918; Val ROC 0.8135945567357562\n",
      "Val recall 0.47247549891471863\n",
      "Epoch 221; Train loss 0.4748942255973816; Val loss 0.48324453830718994; Train ROC 0.8232872829706718; Val ROC 0.8137340415581454\n",
      "Epoch 222; Train loss 0.4748363196849823; Val loss 0.4833635687828064; Train ROC 0.8236127034146843; Val ROC 0.8138885292983987\n",
      "Epoch 221; Train loss 0.4748942255973816; Val loss 0.48324453830718994; Train ROC 0.8232872829706718; Val ROC 0.8137340415581454\n",
      "Epoch 222; Train loss 0.4748363196849823; Val loss 0.4833635687828064; Train ROC 0.8236127034146843; Val ROC 0.8138885292983987\n",
      "Epoch 223; Train loss 0.47476115822792053; Val loss 0.48348674178123474; Train ROC 0.824006482872491; Val ROC 0.8140478098324804\n",
      "Epoch 224; Train loss 0.4746655821800232; Val loss 0.4835958480834961; Train ROC 0.8244385811100037; Val ROC 0.8142029762915513\n",
      "Epoch 223; Train loss 0.47476115822792053; Val loss 0.48348674178123474; Train ROC 0.824006482872491; Val ROC 0.8140478098324804\n",
      "Epoch 224; Train loss 0.4746655821800232; Val loss 0.4835958480834961; Train ROC 0.8244385811100037; Val ROC 0.8142029762915513\n",
      "Epoch 225; Train loss 0.47635719180107117; Val loss 0.48565673828125; Train ROC 0.8222896186154173; Val ROC 0.810953441330073\n",
      "Epoch 226; Train loss 0.47647085785865784; Val loss 0.4858399033546448; Train ROC 0.8225185458107448; Val ROC 0.8111196756946375\n",
      "Epoch 225; Train loss 0.47635719180107117; Val loss 0.48565673828125; Train ROC 0.8222896186154173; Val ROC 0.810953441330073\n",
      "Epoch 226; Train loss 0.47647085785865784; Val loss 0.4858399033546448; Train ROC 0.8225185458107448; Val ROC 0.8111196756946375\n",
      "Epoch 227; Train loss 0.47652071714401245; Val loss 0.4859994649887085; Train ROC 0.8228643936272401; Val ROC 0.8113136669344914\n",
      "Epoch 228; Train loss 0.4765004813671112; Val loss 0.48611578345298767; Train ROC 0.8232932900499679; Val ROC 0.8115182547839901\n",
      "Epoch 227; Train loss 0.47652071714401245; Val loss 0.4859994649887085; Train ROC 0.8228643936272401; Val ROC 0.8113136669344914\n",
      "Epoch 228; Train loss 0.4765004813671112; Val loss 0.48611578345298767; Train ROC 0.8232932900499679; Val ROC 0.8115182547839901\n",
      "Epoch 229; Train loss 0.476406455039978; Val loss 0.48617294430732727; Train ROC 0.823765378576782; Val ROC 0.8117141288720823\n",
      "Epoch 229; Train loss 0.476406455039978; Val loss 0.48617294430732727; Train ROC 0.823765378576782; Val ROC 0.8117141288720823\n",
      "Epoch 230; Train loss 0.47877228260040283; Val loss 0.48716995120048523; Train ROC 0.8202824047440364; Val ROC 0.8102702871670472\n",
      "Val recall 0.4730105698108673\n",
      "Epoch 230; Train loss 0.47877228260040283; Val loss 0.48716995120048523; Train ROC 0.8202824047440364; Val ROC 0.8102702871670472\n",
      "Val recall 0.4730105698108673\n",
      "Epoch 231; Train loss 0.47883176803588867; Val loss 0.48731333017349243; Train ROC 0.8205661349617894; Val ROC 0.8104924945976626\n",
      "Epoch 232; Train loss 0.4788283705711365; Val loss 0.4874338209629059; Train ROC 0.8209785891433682; Val ROC 0.8107474454140011\n",
      "Epoch 231; Train loss 0.47883176803588867; Val loss 0.48731333017349243; Train ROC 0.8205661349617894; Val ROC 0.8104924945976626\n",
      "Epoch 232; Train loss 0.4788283705711365; Val loss 0.4874338209629059; Train ROC 0.8209785891433682; Val ROC 0.8107474454140011\n",
      "Epoch 233; Train loss 0.4787575602531433; Val loss 0.487513929605484; Train ROC 0.8214804655094551; Val ROC 0.8110135246130474\n",
      "Epoch 234; Train loss 0.4786180555820465; Val loss 0.48754042387008667; Train ROC 0.8220294128311995; Val ROC 0.811272245491593\n",
      "Epoch 233; Train loss 0.4787575602531433; Val loss 0.487513929605484; Train ROC 0.8214804655094551; Val ROC 0.8110135246130474\n",
      "Epoch 234; Train loss 0.4786180555820465; Val loss 0.48754042387008667; Train ROC 0.8220294128311995; Val ROC 0.811272245491593\n",
      "Epoch 235; Train loss 0.4805617332458496; Val loss 0.4899916648864746; Train ROC 0.8190216716337357; Val ROC 0.8080954352768053\n",
      "Epoch 236; Train loss 0.48057523369789124; Val loss 0.4900798499584198; Train ROC 0.8193405464807566; Val ROC 0.8083082750340227\n",
      "Epoch 235; Train loss 0.4805617332458496; Val loss 0.4899916648864746; Train ROC 0.8190216716337357; Val ROC 0.8080954352768053\n",
      "Epoch 236; Train loss 0.48057523369789124; Val loss 0.4900798499584198; Train ROC 0.8193405464807566; Val ROC 0.8083082750340227\n",
      "Epoch 237; Train loss 0.4805191457271576; Val loss 0.4901438057422638; Train ROC 0.8197527615728858; Val ROC 0.8085277608884789\n",
      "Epoch 238; Train loss 0.48039552569389343; Val loss 0.49017220735549927; Train ROC 0.8202276136050652; Val ROC 0.8087415845256996\n",
      "Epoch 237; Train loss 0.4805191457271576; Val loss 0.4901438057422638; Train ROC 0.8197527615728858; Val ROC 0.8085277608884789\n",
      "Epoch 238; Train loss 0.48039552569389343; Val loss 0.49017220735549927; Train ROC 0.8202276136050652; Val ROC 0.8087415845256996\n",
      "Epoch 239; Train loss 0.48020851612091064; Val loss 0.4901561737060547; Train ROC 0.8207328392250004; Val ROC 0.808942450268342\n",
      "Epoch 239; Train loss 0.48020851612091064; Val loss 0.4901561737060547; Train ROC 0.8207328392250004; Val ROC 0.808942450268342\n",
      "Epoch 240; Train loss 0.48177090287208557; Val loss 0.49101725220680237; Train ROC 0.8183086183947953; Val ROC 0.8066449589134226\n",
      "Val recall 0.4743378162384033\n",
      "Epoch 240; Train loss 0.48177090287208557; Val loss 0.49101725220680237; Train ROC 0.8183086183947953; Val ROC 0.8066449589134226\n",
      "Val recall 0.4743378162384033\n",
      "Epoch 241; Train loss 0.48177170753479004; Val loss 0.49109482765197754; Train ROC 0.8184739601539716; Val ROC 0.8066721672972696\n",
      "Epoch 242; Train loss 0.4817078113555908; Val loss 0.4911557137966156; Train ROC 0.8187020475929456; Val ROC 0.8067007018140944\n",
      "Epoch 241; Train loss 0.48177170753479004; Val loss 0.49109482765197754; Train ROC 0.8184739601539716; Val ROC 0.8066721672972696\n",
      "Epoch 242; Train loss 0.4817078113555908; Val loss 0.4911557137966156; Train ROC 0.8187020475929456; Val ROC 0.8067007018140944\n",
      "Epoch 243; Train loss 0.4815860688686371; Val loss 0.491192102432251; Train ROC 0.8189780883002798; Val ROC 0.8067311676759944\n",
      "Epoch 244; Train loss 0.48141443729400635; Val loss 0.491197407245636; Train ROC 0.8192802058555217; Val ROC 0.8067606054929418\n",
      "Epoch 243; Train loss 0.4815860688686371; Val loss 0.491192102432251; Train ROC 0.8189780883002798; Val ROC 0.8067311676759944\n",
      "Epoch 244; Train loss 0.48141443729400635; Val loss 0.491197407245636; Train ROC 0.8192802058555217; Val ROC 0.8067606054929418\n",
      "Epoch 245; Train loss 0.48366501927375793; Val loss 0.49390849471092224; Train ROC 0.8160439586688023; Val ROC 0.803167181660219\n",
      "Epoch 246; Train loss 0.4837208390235901; Val loss 0.4940824806690216; Train ROC 0.816186254611571; Val ROC 0.8032959760605198\n",
      "Epoch 245; Train loss 0.48366501927375793; Val loss 0.49390849471092224; Train ROC 0.8160439586688023; Val ROC 0.803167181660219\n",
      "Epoch 246; Train loss 0.4837208390235901; Val loss 0.4940824806690216; Train ROC 0.816186254611571; Val ROC 0.8032959760605198\n",
      "Epoch 247; Train loss 0.48375505208969116; Val loss 0.49426859617233276; Train ROC 0.8164987135151094; Val ROC 0.8034974657642114\n",
      "Epoch 248; Train loss 0.4837566912174225; Val loss 0.4944399893283844; Train ROC 0.8169364206312947; Val ROC 0.8037372530782538\n",
      "Epoch 247; Train loss 0.48375505208969116; Val loss 0.49426859617233276; Train ROC 0.8164987135151094; Val ROC 0.8034974657642114\n",
      "Epoch 248; Train loss 0.4837566912174225; Val loss 0.4944399893283844; Train ROC 0.8169364206312947; Val ROC 0.8037372530782538\n",
      "Epoch 249; Train loss 0.4837142825126648; Val loss 0.4945717751979828; Train ROC 0.817445848458018; Val ROC 0.8039858483384181\n",
      "Epoch 249; Train loss 0.4837142825126648; Val loss 0.4945717751979828; Train ROC 0.817445848458018; Val ROC 0.8039858483384181\n",
      "Epoch 250; Train loss 0.4857104420661926; Val loss 0.49407052993774414; Train ROC 0.8144547916385517; Val ROC 0.8042887369751616\n",
      "Val recall 0.4772597551345825\n",
      "Epoch 250; Train loss 0.4857104420661926; Val loss 0.49407052993774414; Train ROC 0.8144547916385517; Val ROC 0.8042887369751616\n",
      "Val recall 0.4772597551345825\n",
      "Epoch 251; Train loss 0.48586010932922363; Val loss 0.4942775070667267; Train ROC 0.8147571956228113; Val ROC 0.8045397607322051\n",
      "Epoch 251; Train loss 0.48586010932922363; Val loss 0.4942775070667267; Train ROC 0.8147571956228113; Val ROC 0.8045397607322051\n",
      "Epoch 252; Train loss 0.485919326543808; Val loss 0.49443066120147705; Train ROC 0.8152040039174168; Val ROC 0.8048260581035273\n",
      "Epoch 253; Train loss 0.4858834743499756; Val loss 0.4945136606693268; Train ROC 0.8157471422749395; Val ROC 0.8051248673880474\n",
      "Epoch 252; Train loss 0.485919326543808; Val loss 0.49443066120147705; Train ROC 0.8152040039174168; Val ROC 0.8048260581035273\n",
      "Epoch 253; Train loss 0.4858834743499756; Val loss 0.4945136606693268; Train ROC 0.8157471422749395; Val ROC 0.8051248673880474\n",
      "Epoch 254; Train loss 0.48575180768966675; Val loss 0.4945161044597626; Train ROC 0.8163400006676429; Val ROC 0.8054156612588452\n",
      "Epoch 254; Train loss 0.48575180768966675; Val loss 0.4945161044597626; Train ROC 0.8163400006676429; Val ROC 0.8054156612588452\n",
      "Epoch 255; Train loss 0.487812876701355; Val loss 0.4972809851169586; Train ROC 0.8131620652289; Val ROC 0.8007328828602776\n",
      "Epoch 256; Train loss 0.4878281354904175; Val loss 0.49738144874572754; Train ROC 0.8134617450209412; Val ROC 0.8008886840762738\n",
      "Epoch 255; Train loss 0.487812876701355; Val loss 0.4972809851169586; Train ROC 0.8131620652289; Val ROC 0.8007328828602776\n",
      "Epoch 256; Train loss 0.4878281354904175; Val loss 0.49738144874572754; Train ROC 0.8134617450209412; Val ROC 0.8008886840762738\n",
      "Epoch 257; Train loss 0.48777323961257935; Val loss 0.49746033549308777; Train ROC 0.8138555871516555; Val ROC 0.8010515773370461\n",
      "Epoch 257; Train loss 0.48777323961257935; Val loss 0.49746033549308777; Train ROC 0.8138555871516555; Val ROC 0.8010515773370461\n",
      "Epoch 258; Train loss 0.487653911113739; Val loss 0.4975087642669678; Train ROC 0.8143160491797042; Val ROC 0.801213598940782\n",
      "Epoch 259; Train loss 0.4874773621559143; Val loss 0.4975196123123169; Train ROC 0.8148096674088848; Val ROC 0.8013668477272983\n",
      "Epoch 258; Train loss 0.487653911113739; Val loss 0.4975087642669678; Train ROC 0.8143160491797042; Val ROC 0.801213598940782\n",
      "Epoch 259; Train loss 0.4874773621559143; Val loss 0.4975196123123169; Train ROC 0.8148096674088848; Val ROC 0.8013668477272983\n",
      "Epoch 260; Train loss 0.48970016837120056; Val loss 0.49881383776664734; Train ROC 0.8112427521381625; Val ROC 0.799962551114755\n",
      "Val recall 0.47619572281837463\n",
      "Epoch 260; Train loss 0.48970016837120056; Val loss 0.49881383776664734; Train ROC 0.8112427521381625; Val ROC 0.799962551114755\n",
      "Val recall 0.47619572281837463\n",
      "Epoch 261; Train loss 0.48975998163223267; Val loss 0.49897080659866333; Train ROC 0.8113994206056453; Val ROC 0.8000350823140547\n",
      "Epoch 262; Train loss 0.48977845907211304; Val loss 0.49912339448928833; Train ROC 0.8116769370804622; Val ROC 0.800149256322485\n",
      "Epoch 261; Train loss 0.48975998163223267; Val loss 0.49897080659866333; Train ROC 0.8113994206056453; Val ROC 0.8000350823140547\n",
      "Epoch 262; Train loss 0.48977845907211304; Val loss 0.49912339448928833; Train ROC 0.8116769370804622; Val ROC 0.800149256322485\n",
      "Epoch 263; Train loss 0.48975226283073425; Val loss 0.4992527365684509; Train ROC 0.8120395439758384; Val ROC 0.8002824154042711\n",
      "Epoch 264; Train loss 0.4896770119667053; Val loss 0.4993411600589752; Train ROC 0.8124430495664695; Val ROC 0.8004069159562196\n",
      "Epoch 263; Train loss 0.48975226283073425; Val loss 0.4992527365684509; Train ROC 0.8120395439758384; Val ROC 0.8002824154042711\n",
      "Epoch 264; Train loss 0.4896770119667053; Val loss 0.4993411600589752; Train ROC 0.8124430495664695; Val ROC 0.8004069159562196\n",
      "Epoch 265; Train loss 0.49167394638061523; Val loss 0.5010197758674622; Train ROC 0.8091435727153008; Val ROC 0.7971550426891187\n",
      "Epoch 266; Train loss 0.49181482195854187; Val loss 0.501226007938385; Train ROC 0.8092905960134723; Val ROC 0.7972788582951611\n",
      "Epoch 265; Train loss 0.49167394638061523; Val loss 0.5010197758674622; Train ROC 0.8091435727153008; Val ROC 0.7971550426891187\n",
      "Epoch 266; Train loss 0.49181482195854187; Val loss 0.501226007938385; Train ROC 0.8092905960134723; Val ROC 0.7972788582951611\n",
      "Epoch 267; Train loss 0.4918825924396515; Val loss 0.5013971924781799; Train ROC 0.8096061419050211; Val ROC 0.7974730472112361\n",
      "Epoch 267; Train loss 0.4918825924396515; Val loss 0.5013971924781799; Train ROC 0.8096061419050211; Val ROC 0.7974730472112361\n",
      "Epoch 268; Train loss 0.49187299609184265; Val loss 0.5015167593955994; Train ROC 0.8100439358728336; Val ROC 0.7977084856822576\n",
      "Epoch 269; Train loss 0.4917842447757721; Val loss 0.5015714764595032; Train ROC 0.8105589272837286; Val ROC 0.7979623032699943\n",
      "Epoch 268; Train loss 0.49187299609184265; Val loss 0.5015167593955994; Train ROC 0.8100439358728336; Val ROC 0.7977084856822576\n",
      "Epoch 269; Train loss 0.4917842447757721; Val loss 0.5015714764595032; Train ROC 0.8105589272837286; Val ROC 0.7979623032699943\n",
      "Epoch 270; Train loss 0.4944402873516083; Val loss 0.5028542280197144; Train ROC 0.8067769048946221; Val ROC 0.7966803318910529\n",
      "Val recall 0.4797230660915375\n",
      "Epoch 270; Train loss 0.4944402873516083; Val loss 0.5028542280197144; Train ROC 0.8067769048946221; Val ROC 0.7966803318910529\n",
      "Val recall 0.4797230660915375\n",
      "Epoch 271; Train loss 0.49454277753829956; Val loss 0.5030364990234375; Train ROC 0.8071121666183572; Val ROC 0.7969582496316536\n",
      "Epoch 272; Train loss 0.49458399415016174; Val loss 0.5031887888908386; Train ROC 0.8076039451638275; Val ROC 0.7972906729098044\n",
      "Epoch 271; Train loss 0.49454277753829956; Val loss 0.5030364990234375; Train ROC 0.8071121666183572; Val ROC 0.7969582496316536\n",
      "Epoch 272; Train loss 0.49458399415016174; Val loss 0.5031887888908386; Train ROC 0.8076039451638275; Val ROC 0.7972906729098044\n",
      "Epoch 273; Train loss 0.4945531189441681; Val loss 0.5032855272293091; Train ROC 0.8082073635296223; Val ROC 0.7976653818086147\n",
      "Epoch 274; Train loss 0.4944393038749695; Val loss 0.5033046007156372; Train ROC 0.8088829328454414; Val ROC 0.7980593762378074\n",
      "Epoch 273; Train loss 0.4945531189441681; Val loss 0.5032855272293091; Train ROC 0.8082073635296223; Val ROC 0.7976653818086147\n",
      "Epoch 274; Train loss 0.4944393038749695; Val loss 0.5033046007156372; Train ROC 0.8088829328454414; Val ROC 0.7980593762378074\n",
      "Epoch 275; Train loss 0.49633023142814636; Val loss 0.5052169561386108; Train ROC 0.8056246283539711; Val ROC 0.7942675174267941\n",
      "Epoch 276; Train loss 0.4964490234851837; Val loss 0.5053990483283997; Train ROC 0.8058011625759993; Val ROC 0.794229553473328\n",
      "Epoch 275; Train loss 0.49633023142814636; Val loss 0.5052169561386108; Train ROC 0.8056246283539711; Val ROC 0.7942675174267941\n",
      "Epoch 276; Train loss 0.4964490234851837; Val loss 0.5053990483283997; Train ROC 0.8058011625759993; Val ROC 0.794229553473328\n",
      "Epoch 277; Train loss 0.49647265672683716; Val loss 0.5055308938026428; Train ROC 0.8060477077347744; Val ROC 0.7942030310168304\n",
      "Epoch 278; Train loss 0.4964083135128021; Val loss 0.5056061744689941; Train ROC 0.806346490831823; Val ROC 0.794178933097596\n",
      "Epoch 277; Train loss 0.49647265672683716; Val loss 0.5055308938026428; Train ROC 0.8060477077347744; Val ROC 0.7942030310168304\n",
      "Epoch 278; Train loss 0.4964083135128021; Val loss 0.5056061744689941; Train ROC 0.806346490831823; Val ROC 0.794178933097596\n",
      "Epoch 279; Train loss 0.49626535177230835; Val loss 0.5056219696998596; Train ROC 0.8066619905772773; Val ROC 0.7941425684229357\n",
      "Epoch 279; Train loss 0.49626535177230835; Val loss 0.5056219696998596; Train ROC 0.8066619905772773; Val ROC 0.7941425684229357\n",
      "Epoch 280; Train loss 0.4986279308795929; Val loss 0.5072967410087585; Train ROC 0.8025771973801454; Val ROC 0.7901699043676103\n",
      "Val recall 0.476782888174057\n",
      "Epoch 280; Train loss 0.4986279308795929; Val loss 0.5072967410087585; Train ROC 0.8025771973801454; Val ROC 0.7901699043676103\n",
      "Val recall 0.476782888174057\n",
      "Epoch 281; Train loss 0.4987070858478546; Val loss 0.5074672102928162; Train ROC 0.8025815206685665; Val ROC 0.7901906487627182\n",
      "Epoch 282; Train loss 0.4987311065196991; Val loss 0.5076230764389038; Train ROC 0.8028106527668498; Val ROC 0.7903349288833086\n",
      "Epoch 281; Train loss 0.4987070858478546; Val loss 0.5074672102928162; Train ROC 0.8025815206685665; Val ROC 0.7901906487627182\n",
      "Epoch 282; Train loss 0.4987311065196991; Val loss 0.5076230764389038; Train ROC 0.8028106527668498; Val ROC 0.7903349288833086\n",
      "Epoch 283; Train loss 0.49869871139526367; Val loss 0.5077464580535889; Train ROC 0.803220262813661; Val ROC 0.7905696120152481\n",
      "Epoch 284; Train loss 0.49860724806785583; Val loss 0.5078202486038208; Train ROC 0.8037509112625978; Val ROC 0.7908465483125323\n",
      "Epoch 283; Train loss 0.49869871139526367; Val loss 0.5077464580535889; Train ROC 0.803220262813661; Val ROC 0.7905696120152481\n",
      "Epoch 284; Train loss 0.49860724806785583; Val loss 0.5078202486038208; Train ROC 0.8037509112625978; Val ROC 0.7908465483125323\n",
      "Epoch 285; Train loss 0.5005745887756348; Val loss 0.5099260210990906; Train ROC 0.8004323953689432; Val ROC 0.7879140539604633\n",
      "Epoch 286; Train loss 0.50069659948349; Val loss 0.5101124048233032; Train ROC 0.8007023159837277; Val ROC 0.7881105021928957\n",
      "Epoch 285; Train loss 0.5005745887756348; Val loss 0.5099260210990906; Train ROC 0.8004323953689432; Val ROC 0.7879140539604633\n",
      "Epoch 286; Train loss 0.50069659948349; Val loss 0.5101124048233032; Train ROC 0.8007023159837277; Val ROC 0.7881105021928957\n",
      "Epoch 287; Train loss 0.5007289052009583; Val loss 0.5102497339248657; Train ROC 0.8011434426785751; Val ROC 0.7883514364419033\n",
      "Epoch 288; Train loss 0.500667929649353; Val loss 0.5103200078010559; Train ROC 0.8016809480330955; Val ROC 0.7886028351441366\n",
      "Epoch 287; Train loss 0.5007289052009583; Val loss 0.5102497339248657; Train ROC 0.8011434426785751; Val ROC 0.7883514364419033\n",
      "Epoch 288; Train loss 0.500667929649353; Val loss 0.5103200078010559; Train ROC 0.8016809480330955; Val ROC 0.7886028351441366\n",
      "Epoch 289; Train loss 0.5005115270614624; Val loss 0.5103095769882202; Train ROC 0.8022644583832561; Val ROC 0.7888484564956806\n",
      "Epoch 289; Train loss 0.5005115270614624; Val loss 0.5103095769882202; Train ROC 0.8022644583832561; Val ROC 0.7888484564956806\n",
      "Epoch 290; Train loss 0.5027427673339844; Val loss 0.5118236541748047; Train ROC 0.79824815691792; Val ROC 0.7869284131801823\n",
      "Val recall 0.48195329308509827\n",
      "Epoch 290; Train loss 0.5027427673339844; Val loss 0.5118236541748047; Train ROC 0.79824815691792; Val ROC 0.7869284131801823\n",
      "Val recall 0.48195329308509827\n",
      "Epoch 291; Train loss 0.5028007626533508; Val loss 0.5119432806968689; Train ROC 0.7984585734112968; Val ROC 0.7869901411252548\n",
      "Epoch 292; Train loss 0.5027656555175781; Val loss 0.5120111703872681; Train ROC 0.7987832442124331; Val ROC 0.7871030064297869\n",
      "Epoch 291; Train loss 0.5028007626533508; Val loss 0.5119432806968689; Train ROC 0.7984585734112968; Val ROC 0.7869901411252548\n",
      "Epoch 292; Train loss 0.5027656555175781; Val loss 0.5120111703872681; Train ROC 0.7987832442124331; Val ROC 0.7871030064297869\n",
      "Epoch 293; Train loss 0.5026428699493408; Val loss 0.5120179653167725; Train ROC 0.7992014560683917; Val ROC 0.7872785884327688\n",
      "Epoch 294; Train loss 0.5024378299713135; Val loss 0.5119569301605225; Train ROC 0.799697572031271; Val ROC 0.7875326018353684\n",
      "Epoch 293; Train loss 0.5026428699493408; Val loss 0.5120179653167725; Train ROC 0.7992014560683917; Val ROC 0.7872785884327688\n",
      "Epoch 294; Train loss 0.5024378299713135; Val loss 0.5119569301605225; Train ROC 0.799697572031271; Val ROC 0.7875326018353684\n",
      "Epoch 295; Train loss 0.5050439834594727; Val loss 0.5135793685913086; Train ROC 0.7956286670533134; Val ROC 0.7834059150070218\n",
      "Epoch 295; Train loss 0.5050439834594727; Val loss 0.5135793685913086; Train ROC 0.7956286670533134; Val ROC 0.7834059150070218\n",
      "Epoch 296; Train loss 0.5050753355026245; Val loss 0.5137419700622559; Train ROC 0.795927965608219; Val ROC 0.7836767062169591\n",
      "Epoch 296; Train loss 0.5050753355026245; Val loss 0.5137419700622559; Train ROC 0.795927965608219; Val ROC 0.7836767062169591\n",
      "Epoch 297; Train loss 0.5050661563873291; Val loss 0.5139029026031494; Train ROC 0.7964585337665258; Val ROC 0.7840250444445573\n",
      "Epoch 298; Train loss 0.5050095915794373; Val loss 0.5140386819839478; Train ROC 0.7971273479126213; Val ROC 0.7843799554657316\n",
      "Epoch 297; Train loss 0.5050661563873291; Val loss 0.5139029026031494; Train ROC 0.7964585337665258; Val ROC 0.7840250444445573\n",
      "Epoch 298; Train loss 0.5050095915794373; Val loss 0.5140386819839478; Train ROC 0.7971273479126213; Val ROC 0.7843799554657316\n",
      "Epoch 299; Train loss 0.5048987865447998; Val loss 0.5141286849975586; Train ROC 0.7978391467004058; Val ROC 0.7846922051765131\n",
      "Epoch 299; Train loss 0.5048987865447998; Val loss 0.5141286849975586; Train ROC 0.7978391467004058; Val ROC 0.7846922051765131\n",
      "Epoch 300; Train loss 0.5074471235275269; Val loss 0.5174171924591064; Train ROC 0.7934667223422546; Val ROC 0.7804675206915347\n",
      "Val recall 0.4772663414478302\n",
      "Epoch 300; Train loss 0.5074471235275269; Val loss 0.5174171924591064; Train ROC 0.7934667223422546; Val ROC 0.7804675206915347\n",
      "Val recall 0.4772663414478302\n"
     ]
    }
   ],
   "source": [
    "## For example:\n",
    "\n",
    "# using BPR loss\n",
    "loss_fn = \"BPR\"\n",
    "\n",
    "# using hard sampling\n",
    "neg_samp = \"hard\"\n",
    "\n",
    "# for LGConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 4\n",
    "model, optimizer = init_model(\"LGC\", args)\n",
    "lgc_stats_hard = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GATConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"GAT\", args)\n",
    "gat_stats_hard = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5900e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SAGEConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"SAGE\", args)\n",
    "sage_stats_hard = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using random sampling\n",
    "neg_samp = \"random\"\n",
    "\n",
    "# for LGConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 4\n",
    "model, optimizer = init_model(\"LGC\", args)\n",
    "lgc_stats = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GATConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"GAT\", args)\n",
    "gat_stats = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SAGEConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"SAGE\", args)\n",
    "sage_stats = train(datasets, model, optimizer, loss_fn, args, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cab7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_loss(stats):\n",
    "  return [loss.detach().cpu().numpy().item() for loss in stats]\n",
    "\n",
    "def plot_train_val_loss(stats_dict):\n",
    "  fig, ax = plt.subplots(1,1, figsize = (6, 4))\n",
    "  train_loss = detach_loss(stats_dict[\"train\"][\"loss\"])\n",
    "  val_loss = detach_loss(stats_dict[\"val\"][\"loss\"])\n",
    "  idx = np.arange(0, len(train_loss), 1)\n",
    "  ax.plot(idx, train_loss, label = \"train\")\n",
    "  ax.plot(idx, val_loss, label = \"val\")\n",
    "  ax.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c22c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you had to stop for whatever reason, you can always reload the stats here! (just uncomment and change to correct paths)\n",
    "# lgc_stats = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_LGC_4_e64_nodes34810__BPR_random.pkl\", \"rb\"))\n",
    "# gat_stats = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_GAT_3_e64_nodes34810__BPR_random.pkl\", \"rb\"))\n",
    "# sage_stats = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_SAGE_3_e64_nodes34810__BPR_random.pkl\", \"rb\"))\n",
    "# lgc_stats_hard = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_LGC_4_e64_nodes34810__BPR_hard.pkl\", \"rb\"))\n",
    "# gat_stats_hard = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_GAT_3_e64_nodes34810__BPR_hard.pkl\", \"rb\"))\n",
    "# sage_stats_hard = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_SAGE_3_e64_nodes34810__BPR_hard.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(lgc_stats)\n",
    "plot_train_val_loss(lgc_stats_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(sage_stats)\n",
    "plot_train_val_loss(sage_stats_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be522c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(gat_stats)\n",
    "plot_train_val_loss(gat_stats_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
    "key = \"loss\"\n",
    "lgc_loss = pd.Series(detach_loss(lgc_stats[\"val\"][key])).rolling(3).mean()\n",
    "gat_loss = pd.Series(detach_loss(gat_stats[\"val\"][key])).rolling(3).mean()\n",
    "sage_loss = pd.Series(detach_loss(sage_stats[\"val\"][key])).rolling(3).mean()\n",
    "idx = np.arange(0, len(lgc_loss), 1)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "ax.plot(idx, lgc_loss, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
    "ax.plot(idx, gat_loss, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
    "ax.plot(idx, sage_loss, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
    "ax.legend(loc = 'lower right')\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"BPR Loss\")\n",
    "ax.set_title(\"Model BPR Loss, by convolution type and negative sampling\")\n",
    "ax.set_ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ab87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
    "key = \"loss\"\n",
    "lgc_loss = pd.Series(detach_loss(lgc_stats[\"val\"][key])).rolling(3).mean()\n",
    "gat_loss = pd.Series(detach_loss(gat_stats[\"val\"][key])).rolling(3).mean()\n",
    "sage_loss = pd.Series(detach_loss(sage_stats[\"val\"][key])).rolling(3).mean()\n",
    "lgc_hard_loss = pd.Series(detach_loss(lgc_stats_hard[\"val\"][key])).rolling(3).mean()\n",
    "gat_hard_loss = pd.Series(detach_loss(gat_stats_hard[\"val\"][key])).rolling(3).mean()\n",
    "sage_hard_loss = pd.Series(detach_loss(sage_stats_hard[\"val\"][key])).rolling(3).mean()\n",
    "idx = np.arange(0, len(lgc_loss), 1)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "ax.plot(idx, lgc_loss, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
    "ax.plot(idx, lgc_hard_loss, color = colors[0], label = \"LGC - hard\")\n",
    "ax.plot(idx, gat_loss, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
    "ax.plot(idx, gat_hard_loss, color = colors[1], label = \"GAT - hard\")\n",
    "ax.plot(idx, sage_loss, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
    "ax.plot(idx, sage_hard_loss, color = colors[2], label = \"SAGE - hard\")\n",
    "ax.legend(loc = 'lower left')\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"BPR Loss\")\n",
    "ax.set_title(\"Model BPR Loss, by convolution type and negative sampling\")\n",
    "ax.set_ylim(0, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
    "key = \"recall\"\n",
    "lgc_recall = lgc_stats[\"val\"][key]\n",
    "gat_recall = gat_stats[\"val\"][key]\n",
    "sage_recall = sage_stats[\"val\"][key]\n",
    "lgc_hard_recall = lgc_stats_hard[\"val\"][key]\n",
    "gat_hard_recall = gat_stats_hard[\"val\"][key]\n",
    "sage_hard_recall = sage_stats_hard[\"val\"][key]\n",
    "# increment by 10\n",
    "idx = np.arange(0, 10 * len(lgc_recall), 10)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "ax.plot(idx, lgc_recall, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
    "ax.plot(idx, lgc_hard_recall, color = colors[0], label = \"LGC - hard\")\n",
    "ax.plot(idx, gat_recall, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
    "ax.plot(idx, gat_hard_recall, color = colors[1], label = \"GAT - hard\")\n",
    "ax.plot(idx, sage_recall, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
    "ax.plot(idx, sage_hard_recall, color = colors[2], label = \"SAGE - hard\")\n",
    "ax.legend(loc = 'lower right')\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Recall@300\")\n",
    "ax.set_title(\"Model Recall@300, by convolution type and negative sampling\")\n",
    "ax.set_ylim(0, 0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
